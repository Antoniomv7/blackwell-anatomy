<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Background and Related Work – The Anatomy of an Efficient Blackwell GEMM</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./03-architecture.html" rel="next">
<link href="./01-intro.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-5ea8f22911a6df30576a2a25a24cdd7a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-intro.html">Part II: Introduction and Theoretical Background</a></li><li class="breadcrumb-item"><a href="./02-background.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Background and Related Work</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">The Anatomy of an Efficient Blackwell GEMM</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Part I: Preliminaries</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./abstract.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Abstract</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">index.html</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Part II: Introduction and Theoretical Background</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-background.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Background and Related Work</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Part III: System Design</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-architecture.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Architecture Comparison: Hopper vs Blackwell</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-metrics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Metrics for GPU Efficiency</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Part IV: Implementation and Methodology</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-programming.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Programming Models for Modern GPUs</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-methodology.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Methodology and Experimental Setup</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Part V: Results and Discussion</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-results.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Results and Discussion</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Conclusions and Future Work</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Part VI: References and Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-appendices.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Appendices</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#evolution-of-gpu-architectures-from-volta-to-blackwell" id="toc-evolution-of-gpu-architectures-from-volta-to-blackwell" class="nav-link active" data-scroll-target="#evolution-of-gpu-architectures-from-volta-to-blackwell"><span class="header-section-number">2.1</span> Evolution of GPU Architectures: From Volta to Blackwell</a>
  <ul class="collapse">
  <li><a href="#volta-tensor-cores-and-the-shift-toward-ai-centric-design" id="toc-volta-tensor-cores-and-the-shift-toward-ai-centric-design" class="nav-link" data-scroll-target="#volta-tensor-cores-and-the-shift-toward-ai-centric-design"><span class="header-section-number">2.1.1</span> Volta: Tensor Cores and the Shift Toward AI-Centric Design</a></li>
  <li><a href="#turing-and-ampere-maturation-and-generalization-of-tensor-acceleration" id="toc-turing-and-ampere-maturation-and-generalization-of-tensor-acceleration" class="nav-link" data-scroll-target="#turing-and-ampere-maturation-and-generalization-of-tensor-acceleration"><span class="header-section-number">2.1.2</span> Turing and Ampere: Maturation and Generalization of Tensor Acceleration</a></li>
  <li><a href="#hopper-asynchronous-execution-and-data-movement-awareness" id="toc-hopper-asynchronous-execution-and-data-movement-awareness" class="nav-link" data-scroll-target="#hopper-asynchronous-execution-and-data-movement-awareness"><span class="header-section-number">2.1.3</span> Hopper: Asynchronous Execution and Data Movement Awareness</a></li>
  <li><a href="#blackwell-architecture-for-scaling-efficiency-and-reduced-precision" id="toc-blackwell-architecture-for-scaling-efficiency-and-reduced-precision" class="nav-link" data-scroll-target="#blackwell-architecture-for-scaling-efficiency-and-reduced-precision"><span class="header-section-number">2.1.4</span> Blackwell: Architecture for Scaling, Efficiency, and Reduced Precision</a></li>
  <li><a href="#why-new-architectures-are-necessary" id="toc-why-new-architectures-are-necessary" class="nav-link" data-scroll-target="#why-new-architectures-are-necessary"><span class="header-section-number">2.1.5</span> Why New Architectures Are Necessary</a></li>
  </ul></li>
  <li><a href="#programming-models-and-software-stacks-for-modern-gpus" id="toc-programming-models-and-software-stacks-for-modern-gpus" class="nav-link" data-scroll-target="#programming-models-and-software-stacks-for-modern-gpus"><span class="header-section-number">2.2</span> Programming Models and Software Stacks for Modern GPUs</a>
  <ul class="collapse">
  <li><a href="#cuda-as-the-foundation-explicit-parallelism-and-memory-hierarchy" id="toc-cuda-as-the-foundation-explicit-parallelism-and-memory-hierarchy" class="nav-link" data-scroll-target="#cuda-as-the-foundation-explicit-parallelism-and-memory-hierarchy"><span class="header-section-number">2.2.1</span> CUDA as the Foundation: Explicit Parallelism and Memory Hierarchy</a></li>
  <li><a href="#library-centric-optimization-cublas-cudnn-and-kernel-specialization" id="toc-library-centric-optimization-cublas-cudnn-and-kernel-specialization" class="nav-link" data-scroll-target="#library-centric-optimization-cublas-cudnn-and-kernel-specialization"><span class="header-section-number">2.2.2</span> Library-Centric Optimization: cuBLAS, cuDNN, and Kernel Specialization</a></li>
  <li><a href="#transformer-engine-and-precision-aware-software" id="toc-transformer-engine-and-precision-aware-software" class="nav-link" data-scroll-target="#transformer-engine-and-precision-aware-software"><span class="header-section-number">2.2.3</span> Transformer Engine and Precision-Aware Software</a></li>
  <li><a href="#compiler-and-framework-level-abstractions-jax-xla-and-beyond" id="toc-compiler-and-framework-level-abstractions-jax-xla-and-beyond" class="nav-link" data-scroll-target="#compiler-and-framework-level-abstractions-jax-xla-and-beyond"><span class="header-section-number">2.2.4</span> Compiler and Framework-Level Abstractions: JAX, XLA, and Beyond</a></li>
  <li><a href="#implications-for-edge-ai-and-microbenchmarking" id="toc-implications-for-edge-ai-and-microbenchmarking" class="nav-link" data-scroll-target="#implications-for-edge-ai-and-microbenchmarking"><span class="header-section-number">2.2.5</span> Implications for Edge AI and Microbenchmarking</a></li>
  </ul></li>
  <li><a href="#hardware-software-co-design-case-studies" id="toc-hardware-software-co-design-case-studies" class="nav-link" data-scroll-target="#hardware-software-co-design-case-studies"><span class="header-section-number">2.3</span> Hardware-Software Co-Design Case Studies</a>
  <ul class="collapse">
  <li><a href="#gemm-as-the-canonical-co-design-primitive" id="toc-gemm-as-the-canonical-co-design-primitive" class="nav-link" data-scroll-target="#gemm-as-the-canonical-co-design-primitive"><span class="header-section-number">2.3.1</span> GEMM as the Canonical Co-Design Primitive</a></li>
  <li><a href="#transformer-attention-balancing-compute-memory-and-precision" id="toc-transformer-attention-balancing-compute-memory-and-precision" class="nav-link" data-scroll-target="#transformer-attention-balancing-compute-memory-and-precision"><span class="header-section-number">2.3.2</span> Transformer Attention: Balancing Compute, Memory, and Precision</a></li>
  <li><a href="#mixture-of-experts-sparsity-routing-and-system-level-co-design" id="toc-mixture-of-experts-sparsity-routing-and-system-level-co-design" class="nav-link" data-scroll-target="#mixture-of-experts-sparsity-routing-and-system-level-co-design"><span class="header-section-number">2.3.3</span> Mixture-of-Experts: Sparsity, Routing, and System-Level Co-Design</a></li>
  <li><a href="#lessons-for-micro-and-nanoelectronic-design" id="toc-lessons-for-micro-and-nanoelectronic-design" class="nav-link" data-scroll-target="#lessons-for-micro-and-nanoelectronic-design"><span class="header-section-number">2.3.4</span> Lessons for Micro and Nanoelectronic Design</a></li>
  </ul></li>
  <li><a href="#general-matrix-matrix-multiplication-gemm-in-ai-workloads" id="toc-general-matrix-matrix-multiplication-gemm-in-ai-workloads" class="nav-link" data-scroll-target="#general-matrix-matrix-multiplication-gemm-in-ai-workloads"><span class="header-section-number">2.4</span> General Matrix-Matrix Multiplication (GEMM) in AI Workloads</a>
  <ul class="collapse">
  <li><a href="#gemm-as-the-computational-core-of-ai-models" id="toc-gemm-as-the-computational-core-of-ai-models" class="nav-link" data-scroll-target="#gemm-as-the-computational-core-of-ai-models"><span class="header-section-number">2.4.1</span> GEMM as the Computational Core of AI Models</a></li>
  <li><a href="#arithmetic-intensity-and-the-roofline-perspective" id="toc-arithmetic-intensity-and-the-roofline-perspective" class="nav-link" data-scroll-target="#arithmetic-intensity-and-the-roofline-perspective"><span class="header-section-number">2.4.2</span> Arithmetic Intensity and the Roofline Perspective</a></li>
  <li><a href="#data-movement-and-memory-hierarchy-considerations" id="toc-data-movement-and-memory-hierarchy-considerations" class="nav-link" data-scroll-target="#data-movement-and-memory-hierarchy-considerations"><span class="header-section-number">2.4.3</span> Data Movement and Memory Hierarchy Considerations</a></li>
  <li><a href="#gemm-as-a-microbenchmarking-tool" id="toc-gemm-as-a-microbenchmarking-tool" class="nav-link" data-scroll-target="#gemm-as-a-microbenchmarking-tool"><span class="header-section-number">2.4.4</span> GEMM as a Microbenchmarking Tool</a></li>
  </ul></li>
  <li><a href="#domain-specific-languages-dsls-for-gpu-programming" id="toc-domain-specific-languages-dsls-for-gpu-programming" class="nav-link" data-scroll-target="#domain-specific-languages-dsls-for-gpu-programming"><span class="header-section-number">2.5</span> Domain-Specific Languages (DSLs) for GPU Programming</a>
  <ul class="collapse">
  <li><a href="#motivation-for-dsls-in-gpu-computing" id="toc-motivation-for-dsls-in-gpu-computing" class="nav-link" data-scroll-target="#motivation-for-dsls-in-gpu-computing"><span class="header-section-number">2.5.1</span> Motivation for DSLs in GPU Computing</a></li>
  <li><a href="#triton-productivity-oriented-kernel-specialization" id="toc-triton-productivity-oriented-kernel-specialization" class="nav-link" data-scroll-target="#triton-productivity-oriented-kernel-specialization"><span class="header-section-number">2.5.2</span> Triton: Productivity-Oriented Kernel Specialization</a></li>
  <li><a href="#tk-tensor-kernel-explicit-co-design-with-tensor-cores" id="toc-tk-tensor-kernel-explicit-co-design-with-tensor-cores" class="nav-link" data-scroll-target="#tk-tensor-kernel-explicit-co-design-with-tensor-cores"><span class="header-section-number">2.5.3</span> TK (Tensor Kernel): Explicit Co-Design with Tensor Cores</a></li>
  <li><a href="#tilelang-tiling-as-a-first-class-abstraction" id="toc-tilelang-tiling-as-a-first-class-abstraction" class="nav-link" data-scroll-target="#tilelang-tiling-as-a-first-class-abstraction"><span class="header-section-number">2.5.4</span> TileLang: Tiling as a First-Class Abstraction</a></li>
  <li><a href="#cute-cutlass-template-based-architectural-specialization" id="toc-cute-cutlass-template-based-architectural-specialization" class="nav-link" data-scroll-target="#cute-cutlass-template-based-architectural-specialization"><span class="header-section-number">2.5.5</span> CUTE (CUTLASS): Template-Based Architectural Specialization</a></li>
  <li><a href="#gluon-bridging-dsls-and-high-level-frameworks" id="toc-gluon-bridging-dsls-and-high-level-frameworks" class="nav-link" data-scroll-target="#gluon-bridging-dsls-and-high-level-frameworks"><span class="header-section-number">2.5.6</span> Gluon: Bridging DSLs and High-Level Frameworks</a></li>
  <li><a href="#pallas-and-the-broader-dsl-ecosystem" id="toc-pallas-and-the-broader-dsl-ecosystem" class="nav-link" data-scroll-target="#pallas-and-the-broader-dsl-ecosystem"><span class="header-section-number">2.5.7</span> Pallas and the Broader DSL Ecosystem</a></li>
  </ul></li>
  <li><a href="#roofline-model-and-performance-metrics" id="toc-roofline-model-and-performance-metrics" class="nav-link" data-scroll-target="#roofline-model-and-performance-metrics"><span class="header-section-number">2.6</span> Roofline Model and Performance Metrics</a>
  <ul class="collapse">
  <li><a href="#the-roofline-model-fundamentals" id="toc-the-roofline-model-fundamentals" class="nav-link" data-scroll-target="#the-roofline-model-fundamentals"><span class="header-section-number">2.6.1</span> The Roofline Model: Fundamentals</a></li>
  <li><a href="#arithmetic-intensity-in-gemm-and-ai-kernels" id="toc-arithmetic-intensity-in-gemm-and-ai-kernels" class="nav-link" data-scroll-target="#arithmetic-intensity-in-gemm-and-ai-kernels"><span class="header-section-number">2.6.2</span> Arithmetic Intensity in GEMM and AI Kernels</a></li>
  <li><a href="#beyond-a-single-roof-hierarchical-roofline-models" id="toc-beyond-a-single-roof-hierarchical-roofline-models" class="nav-link" data-scroll-target="#beyond-a-single-roof-hierarchical-roofline-models"><span class="header-section-number">2.6.3</span> Beyond a Single Roof: Hierarchical Roofline Models</a></li>
  <li><a href="#performance-per-watt-and-energy-aware-metrics" id="toc-performance-per-watt-and-energy-aware-metrics" class="nav-link" data-scroll-target="#performance-per-watt-and-energy-aware-metrics"><span class="header-section-number">2.6.4</span> Performance per Watt and Energy-Aware Metrics</a></li>
  <li><a href="#memory-efficiency-and-bandwidth-utilization" id="toc-memory-efficiency-and-bandwidth-utilization" class="nav-link" data-scroll-target="#memory-efficiency-and-bandwidth-utilization"><span class="header-section-number">2.6.5</span> Memory Efficiency and Bandwidth Utilization</a></li>
  <li><a href="#roofline-as-a-tool-for-architectural-comparison" id="toc-roofline-as-a-tool-for-architectural-comparison" class="nav-link" data-scroll-target="#roofline-as-a-tool-for-architectural-comparison"><span class="header-section-number">2.6.6</span> Roofline as a Tool for Architectural Comparison</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-intro.html">Part II: Introduction and Theoretical Background</a></li><li class="breadcrumb-item"><a href="./02-background.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Background and Related Work</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Background and Related Work</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>The rapid evolution of artificial intelligence workloads over the last decade has fundamentally reshaped the design space of high-performance and embedded computing systems. GPUs, originally conceived as fixed-function graphics accelerators, have progressively evolved into highly programmable, throughput-oriented processors optimized for data-parallel workloads. This evolution has been driven not only by raw performance demands, but by the changing <strong>computational structure of AI models</strong>, the growing dominance of data movement and energy constraints, and the breakdown of traditional scaling laws at the device level.</p>
<p>This chapter provides the architectural background necessary to contextualize the experimental analysis presented later in this thesis. In particular, it traces the evolution of NVIDIA GPU architectures from Volta to Blackwell, emphasizing how each generation introduces architectural mechanisms explicitly designed to address emerging limitations in efficiency, scalability, and programmability. The goal is to motivate, from a micro and nanoelectronic perspective, <strong>why new architectures are required</strong>, and why incremental improvements are insufficient to sustain AI performance growth.</p>
<section id="evolution-of-gpu-architectures-from-volta-to-blackwell" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="evolution-of-gpu-architectures-from-volta-to-blackwell"><span class="header-section-number">2.1</span> Evolution of GPU Architectures: From Volta to Blackwell</h2>
<section id="volta-tensor-cores-and-the-shift-toward-ai-centric-design" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="volta-tensor-cores-and-the-shift-toward-ai-centric-design"><span class="header-section-number">2.1.1</span> Volta: Tensor Cores and the Shift Toward AI-Centric Design</h3>
<p>The introduction of the Volta architecture marked a structural inflection point in GPU design. While earlier architectures primarily focused on graphics and general-purpose throughput, Volta (V100) was the first NVIDIA GPU to explicitly integrate <strong>Tensor Cores</strong>, specialized units designed to accelerate dense matrix-matrix multiplication (GEMM) with mixed precision arithmetic. This design decision reflected the recognition that AI workloads—particularly deep neural networks—are dominated by linear algebra primitives rather than traditional scalar or vector operations <span class="citation" data-cites="nvidiaVoltaWhitepaper2017">(<a href="#ref-nvidiaVoltaWhitepaper2017" role="doc-biblioref">NVIDIA 2017</a>)</span>.</p>
<p>From a microarchitectural standpoint, Tensor Cores represented a move toward <strong>domain-specific acceleration</strong> within a general-purpose framework. Instead of relying solely on CUDA cores executing fused multiply–add instructions, Volta introduced wide, deeply pipelined datapaths optimized for matrix operations. This significantly improved throughput and energy efficiency for AI workloads, but also established a new dependency: software had to be explicitly adapted to exploit these units, initiating a tighter coupling between architecture and programming models.</p>
</section>
<section id="turing-and-ampere-maturation-and-generalization-of-tensor-acceleration" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="turing-and-ampere-maturation-and-generalization-of-tensor-acceleration"><span class="header-section-number">2.1.2</span> Turing and Ampere: Maturation and Generalization of Tensor Acceleration</h3>
<p>Subsequent architectures, notably Turing and Ampere, refined and generalized the Tensor Core concept. Ampere (A100) expanded Tensor Core support to additional precisions (TF32, BF16) and improved memory hierarchy characteristics, including larger caches and higher-bandwidth memory subsystems <span class="citation" data-cites="nvidiaAmpereWhitepaper2020">(<a href="#ref-nvidiaAmpereWhitepaper2020" role="doc-biblioref">NVIDIA 2020</a>)</span>.</p>
<p>These changes addressed a growing mismatch between compute capability and data movement. As AI models increased in size and depth, memory bandwidth and latency increasingly constrained performance, reinforcing the insight that <strong>raw compute scaling alone is insufficient</strong>. From a design perspective, this period reflects an architectural response to the early manifestations of the memory wall in AI workloads, with incremental improvements in on-chip storage and off-chip bandwidth.</p>
<p>However, despite these advances, Ampere largely preserved the existing execution and programming paradigms. While effective for data center workloads, this continuity limited the ability to exploit more aggressive forms of parallelism and asynchronous execution, particularly under strict power and latency constraints relevant to Edge AI.</p>
</section>
<section id="hopper-asynchronous-execution-and-data-movement-awareness" class="level3" data-number="2.1.3">
<h3 data-number="2.1.3" class="anchored" data-anchor-id="hopper-asynchronous-execution-and-data-movement-awareness"><span class="header-section-number">2.1.3</span> Hopper: Asynchronous Execution and Data Movement Awareness</h3>
<p>The Hopper architecture (H100) represents a qualitative shift from compute-centric optimization toward <strong>data-movement-aware architecture</strong>. While Hopper continues to enhance Tensor Core throughput and precision support (including FP8), its most significant innovations address how data is transferred, scheduled, and synchronized across the chip <span class="citation" data-cites="nvidiaHopperInDepthBlog2022 nvidiaH100Whitepaper2022">(<a href="#ref-nvidiaHopperInDepthBlog2022" role="doc-biblioref">NVIDIA 2022b</a>, <a href="#ref-nvidiaH100Whitepaper2022" role="doc-biblioref">2022a</a>)</span>.</p>
<p>Key features such as <strong>thread block clusters</strong><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> and the <strong>Tensor Memory Accelerator (TMA)</strong><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> enable more explicit control over data movement between global memory and shared memory, allowing software to overlap computation and communication more effectively. From a microelectronic standpoint, this reflects an architectural acknowledgment that memory accesses—not arithmetic—dominate both latency and energy consumption.</p>
<p>Hopper therefore deepens the hardware–software contract: performance is no longer achieved automatically by launching enough threads, but by structuring computation to align with architectural primitives. This shift is particularly relevant for microbenchmarking studies, as it exposes how architectural features translate into achievable efficiency only under specific software conditions.</p>
</section>
<section id="blackwell-architecture-for-scaling-efficiency-and-reduced-precision" class="level3" data-number="2.1.4">
<h3 data-number="2.1.4" class="anchored" data-anchor-id="blackwell-architecture-for-scaling-efficiency-and-reduced-precision"><span class="header-section-number">2.1.4</span> Blackwell: Architecture for Scaling, Efficiency, and Reduced Precision</h3>
<p>Blackwell (B200) extends the Hopper philosophy further, motivated by the explosive growth of large language models (LLMs) and <em>Mixture-of-Experts</em><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> architectures. These models stress in compute, memory and also <strong>system-level scalability and energy efficiency</strong>, pushing existing architectural paradigms to their limits <span class="citation" data-cites="nvidiaBlackwellArchitecturePage">(<a href="#ref-nvidiaBlackwellArchitecturePage" role="doc-biblioref">NVIDIA 2025d</a>)</span>.</p>
<p>A defining characteristic of Blackwell is its support for more aggressive reduced-precision formats, including FP4 with microscaling (e.g., NVFP4), alongside second-generation Transformer Engine support <span class="citation" data-cites="nvidiaNVFP4Blog2025">(<a href="#ref-nvidiaNVFP4Blog2025" role="doc-biblioref">NVIDIA 2025c</a>)</span>. These formats significantly reduce data movement and storage requirements, directly addressing energy and bandwidth constraints. However, they also impose stringent requirements on numerical handling, accumulation, and software tooling.</p>
<p>From a design perspective, Blackwell illustrates why new architectures are necessary: emerging AI workloads cannot be efficiently mapped onto legacy datapaths, memory organizations, or precision assumptions without incurring unacceptable inefficiencies. Supporting FP4-scale computation efficiently requires rethinking datapath widths, register file organization, memory packing, and error management—decisions that must be made at the micro and nanoelectronic design level.</p>
</section>
<section id="why-new-architectures-are-necessary" class="level3" data-number="2.1.5">
<h3 data-number="2.1.5" class="anchored" data-anchor-id="why-new-architectures-are-necessary"><span class="header-section-number">2.1.5</span> Why New Architectures Are Necessary</h3>
<p>The progression from Volta to Blackwell highlights a fundamental conclusion: <strong>architectural evolution is driven less by peak performance targets and more by efficiency, scalability, and workload structure</strong>. Several converging factors necessitate new architectures:</p>
<ol type="1">
<li><strong>Breakdown of traditional scaling laws</strong>, which prevents frequency or voltage scaling from delivering “free” performance gains <span class="citation" data-cites="bohr2007dennard">(<a href="#ref-bohr2007dennard" role="doc-biblioref">Bohr 2007</a>)</span>.</li>
<li><strong>Dominance of data movement costs</strong>, making memory hierarchy and interconnect design central to performance and energy efficiency.</li>
<li><strong>Emergence of new numerical formats</strong>, which require native hardware support to be both efficient and numerically robust.</li>
<li><strong>Increasing software complexity</strong>, which demands architectures that expose controllable primitives for scheduling and data movement rather than opaque execution.</li>
</ol>
<p>These pressures are not limited to data centers. In Edge AI, where power, area, and thermal budgets are far tighter, the inefficiencies tolerated in large systems become prohibitive. As a result, insights gained from architectures such as Hopper and Blackwell are directly relevant to the design of future edge accelerators, reinforcing the importance of micro and nanoelectronic expertise in shaping next-generation AI systems.</p>
</section>
</section>
<section id="programming-models-and-software-stacks-for-modern-gpus" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="programming-models-and-software-stacks-for-modern-gpus"><span class="header-section-number">2.2</span> Programming Models and Software Stacks for Modern GPUs</h2>
<p>As GPU architectures have evolved from general-purpose throughput processors to highly specialized accelerators for AI, the <strong>programming models and software stacks</strong> have become a decisive factor in determining achievable performance and efficiency. Architectural features such as Tensor Cores, asynchronous data movement engines, and reduced-precision datapaths only deliver their theoretical benefits if they are explicitly exposed and exploited by software. Consequently, the evolution of GPU programming models mirrors the architectural transition discussed in Section 2.1, reinforcing the necessity of <strong>hardware–software co-design</strong>.</p>
<p>This section reviews the evolution of GPU programming abstractions and software ecosystems—from CUDA and low-level libraries to modern AI-centric frameworks—and explains why increasingly complex software stacks are required to fully utilize architectures such as Hopper and Blackwell.</p>
<section id="cuda-as-the-foundation-explicit-parallelism-and-memory-hierarchy" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="cuda-as-the-foundation-explicit-parallelism-and-memory-hierarchy"><span class="header-section-number">2.2.1</span> CUDA as the Foundation: Explicit Parallelism and Memory Hierarchy</h3>
<p>CUDA remains the foundational programming model for NVIDIA GPUs. Its design exposes a hierarchical execution and memory model (threads, warps, thread blocks; registers, shared memory, global memory) that maps closely to GPU microarchitecture. This explicitness has been instrumental in enabling high performance, but it also places a significant burden on the programmer or compiler to manage locality, synchronization, and parallelism efficiently <span class="citation" data-cites="nvidiaCUDAProgrammingGuide">(<a href="#ref-nvidiaCUDAProgrammingGuide" role="doc-biblioref">NVIDIA 2025b</a>)</span>.</p>
<p>From a micro and nanoelectronic perspective, CUDA’s memory hierarchy abstraction directly reflects physical design trade-offs: shared memory corresponds to on-chip SRAM with low latency and high energy efficiency, while global memory accesses map to off-chip DRAM or HBM with far higher energy cost. As architectures scale, the gap between these levels widens, making correct software usage increasingly critical.</p>
<p>While CUDA has remained relatively stable conceptually, each new architecture extends it with additional primitives (e.g., warp-level operations, asynchronous copies, cluster-level execution). These extensions illustrate a key point: <strong>architectural innovation increasingly requires new software constructs</strong>, rather than transparent performance gains.</p>
</section>
<section id="library-centric-optimization-cublas-cudnn-and-kernel-specialization" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="library-centric-optimization-cublas-cudnn-and-kernel-specialization"><span class="header-section-number">2.2.2</span> Library-Centric Optimization: cuBLAS, cuDNN, and Kernel Specialization</h3>
<p>For most AI practitioners, direct CUDA kernel development is abstracted away through highly optimized libraries such as <strong>cuBLAS</strong> (dense linear algebra) and <strong>cuDNN</strong> (deep neural networks). These libraries encode architecture-specific knowledge—tiling strategies, memory layouts, pipeline scheduling—that would be impractical to reproduce manually for each application <span class="citation" data-cites="nvidiaCuBLASGuide nvidiaCuDNNDocs">(<a href="#ref-nvidiaCuBLASGuide" role="doc-biblioref">NVIDIA 2025a</a>, <a href="#ref-nvidiaCuDNNDocs" role="doc-biblioref">2025e</a>)</span>.</p>
<p>From a co-design standpoint, these libraries serve as a critical translation layer between hardware capabilities and application-level performance. For example, Tensor Core utilization depends not only on hardware availability, but on precise data layouts, alignment, and precision choices implemented within library kernels. As new precisions (FP8, FP4) and data movement mechanisms (e.g., TMA) are introduced, library implementations must be redesigned accordingly.</p>
<p>This dependency underscores why new architectures cannot rely on legacy software stacks: without corresponding library evolution, hardware innovations remain underutilized. In Edge AI contexts, where custom accelerators or constrained GPUs may lack mature libraries, this challenge is even more pronounced.</p>
</section>
<section id="transformer-engine-and-precision-aware-software" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3" class="anchored" data-anchor-id="transformer-engine-and-precision-aware-software"><span class="header-section-number">2.2.3</span> Transformer Engine and Precision-Aware Software</h3>
<p>The emergence of <strong>Transformer Engine (TE)</strong><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> represents a shift toward <em>architecture-aware, model-specific software</em>. Rather than providing generic linear algebra primitives, TE integrates numerical precision management, scaling policies, and kernel selection specifically optimized for Transformer workloads <span class="citation" data-cites="nvidiaTransformerEngineDocs">(<a href="#ref-nvidiaTransformerEngineDocs" role="doc-biblioref">NVIDIA 2025f</a>)</span>.</p>
<p>This approach reflects a deeper level of co-design. Hardware introduces new numerical formats (e.g., FP8 in Hopper, FP4/NVFP4 in Blackwell), while software encodes domain knowledge about which layers tolerate reduced precision and how to manage accumulation and scaling. From a microelectronic design viewpoint, this tight coupling justifies the inclusion of specialized datapaths and control logic that would be inefficient or unused under a generic programming model.</p>
<p>In Edge AI, similar ideas are increasingly adopted through quantization-aware runtimes and compiler toolchains, reinforcing the notion that <strong>precision is a cross-layer design decision</strong>, not merely a software optimization.</p>
</section>
<section id="compiler-and-framework-level-abstractions-jax-xla-and-beyond" class="level3" data-number="2.2.4">
<h3 data-number="2.2.4" class="anchored" data-anchor-id="compiler-and-framework-level-abstractions-jax-xla-and-beyond"><span class="header-section-number">2.2.4</span> Compiler and Framework-Level Abstractions: JAX, XLA, and Beyond</h3>
<p>At a higher abstraction level, modern AI development increasingly relies on compiler-driven frameworks such as <strong>JAX</strong>, PyTorch, and TensorFlow. These frameworks delegate kernel fusion, operation scheduling, and memory planning to intermediate representations and optimizing compilers (e.g., XLA), aiming to generate hardware-efficient code automatically <span class="citation" data-cites="jaxScalingBook2025">(<a href="#ref-jaxScalingBook2025" role="doc-biblioref">Austin et al. 2025a</a>)</span>.</p>
<p>The JAX ML Scaling Book highlights how performance at scale depends on understanding both the algorithmic structure of models and the characteristics of the underlying hardware, including communication and memory costs. This reinforces a central theme of this thesis: <strong>efficient execution emerges from coordinated decisions across abstraction layers</strong>, not from isolated optimizations.</p>
<p>However, compiler-based approaches also introduce challenges. Automatically generated code must target increasingly complex architectural features (asynchronous execution, cluster-level parallelism), which may be difficult to infer without explicit annotations or architectural hints. This tension motivates continued exposure of low-level primitives alongside high-level abstractions, particularly for research and microbenchmarking purposes.</p>
</section>
<section id="implications-for-edge-ai-and-microbenchmarking" class="level3" data-number="2.2.5">
<h3 data-number="2.2.5" class="anchored" data-anchor-id="implications-for-edge-ai-and-microbenchmarking"><span class="header-section-number">2.2.5</span> Implications for Edge AI and Microbenchmarking</h3>
<p>For Edge AI, the complexity of modern software stacks presents both an opportunity and a challenge. On one hand, advanced libraries and compilers can hide architectural complexity and deliver efficient execution on constrained devices. On the other hand, limited resources and heterogeneous hardware often require explicit control and customization.</p>
<p>In this context, <strong>microbenchmarking</strong> plays a crucial role. By isolating kernels such as GEMM and implementing them across different programming layers (raw CUDA, library calls, framework-generated code), it becomes possible to:</p>
<ul>
<li>Quantify the gap between theoretical peak and achieved performance,</li>
<li>Identify whether bottlenecks arise from hardware limits or software inefficiencies,</li>
<li>Evaluate how architectural features are exposed—or obscured—by the software stack.</li>
</ul>
<p>These insights are essential for assessing the true impact of architectural innovations from Volta through Blackwell and for translating lessons learned to future edge-oriented accelerators.</p>
</section>
</section>
<section id="hardware-software-co-design-case-studies" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="hardware-software-co-design-case-studies"><span class="header-section-number">2.3</span> Hardware-Software Co-Design Case Studies</h2>
<p>Having reviewed the architectural evolution of GPUs and the corresponding programming models, this section presents <strong>concrete case studies</strong> that exemplify how hardware–software co-design materializes in practice. Rather than abstract principles, these cases focus on representative AI workloads whose performance and efficiency are tightly coupled to microarchitectural features and software decisions. The selected case studies—GEMM<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, Transformer attention<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>, and Mixture-of-Experts<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> (MoE)—are directly relevant to modern AI systems and provide a clear bridge to the experimental microbenchmarking work developed later in this thesis.</p>
<p>From a micro and nanoelectronic perspective, these cases illustrate how architectural features justify their area, power, and design complexity <strong>only when matched by appropriate software abstractions</strong>. Conversely, they also show why new architectures are required when existing ones can no longer support emerging workload characteristics efficiently.</p>
<section id="gemm-as-the-canonical-co-design-primitive" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="gemm-as-the-canonical-co-design-primitive"><span class="header-section-number">2.3.1</span> GEMM as the Canonical Co-Design Primitive</h3>
<p>General Matrix–Matrix Multiplication (GEMM) is the foundational kernel underlying the majority of compute in deep learning models, including linear layers<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>, projections in attention mechanisms<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>, and MLP blocks<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>. Its importance stems from two properties: it is both <strong>compute-intensive</strong> and <strong>highly sensitive to data movement</strong>.</p>
<p>From the hardware side, architectures from Volta onward have introduced Tensor Cores explicitly optimized for GEMM, enabling orders-of-magnitude higher throughput per watt compared to traditional SIMD execution<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> <span class="citation" data-cites="nvidiaVoltaWhitepaper2017 nvidiaH100Whitepaper2022">(<a href="#ref-nvidiaVoltaWhitepaper2017" role="doc-biblioref">NVIDIA 2017</a>, <a href="#ref-nvidiaH100Whitepaper2022" role="doc-biblioref">2022a</a>)</span>. However, achieving peak Tensor Core performance requires careful software orchestration:</p>
<ul>
<li>Data must be tiled to fit on-chip memories (registers and shared memory),</li>
<li>Memory accesses must be aligned and coalesced,</li>
<li>Computation and data transfers must be overlapped to hide latency.</li>
</ul>
<p>These requirements highlight a central co-design insight: <strong>Tensor Cores alone do not guarantee performance</strong>. Only when software kernels are structured to match the microarchitectural pipeline—often through hand-tuned libraries such as cuBLAS—does the hardware investment translate into effective performance <span class="citation" data-cites="nvidiaCuBLASGuide">(<a href="#ref-nvidiaCuBLASGuide" role="doc-biblioref">NVIDIA 2025a</a>)</span>.</p>
<p>For this reason, GEMM is a primary target for microbenchmarking in this thesis. By driving the hardware toward the <em>compute-bound</em> Roofline regime, GEMM exposes the true computational ceiling of each architecture and reveals how features such as asynchronous data movement (e.g., TMA in Hopper) affect achievable efficiency.</p>
</section>
<section id="transformer-attention-balancing-compute-memory-and-precision" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="transformer-attention-balancing-compute-memory-and-precision"><span class="header-section-number">2.3.2</span> Transformer Attention: Balancing Compute, Memory, and Precision</h3>
<p>While GEMM captures the compute core of AI workloads, <strong>Transformer attention mechanisms</strong> illustrate a more complex co-design challenge involving irregular data access patterns, intermediate storage, and numerical stability. Attention layers combine matrix multiplications with softmax operations, normalization, and reductions, making them sensitive to both compute throughput and memory bandwidth.</p>
<p>Architecturally, Hopper and Blackwell address these challenges through a combination of higher Tensor Core throughput, expanded shared memory capabilities, and support for reduced-precision formats (FP8, FP4). However, attention performance depends critically on software techniques such as kernel fusion and precision-aware computation <span class="citation" data-cites="nvidiaTransformerEngineDocs">(<a href="#ref-nvidiaTransformerEngineDocs" role="doc-biblioref">NVIDIA 2025f</a>)</span>.</p>
<p>The introduction of optimized attention kernels (e.g., fused attention) demonstrates co-design in action: software reorganizes the computation to minimize memory traffic and exploit on-chip storage, while hardware provides sufficient flexibility and bandwidth to support these fused execution patterns. Without such coordination, attention layers quickly become <em>memory-bound</em>, negating the benefits of increased compute capacity.</p>
<p>For Edge AI, attention exemplifies why architectural generality is insufficient. Efficient execution under tight power and latency constraints requires architectures that anticipate such fused, domain-specific workloads and software stacks capable of exploiting them.</p>
</section>
<section id="mixture-of-experts-sparsity-routing-and-system-level-co-design" class="level3" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="mixture-of-experts-sparsity-routing-and-system-level-co-design"><span class="header-section-number">2.3.3</span> Mixture-of-Experts: Sparsity, Routing, and System-Level Co-Design</h3>
<p>Mixture-of-Experts (MoE) models represent a further escalation in architectural demands. By activating only a subset of parameters per input, MoE introduces <strong>conditional computation</strong> and sparsity at scale. While this reduces average compute per token, it significantly increases pressure on memory systems and interconnects due to expert routing and load imbalance.</p>
<p>From a hardware perspective, MoE stresses not only compute units but also memory bandwidth, cache coherence, and communication fabrics. Blackwell explicitly targets these workloads by combining aggressive reduced-precision formats with architectural features designed for large-scale parallelism and efficient data movement <span class="citation" data-cites="nvidiaBlackwellArchitecturePage">(<a href="#ref-nvidiaBlackwellArchitecturePage" role="doc-biblioref">NVIDIA 2025d</a>)</span>.</p>
<p>Software frameworks must orchestrate expert selection, data routing, and synchronization across devices, often relying on compiler-level transformations and runtime scheduling policies. The JAX ML Scaling Book discusses how such system-level considerations dominate performance as models scale, reinforcing that <strong>co-design must extend beyond the chip to the system level</strong> <span class="citation" data-cites="scalingBook2025">(<a href="#ref-scalingBook2025" role="doc-biblioref">Austin et al. 2025b</a>)</span>.</p>
</section>
<section id="lessons-for-micro-and-nanoelectronic-design" class="level3" data-number="2.3.4">
<h3 data-number="2.3.4" class="anchored" data-anchor-id="lessons-for-micro-and-nanoelectronic-design"><span class="header-section-number">2.3.4</span> Lessons for Micro and Nanoelectronic Design</h3>
<p>Across these case studies, several recurring themes emerge that are directly relevant to micro and nanoelectronic design:</p>
<ol type="1">
<li><strong>Specialization is unavoidable.</strong> General-purpose execution units cannot efficiently sustain the performance and energy demands of modern AI workloads.</li>
<li><strong>Data movement dominates energy and performance.</strong> Architectural resources devoted to computation must be matched by proportional investment in memory hierarchy and interconnect design.</li>
<li><strong>Precision is a hardware concern.</strong> Supporting FP8 and FP4 efficiently requires dedicated datapaths and storage structures, not emulation on wider formats.</li>
<li><strong>Software defines realizable efficiency.</strong> Architectural features only justify their silicon cost when software exposes and exploits them effectively.</li>
</ol>
<p>These insights reinforce the central thesis motivation: new architectures such as Hopper and Blackwell are not incremental upgrades, but <strong>necessary responses to fundamental shifts in workload structure and physical constraints</strong>. Microbenchmarking GEMM and related kernels provides a rigorous methodology to evaluate how well these co-designed systems meet their goals.</p>
</section>
</section>
<section id="general-matrix-matrix-multiplication-gemm-in-ai-workloads" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="general-matrix-matrix-multiplication-gemm-in-ai-workloads"><span class="header-section-number">2.4</span> General Matrix-Matrix Multiplication (GEMM) in AI Workloads</h2>
<p>General Matrix–Matrix Multiplication (GEMM), defined as <span class="math inline">\(C = \alpha AB + \beta C\)</span>, is the fundamental computational primitive underpinning the majority of modern AI workloads. Despite its apparent simplicity, GEMM captures the essential tension between compute throughput, memory bandwidth, and numerical precision that defines efficient execution on contemporary accelerators. For this reason, GEMM serves not only as a building block for higher-level operations, but also as a <strong>diagnostic kernel</strong> for evaluating hardware–software co-design.</p>
<p>In the context of this thesis, GEMM is used as the primary microbenchmark to analyze and compare NVIDIA Hopper and Blackwell architectures, as it directly exercises Tensor Cores, memory hierarchies, and reduced-precision datapaths.</p>
<section id="gemm-as-the-computational-core-of-ai-models" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="gemm-as-the-computational-core-of-ai-models"><span class="header-section-number">2.4.1</span> GEMM as the Computational Core of AI Models</h3>
<p>Most layers in deep neural networks can be expressed as matrix multiplications. In Transformers, for example, GEMM dominates:</p>
<ul>
<li>Linear projections for queries, keys, and values.</li>
<li>Feed-forward network (MLP) layers.</li>
<li>Output projections and embedding transformations.</li>
</ul>
<p>As a result, the performance and energy efficiency of GEMM strongly correlate with end-to-end model throughput and cost. This centrality has motivated decades of optimization research and has directly influenced accelerator architecture design. NVIDIA’s introduction of Tensor Cores in Volta and their subsequent evolution in Ampere, Hopper, and Blackwell reflect the recognition that <strong>optimizing GEMM yields disproportionate benefits for AI workloads</strong> <span class="citation" data-cites="nvidiaVoltaWhitepaper2017 nvidiaH100Whitepaper2022">(<a href="#ref-nvidiaVoltaWhitepaper2017" role="doc-biblioref">NVIDIA 2017</a>, <a href="#ref-nvidiaH100Whitepaper2022" role="doc-biblioref">2022a</a>)</span>.</p>
<p>From a micro and nanoelectronic perspective, GEMM is attractive because it exhibits high arithmetic intensity when data reuse is maximized, making it well suited for dense on-chip computation. However, achieving this theoretical advantage in practice requires careful coordination between hardware capabilities and software implementation.</p>
</section>
<section id="arithmetic-intensity-and-the-roofline-perspective" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="arithmetic-intensity-and-the-roofline-perspective"><span class="header-section-number">2.4.2</span> Arithmetic Intensity and the Roofline Perspective</h3>
<p>The efficiency of GEMM can be formally analyzed using the Roofline model, which relates achieved performance to arithmetic intensity (operations per byte transferred) and available memory bandwidth <span class="citation" data-cites="williams2009roofline">(<a href="#ref-williams2009roofline" role="doc-biblioref">Williams, Waterman, and Patterson 2009</a>)</span>. GEMM is one of the few kernels capable of reaching the <em>compute-bound</em> regime on modern GPUs, provided that data is sufficiently reused through blocking and tiling strategies.</p>
<p>This property makes GEMM particularly valuable for architectural evaluation. If a well-optimized GEMM kernel fails to approach peak performance, the limiting factor is likely architectural (e.g., insufficient compute throughput, pipeline inefficiencies, or suboptimal memory hierarchy design) rather than algorithmic. Consequently, GEMM microbenchmarks are widely used to assess the <em>effective</em> peak performance of accelerators.</p>
<p>In Edge AI scenarios, arithmetic intensity is often reduced due to smaller problem sizes and limited on-chip memory, increasing the likelihood of <em>memory-bound</em> execution. This reinforces the need for architectures and software stacks that can adapt GEMM execution strategies across a wide range of operating points.</p>
</section>
<section id="data-movement-and-memory-hierarchy-considerations" class="level3" data-number="2.4.3">
<h3 data-number="2.4.3" class="anchored" data-anchor-id="data-movement-and-memory-hierarchy-considerations"><span class="header-section-number">2.4.3</span> Data Movement and Memory Hierarchy Considerations</h3>
<p>Although GEMM is compute-intensive in principle, its realized performance is highly sensitive to data movement. Efficient implementations rely on:</p>
<ul>
<li>Blocking matrices to fit into registers and shared memory.</li>
<li>Reusing operands across multiple multiply–accumulate operations.</li>
<li>Overlapping data transfers with computation.</li>
</ul>
<p>From a hardware standpoint, this places stringent requirements on the memory hierarchy: sufficient register file bandwidth, low-latency shared memory, and high-throughput connections to off-chip memory (e.g., HBM). Hopper introduces explicit mechanisms, such as asynchronous memory operations and the Tensor Memory Accelerator (TMA), to reduce the overhead of moving data into on-chip storage <span class="citation" data-cites="nvidiaHopperInDepthBlog2022">(<a href="#ref-nvidiaHopperInDepthBlog2022" role="doc-biblioref">NVIDIA 2022b</a>)</span>.</p>
<p>However, these mechanisms are not transparent. Software must be structured to exploit them, which further strengthens the argument for GEMM as a co-design case study: its performance reflects raw hardware capability and also the maturity and quality of the software stack.</p>
</section>
<section id="gemm-as-a-microbenchmarking-tool" class="level3" data-number="2.4.4">
<h3 data-number="2.4.4" class="anchored" data-anchor-id="gemm-as-a-microbenchmarking-tool"><span class="header-section-number">2.4.4</span> GEMM as a Microbenchmarking Tool</h3>
<p>Beyond its role in applications, GEMM is uniquely suited as a <strong>microbenchmark</strong> for architectural analysis. Its regular structure allows precise control over problem size, data layout, and precision, enabling systematic exploration of performance regimes.</p>
<p>In this thesis, GEMM microbenchmarking serves multiple purposes:</p>
<ul>
<li>Estimating the effective peak performance of Hopper and Blackwell.</li>
<li>Identifying transitions between memory-bound and compute-bound regimes.</li>
<li>Evaluating the impact of architectural features such as Tensor Cores, TMA, and reduced-precision support.</li>
</ul>
<p>By correlating empirical results with Roofline models and architectural specifications, GEMM benchmarks provide a rigorous basis for comparing architectures and for understanding how hardware–software co-design choices translate into realized efficiency.</p>
</section>
</section>
<section id="domain-specific-languages-dsls-for-gpu-programming" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="domain-specific-languages-dsls-for-gpu-programming"><span class="header-section-number">2.5</span> Domain-Specific Languages (DSLs) for GPU Programming</h2>
<p>As GPU architectures evolve toward increasingly specialized and asynchronous execution models, the gap between architectural capability and software productivity widens. Traditional CUDA programming exposes low-level control but demands deep microarchitectural expertise to achieve peak efficiency, particularly for kernels such as GEMM that must carefully orchestrate data movement, tiling, and synchronization. <strong>Domain-Specific Languages (DSLs)</strong> for GPU programming have emerged to bridge this gap by providing higher-level abstractions that encode architectural knowledge while retaining fine-grained control over performance-critical details.</p>
<p>In the context of this thesis, DSLs are especially relevant because they embody <strong>hardware–software co-design principles</strong>: they are explicitly designed around GPU execution models, memory hierarchies, and tensor acceleration units. This section reviews representative DSLs for GPU programming, with particular emphasis on <strong>TK, TileLang, CUTE (CUTLASS), and Gluon</strong>, which are most closely aligned with microarchitectural concerns and GEMM-style workloads.</p>
<section id="motivation-for-dsls-in-gpu-computing" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="motivation-for-dsls-in-gpu-computing"><span class="header-section-number">2.5.1</span> Motivation for DSLs in GPU Computing</h3>
<p>The primary motivation for GPU DSLs is not abstraction for its own sake, but <strong>controlled specialization</strong>. Modern AI kernels require:</p>
<ul>
<li>Precise tiling and data layout choices.</li>
<li>Explicit management of shared memory and registers.</li>
<li>Overlap of computation and data movement.</li>
<li>Efficient use of Tensor Cores and reduced-precision formats.</li>
</ul>
<p>Encoding these decisions directly in CUDA leads to code that is complex, brittle, and architecture-specific. DSLs address this challenge by providing structured representations of computation and data movement that can be systematically mapped to hardware. From a micro and nanoelectronic perspective, DSLs act as <em>software counterparts</em> to architectural primitives, enabling efficient exploitation of silicon features without sacrificing portability or maintainability.</p>
</section>
<section id="triton-productivity-oriented-kernel-specialization" class="level3" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="triton-productivity-oriented-kernel-specialization"><span class="header-section-number">2.5.2</span> Triton: Productivity-Oriented Kernel Specialization</h3>
<p><strong>Triton</strong> is a Python-embedded DSL designed to simplify the development of custom GPU kernels while achieving performance comparable to hand-written CUDA. It exposes a programming model centered on blocks and tiles, abstracting away some low-level details such as thread indexing while retaining explicit control over memory access patterns and parallelism <span class="citation" data-cites="tritonLang2021">(<a href="#ref-tritonLang2021" role="doc-biblioref">Tillet, Kung, and Cox 2021</a>)</span>.</p>
<p>Triton has been particularly successful for rapid prototyping of GEMM and attention kernels in AI frameworks. However, its abstraction level prioritizes productivity and compiler-driven optimization, which can limit explicit control over certain microarchitectural features (e.g., fine-grained shared memory scheduling). As such, Triton is representative of DSLs that trade maximal control for ease of use.</p>
</section>
<section id="tk-tensor-kernel-explicit-co-design-with-tensor-cores" class="level3" data-number="2.5.3">
<h3 data-number="2.5.3" class="anchored" data-anchor-id="tk-tensor-kernel-explicit-co-design-with-tensor-cores"><span class="header-section-number">2.5.3</span> TK (Tensor Kernel): Explicit Co-Design with Tensor Cores</h3>
<p><strong>TK (Tensor Kernel)</strong> represents a class of DSLs explicitly designed around <strong>Tensor Core-centric computation</strong>. Rather than treating tensor acceleration as an opaque backend feature, TK exposes tensor-level operations and tiling strategies as first-class constructs. This design aligns closely with the execution model of modern GPUs, where performance is dominated by how effectively Tensor Cores are fed with data.</p>
<p>From a co-design standpoint, TK is notable because it:</p>
<ul>
<li>Encodes Tensor Core tile shapes and data layouts directly in the language.</li>
<li>Makes register, and shared-memory, level data reuse explicit.</li>
<li>Encourages software structures that map naturally to hardware pipelines.</li>
</ul>
<p>This explicitness is highly relevant for microbenchmarking and architectural evaluation, as it allows controlled experimentation with different tiling and scheduling strategies while preserving a clear mapping to microarchitectural resources. In this sense, TK functions as a <em>research-oriented DSL</em>, enabling exploration of architectural trade-offs without dropping to raw CUDA.</p>
</section>
<section id="tilelang-tiling-as-a-first-class-abstraction" class="level3" data-number="2.5.4">
<h3 data-number="2.5.4" class="anchored" data-anchor-id="tilelang-tiling-as-a-first-class-abstraction"><span class="header-section-number">2.5.4</span> TileLang: Tiling as a First-Class Abstraction</h3>
<p><strong>TileLang</strong> pushes the DSL concept further by elevating <strong>tiling</strong>—the central optimization strategy in GEMM and many AI kernels—to a first-class abstraction. Instead of expressing computation in terms of scalar or vector operations, TileLang structures programs around hierarchical tiles that correspond directly to GPU execution levels (thread, warp, block, cluster).</p>
<p>This approach is particularly powerful from a hardware–software co-design perspective:</p>
<ul>
<li>Tiling decisions directly reflect on-chip memory capacities and bandwidth.</li>
<li>Hierarchical tiles map naturally to registers, shared memory, and global memory.</li>
<li>The language structure mirrors the physical organization of the GPU.</li>
</ul>
<p>For micro and nanoelectronic design, TileLang is interesting because it makes the <strong>constraints of the memory hierarchy explicit in the software representation</strong>. This transparency facilitates reasoning about why certain architectural features (e.g., larger shared memory, faster on-chip interconnects) translate into performance gains for specific workloads.</p>
</section>
<section id="cute-cutlass-template-based-architectural-specialization" class="level3" data-number="2.5.5">
<h3 data-number="2.5.5" class="anchored" data-anchor-id="cute-cutlass-template-based-architectural-specialization"><span class="header-section-number">2.5.5</span> CUTE (CUTLASS): Template-Based Architectural Specialization</h3>
<p><strong>CUTE</strong>, a core component of NVIDIA’s <strong>CUTLASS</strong> library, is a C++ template-based DSL designed to express high-performance tensor operations in a way that is both generic and architecture-aware <span class="citation" data-cites="cutlass2023">(<a href="#ref-cutlass2023" role="doc-biblioref">NVIDIA 2023</a>)</span>. Rather than a standalone language, CUTE provides abstractions for tensor layouts, tiling, and data movement that are resolved at compile time.</p>
<p>CUTE is particularly significant because it underpins many production-grade GEMM kernels used in cuBLAS and related libraries. Its design reflects deep microarchitectural insight:</p>
<ul>
<li>Tensor layouts encode memory coalescing and alignment constraints,</li>
<li>Tiling abstractions map directly to Tensor Core shapes,</li>
<li>Compile-time specialization enables zero-overhead abstractions.</li>
</ul>
<p>From a co-design perspective, CUTE exemplifies how <strong>DSL concepts can be embedded within systems-level programming languages</strong> to achieve both performance and portability. It also illustrates why new architectures require corresponding software evolution: changes in Tensor Core shapes or memory hierarchy necessitate new template instantiations and layout strategies.</p>
</section>
<section id="gluon-bridging-dsls-and-high-level-frameworks" class="level3" data-number="2.5.6">
<h3 data-number="2.5.6" class="anchored" data-anchor-id="gluon-bridging-dsls-and-high-level-frameworks"><span class="header-section-number">2.5.6</span> Gluon: Bridging DSLs and High-Level Frameworks</h3>
<p><strong>Gluon</strong> occupies a complementary position in the DSL landscape by focusing on <strong>integration with higher-level frameworks</strong> while preserving the ability to express hardware-efficient kernels. Rather than targeting only kernel authors, Gluon aims to connect model-level abstractions with low-level execution strategies.</p>
<p>This bridging role is particularly relevant for Edge AI and research workflows, where rapid iteration and deployment matter as much as raw performance. By enabling DSL-defined kernels to be composed within larger computational graphs, Gluon supports end-to-end co-design: architectural constraints influence kernel structure, which in turn informs model design choices.</p>
<p>From a microarchitectural viewpoint, Gluon demonstrates how DSLs can propagate hardware-awareness upward in the software stack, influencing kernel performance and algorithmic structure and deployment decisions.</p>
</section>
<section id="pallas-and-the-broader-dsl-ecosystem" class="level3" data-number="2.5.7">
<h3 data-number="2.5.7" class="anchored" data-anchor-id="pallas-and-the-broader-dsl-ecosystem"><span class="header-section-number">2.5.7</span> Pallas and the Broader DSL Ecosystem</h3>
<p><strong>Pallas</strong>, developed within the JAX ecosystem, represents another approach to GPU DSLs by integrating kernel-level control into a functional, compiler-driven framework <span class="citation" data-cites="pallas2023">(<a href="#ref-pallas2023" role="doc-biblioref">Google 2023</a>)</span>. While powerful, Pallas emphasizes compiler optimization over explicit microarchitectural control, positioning it differently from DSLs such as TK, TileLang, and CUTE.</p>
<p>Collectively, these DSLs illustrate a spectrum of design points:</p>
<ul>
<li><strong>Productivity-oriented</strong> (Triton, Pallas),</li>
<li><strong>Architecture-explicit and research-focused</strong> (TK, TileLang),</li>
<li><strong>Production-grade and template-specialized</strong> (CUTE),</li>
<li><strong>Framework-bridging</strong> (Gluon).</li>
</ul>
</section>
</section>
<section id="roofline-model-and-performance-metrics" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="roofline-model-and-performance-metrics"><span class="header-section-number">2.6</span> Roofline Model and Performance Metrics</h2>
<p>To rigorously evaluate the efficiency of modern GPU architectures, raw peak specifications (e.g., TFLOPS) are insufficient. What ultimately matters is how close real workloads can approach these peaks under practical constraints imposed by memory bandwidth, data movement, and power. The <strong>Roofline model</strong> provides a unifying analytical framework to reason about these limits by linking algorithmic characteristics to architectural capabilities.</p>
<p>In this thesis, the Roofline model is adopted as the primary performance analysis tool to interpret GEMM microbenchmark results on Hopper and Blackwell architectures. Beyond peak performance, complementary metrics such as performance per watt and memory efficiency are introduced to reflect constraints that are especially relevant for Edge AI.</p>
<section id="the-roofline-model-fundamentals" class="level3" data-number="2.6.1">
<h3 data-number="2.6.1" class="anchored" data-anchor-id="the-roofline-model-fundamentals"><span class="header-section-number">2.6.1</span> The Roofline Model: Fundamentals</h3>
<p>The Roofline model characterizes the maximum attainable performance of a kernel as a function of its <strong>arithmetic intensity (AI)</strong>, defined as the ratio of floating-point operations to bytes transferred from main memory <span class="citation" data-cites="williams2009roofline">(<a href="#ref-williams2009roofline" role="doc-biblioref">Williams, Waterman, and Patterson 2009</a>)</span>. Formally,</p>
<p><span class="math display">\[
\text{Performance} \leq \min \left( P_{\text{peak}}, \text{AI} \times B_{\text{mem}} \right)
\]</span></p>
<p>where (<span class="math inline">\(P_{\text{peak}}\)</span>) is the peak compute throughput of the processor and (<span class="math inline">\(B_{\text{mem}}\)</span>) is the achievable memory bandwidth.</p>
<p>Graphically, the model is represented as a piecewise linear curve:</p>
<div id="fig-roofline" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-roofline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../figures/roofline-model.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-roofline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.1: Roofline Model
</figcaption>
</figure>
</div>
<ul>
<li>A <strong>sloped region</strong>, where performance is limited by memory bandwidth (<em>memory-bound</em> regime),</li>
<li>A <strong>flat roof</strong>, where performance is limited by compute throughput (<em>compute-bound</em> regime).</li>
</ul>
<p>The intersection point between these regions defines the minimum arithmetic intensity required to fully utilize the compute units.</p>
<p>From a microarchitectural perspective, Roofline exposes the balance—or imbalance—between compute resources and the memory subsystem. Architectural evolution from Volta to Blackwell can be interpreted as a continuous effort to raise both the roof (through higher Tensor Core throughput) and the slope (through increased memory bandwidth and improved data movement mechanisms).</p>
</section>
<section id="arithmetic-intensity-in-gemm-and-ai-kernels" class="level3" data-number="2.6.2">
<h3 data-number="2.6.2" class="anchored" data-anchor-id="arithmetic-intensity-in-gemm-and-ai-kernels"><span class="header-section-number">2.6.2</span> Arithmetic Intensity in GEMM and AI Kernels</h3>
<p>GEMM is particularly well suited for Roofline analysis because its arithmetic intensity is analytically tractable and tunable through blocking strategies. For an ideal GEMM implementation, arithmetic intensity increases with matrix size and effective data reuse, enabling execution in the compute-bound regime on sufficiently large problems.</p>
<p>However, real implementations deviate from this ideal due to finite on-chip memory, imperfect reuse, and overheads associated with synchronization and data transfers. These effects become more pronounced in Edge AI, where smaller batch sizes and reduced memory capacity often force GEMM into the memory-bound regime.</p>
<p>By systematically varying problem sizes and data layouts, GEMM microbenchmarks allow direct observation of transitions between Roofline regimes. This capability is central to the experimental methodology of this thesis, as it enables attribution of performance gaps to either compute limitations or memory inefficiencies.</p>
</section>
<section id="beyond-a-single-roof-hierarchical-roofline-models" class="level3" data-number="2.6.3">
<h3 data-number="2.6.3" class="anchored" data-anchor-id="beyond-a-single-roof-hierarchical-roofline-models"><span class="header-section-number">2.6.3</span> Beyond a Single Roof: Hierarchical Roofline Models</h3>
<p>Modern GPUs feature <strong>multi-level memory hierarchies</strong>, including registers, shared memory, L2 cache, and off-chip HBM. A single Roofline curve based on off-chip bandwidth may therefore obscure important bottlenecks at intermediate levels.</p>
<p>Hierarchical or multi-roof Roofline models extend the original framework by introducing separate bandwidth ceilings for different memory levels <span class="citation" data-cites="williams2009roofline">(<a href="#ref-williams2009roofline" role="doc-biblioref">Williams, Waterman, and Patterson 2009</a>)</span>. This approach is particularly relevant for architectures such as Hopper and Blackwell, where explicit data movement between global and shared memory (e.g., via TMA) can significantly alter effective bandwidth.</p>
<p>From a co-design perspective, hierarchical Roofline analysis highlights where software must intervene to exploit on-chip locality. If performance is bounded by shared memory bandwidth rather than HBM, architectural investment in Tensor Cores cannot be fully realized without corresponding kernel restructuring.</p>
</section>
<section id="performance-per-watt-and-energy-aware-metrics" class="level3" data-number="2.6.4">
<h3 data-number="2.6.4" class="anchored" data-anchor-id="performance-per-watt-and-energy-aware-metrics"><span class="header-section-number">2.6.4</span> Performance per Watt and Energy-Aware Metrics</h3>
<p>While the Roofline model focuses on performance limits, <strong>energy efficiency</strong> is a first-order constraint in both data center and Edge AI contexts. Performance per watt (GFLOPS/W) provides a direct measure of how effectively an architecture converts power into useful computation.</p>
<p>Architectural features such as reduced-precision Tensor Cores and high-bandwidth on-chip memories aim to improve this metric by reducing energy per operation and per byte moved <span class="citation" data-cites="nvidiaH100Whitepaper2022 nvidiaBlackwellArchitecturePage">(<a href="#ref-nvidiaH100Whitepaper2022" role="doc-biblioref">NVIDIA 2022a</a>, <a href="#ref-nvidiaBlackwellArchitecturePage" role="doc-biblioref">2025d</a>)</span>. However, as with raw performance, these gains are only realized when software achieves high utilization.</p>
<p>In Edge AI, performance per watt often dominates over absolute performance. Consequently, Roofline analysis in this thesis is complemented by energy-normalized metrics to capture trade-offs between throughput and power consumption, providing a more holistic evaluation of architectural efficiency.</p>
</section>
<section id="memory-efficiency-and-bandwidth-utilization" class="level3" data-number="2.6.5">
<h3 data-number="2.6.5" class="anchored" data-anchor-id="memory-efficiency-and-bandwidth-utilization"><span class="header-section-number">2.6.5</span> Memory Efficiency and Bandwidth Utilization</h3>
<p>Another critical metric is <strong>memory efficiency</strong>, defined as the fraction of theoretical peak bandwidth that is actually achieved by a kernel. Low bandwidth utilization may indicate suboptimal access patterns, insufficient concurrency, or architectural mismatches between memory and compute.</p>
<p>For GEMM, high memory efficiency is typically achieved only when data reuse is maximized and transfers are overlapped with computation. Hopper’s asynchronous data movement primitives and Blackwell’s further enhancements are explicitly designed to raise effective bandwidth utilization, but their impact must be quantified empirically.</p>
<p>Memory efficiency metrics are particularly relevant for Edge AI accelerators, where bandwidth is scarce and often shared across heterogeneous components. Insights gained from high-end GPU analysis can therefore inform the design of more balanced edge-oriented architectures.</p>
</section>
<section id="roofline-as-a-tool-for-architectural-comparison" class="level3" data-number="2.6.6">
<h3 data-number="2.6.6" class="anchored" data-anchor-id="roofline-as-a-tool-for-architectural-comparison"><span class="header-section-number">2.6.6</span> Roofline as a Tool for Architectural Comparison</h3>
<p>One of the strengths of the Roofline model is its suitability for <strong>cross-architecture comparison</strong>. By normalizing performance against architectural ceilings, it becomes possible to compare how efficiently different generations (e.g., Hopper vs.&nbsp;Blackwell) exploit their available resources.</p>
<p>In this thesis, Roofline plots are used to:</p>
<ul>
<li>Visualize achievable performance relative to theoretical limits,</li>
<li>Identify shifts in bottlenecks between architectures,</li>
<li>Assess whether architectural innovations effectively translate into higher realized efficiency.</li>
</ul>
<p>This analytical approach aligns naturally with the goals of micro- and nanoelectronic design: evaluating whether increased silicon complexity and power budget deliver proportional gains under realistic workloads.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-jaxScalingBook2025" class="csl-entry" role="listitem">
Austin, Jacob, Sholto Douglas, Roy Frostig, et al. 2025a. <span>“How to Scale Your Model.”</span> Online Book; Google DeepMind.
</div>
<div id="ref-scalingBook2025" class="csl-entry" role="listitem">
———, et al. 2025b. <span>“How to Scale Your Model.”</span> Online Book; Google DeepMind.
</div>
<div id="ref-bohr2007dennard" class="csl-entry" role="listitem">
Bohr, Mark. 2007. <span>“A 30 Year Retrospective on Dennard’s MOSFET Scaling Paper.”</span> IEEE.
</div>
<div id="ref-pallas2023" class="csl-entry" role="listitem">
Google. 2023. <span>“Pallas: A JAX Kernel Language.”</span> Technical Documentation.
</div>
<div id="ref-nvidiaVoltaWhitepaper2017" class="csl-entry" role="listitem">
NVIDIA. 2017. <span>“NVIDIA Tesla V100 GPU Architecture.”</span> Whitepaper.
</div>
<div id="ref-nvidiaAmpereWhitepaper2020" class="csl-entry" role="listitem">
———. 2020. <span>“NVIDIA A100 Tensor Core GPU Architecture.”</span> Whitepaper.
</div>
<div id="ref-nvidiaH100Whitepaper2022" class="csl-entry" role="listitem">
———. 2022a. <span>“NVIDIA H100 Tensor Core GPU Architecture.”</span> Whitepaper.
</div>
<div id="ref-nvidiaHopperInDepthBlog2022" class="csl-entry" role="listitem">
———. 2022b. <span>“NVIDIA Hopper Architecture in-Depth.”</span> NVIDIA Technical Blog.
</div>
<div id="ref-cutlass2023" class="csl-entry" role="listitem">
———. 2023. <span>“CUTLASS: CUDA Templates for Linear Algebra Subroutines.”</span> GitHub Repository.
</div>
<div id="ref-nvidiaCuBLASGuide" class="csl-entry" role="listitem">
———. 2025a. <span>“cuBLAS Library User Guide.”</span> NVIDIA Documentation.
</div>
<div id="ref-nvidiaCUDAProgrammingGuide" class="csl-entry" role="listitem">
———. 2025b. <span>“CUDA c++ Programming Guide.”</span> NVIDIA Documentation.
</div>
<div id="ref-nvidiaNVFP4Blog2025" class="csl-entry" role="listitem">
———. 2025c. <span>“Introducing NVFP4 for Efficient and Accurate Low-Precision Inference.”</span> NVIDIA Technical Blog.
</div>
<div id="ref-nvidiaBlackwellArchitecturePage" class="csl-entry" role="listitem">
———. 2025d. <span>“NVIDIA Blackwell Architecture.”</span> NVIDIA Technology Overview.
</div>
<div id="ref-nvidiaCuDNNDocs" class="csl-entry" role="listitem">
———. 2025e. <span>“NVIDIA cuDNN Documentation.”</span> NVIDIA Documentation.
</div>
<div id="ref-nvidiaTransformerEngineDocs" class="csl-entry" role="listitem">
———. 2025f. <span>“Transformer Engine Documentation.”</span> NVIDIA Documentation.
</div>
<div id="ref-tritonLang2021" class="csl-entry" role="listitem">
Tillet, Philippe, H. T. Kung, and David Cox. 2021. <span>“Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations.”</span> <em>Proceedings of MLSys</em>.
</div>
<div id="ref-williams2009roofline" class="csl-entry" role="listitem">
Williams, Samuel, Andrew Waterman, and David Patterson. 2009. <span>“Roofline: An Insightful Visual Performance Model for Multicore Architectures.”</span> <em>Communications of the ACM</em> 52 (4): 65–76. <a href="https://doi.org/10.1145/1498765.1498785">https://doi.org/10.1145/1498765.1498785</a>.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p><em>Thread block clusters</em> refer to a hierarchical grouping of multiple thread blocks in modern GPU architectures that allows their co-residency and explicit synchronization across a defined set of Streaming Multiprocessors (SMs), enabling efficient data sharing through shared memory and reducing latency from global memory accesses.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><em>Tensor Memory Accelerator</em> refer to a dedicated hardware unit in modern GPUs that asynchronously and programmably manages multidimensional data transfers between global memory and shared memory, offloading these operations from compute cores and enabling overlap of data movement and execution, thereby improving performance and energy efficiency.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p><em>Mixture of Experts (MoE)</em> refer to a neural network architecture in which multiple specialized sub-networks (<em>experts</em>) are selectively activated for each input by a gating mechanism, so that only a subset of the model’s parameters is evaluated per inference or training step, enabling improved scalability and computational efficiency at the cost of increased communication and control complexity.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><em>Transformer Engine</em> refer to a library for accelerating Transformer models on NVIDIA GPUs that automatically manages mixed-precision calculations, leveraging 8-bit floating point (FP8) formats on Tensor Cores to maximize throughput and memory efficiency while maintaining model accuracy.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>GEMM (General Matrix-Matrix Multiplication) is the fundamental linear algebra primitive (<span class="math inline">\(C = \alpha AB + \beta C\)</span>) that underpins the dense layers of deep neural networks. In the context of GPUs, GEMMs are highly optimized to maximize the utilization of Tensor Cores and arithmetic intensity.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Transformer Attention is a mechanism that models dependencies between input tokens by computing scaled dot-products of Query, Key, and Value matrices. From a microarchitectural perspective, attention is often memory-bandwidth bound due to the quadratic complexity of sequence length and the need to load large Key-Value caches.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Mixture-of-Experts (MoE): A sparse model architecture that decouples model size from computational cost by activating only a subset of parameters (specific “experts”) per token. This approach introduces unique hardware challenges, such as dynamic routing logic and high-bandwidth requirements for fetching expert weights.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Linear Layers are also known as fully connected or dense layers, these are fundamental neural network components where inputs are mapped to outputs via a weight matrix (<span class="math inline">\(y=xW^T+b\)</span>). In hardware, they translate directly to large, dense matrix multiplications (GEMMs) that are ideal for saturating GPU Tensor Cores.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>Projections in Attention Mechanisms: The specific linear transformations used to map input embeddings into the Query (Q), Key (K), and Value (V) subspaces required for self-attention. Computationally, these are parallel GEMM operations performed prior to the attention calculation to align data for relevance scoring.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>MLP Blocks (Multilayer Perceptron Blocks): The position-wise feed-forward networks within a Transformer layer, typically composed of two linear transformations separated by a non-linear activation function (e.g., GeLU or SwiGLU). In modern Large Language Models, MLP blocks account for approximately two-thirds of the total parameters and floating-point operations.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>SIMD (Single Instruction, Multiple Data): A parallel computing classification (Flynn’s taxonomy) where a single control instruction triggers the simultaneous execution of the same operation across multiple data points. In the context of NVIDIA GPUs, this refers to the traditional vector-based execution model of CUDA Cores (computing 1D arrays), as opposed to the matrix-based execution model of Tensor Cores.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./01-intro.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./03-architecture.html" class="pagination-link" aria-label="Architecture Comparison: Hopper vs Blackwell">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Architecture Comparison: Hopper vs Blackwell</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>