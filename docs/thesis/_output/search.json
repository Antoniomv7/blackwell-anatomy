[
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Motivation and Context: The Need for Hardware–Software Co-Design\nThe rapid evolution of artificial intelligence (AI), particularly deep learning models based on large-scale matrix operations, has fundamentally reshaped the requirements imposed on modern computing systems. Contemporary workloads such as transformer-based models, large language models (LLMs), and multimodal AI systems exhibit unprecedented computational intensity, memory bandwidth demand, and energy consumption. These characteristics challenge traditional computing paradigms and demand a holistic approach in which hardware and software are designed in close coordination, rather than as independent layers of abstraction (Jouppi et al. 2017; Sze et al. 2020).\nHistorically, general-purpose processors benefited from Dennard scaling1 and instruction-level parallelism, allowing software performance to improve transparently with each technology generation. However, as CMOS scaling entered the sub-10 nm regime, physical constraints related to power density, leakage currents, interconnect delay, and thermal dissipation have severely limited frequency scaling and brute-force performance improvements (Esmaeilzadeh et al. 2011; Thompson et al. 2021). In this context, micro and nanoelectronic design choices, such as transistor architecture, memory hierarchy, interconnect topology, and on-die specialization have become first-order determinants of achievable system-level performance and efficiency.\nGraphics Processing Units (GPUs) have emerged as the dominant computing substrate for AI due to their massive parallelism and high arithmetic throughput. Yet, modern GPUs are no longer generic accelerators; they are highly specialized systems whose efficiency depends critically on software being explicitly structured to exploit architectural features such as tensor cores2, hierarchical memory systems, and fine-grained synchronization mechanisms (NVIDIA 2022). This tight coupling is particularly evident in the evolution from NVIDIA’s Hopper architecture to the more recent Blackwell architecture, where architectural innovations such as ultra-specialized tensor engines3, new low-precision formats (e.g., FP8 and FP4), and advanced memory movement engines fundamentally reshape the optimal software execution model (NVIDIA 2024).\nFrom a hardware design perspective, the motivation for hardware–software co-design is rooted in the need to maximize performance per watt and performance per unit area, two metrics that are especially critical in both datacenter-scale systems and edge deployments. Decisions made at the microarchitectural and circuit levels—such as register file sizing, shared memory organization, cache coherence policies, and on-chip network design—directly constrain which software access patterns are efficient or even feasible (Sze et al. 2020). Consequently, software frameworks that ignore these constraints often underutilize available silicon resources, leading to memory-bound4 execution and suboptimal energy efficiency.\nConversely, from a software and algorithmic standpoint, AI workloads are increasingly designed with an awareness of hardware realities. Techniques such as operator fusion, tiling strategies, mixed-precision arithmetic, and explicit data movement orchestration have become central to achieving peak performance. Domain-specific languages (DSLs)5 and compiler frameworks6—such as Triton, CUTLASS, and JAX-based systems—embody this philosophy by exposing hardware capabilities directly to the programmer or compiler, thereby enabling software to push the hardware toward compute-bound operation (JAX Team 2023; Tillet, Kung, and Cox 2019).\nThis paradigm is particularly relevant for edge AI, where constraints on power, thermal envelope, and silicon area are even more stringent than in datacenter environments. Edge devices require carefully balanced designs in which architectural specialization compensates for limited resources, and software must be explicitly optimized to match the underlying hardware characteristics (Sze et al. 2017). Although this thesis focuses experimentally on high-end GPUs, the insights derived from microbenchmarking architectures such as Hopper and Blackwell are directly transferable to edge-class accelerators, where hardware–software co-design is often the only viable path to achieving acceptable performance and energy efficiency.\nIn this context, the motivation of this thesis is twofold. First, it aims to demonstrate, through microbenchmarking, how architectural innovations at the micro and nanoelectronic level translate into measurable performance and efficiency gains only when matched with appropriately designed software. Second, it seeks to highlight that future progress in AI systems—both at the edge and in large-scale deployments—will depend not on isolated advances in hardware or software, but on their co-evolution as a unified design problem (Jouppi et al. 2023).",
    "crumbs": [
      "Part II: Introduction and Theoretical Background",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#challenges-in-efficient-compute-for-ai-and-edge-applications",
    "href": "01-intro.html#challenges-in-efficient-compute-for-ai-and-edge-applications",
    "title": "1  Introduction",
    "section": "1.2 Challenges in Efficient Compute for AI and Edge Applications",
    "text": "1.2 Challenges in Efficient Compute for AI and Edge Applications\nThe efficient execution of modern AI workloads presents a multifaceted set of challenges that span algorithm design, software systems, microarchitecture, and physical implementation. While advances in deep learning have been driven primarily by increases in model size and data availability, the resulting computational demands increasingly stress the fundamental limits of contemporary hardware, especially in power- and area-constrained environments such as edge devices (Thompson et al. 2021; Sze et al. 2017).\nA primary challenge arises from the mismatch between arithmetic throughput and memory bandwidth, commonly referred to as the memory wall. Although modern accelerators provide massive peak compute capabilities—often measured in tens or hundreds of teraFLOPS—real-world AI kernels frequently fail to achieve these peaks due to insufficient data reuse and limited effective memory bandwidth (Williams, Waterman, and Patterson 2009). This imbalance is particularly pronounced in workloads dominated by General Matrix-Matrix Multiplication (GEMM) and attention mechanisms7, where performance depends critically on maximizing arithmetic intensity through careful tiling, blocking, and data movement strategies.\nFrom a microarchitectural standpoint, this challenge translates directly into design trade-offs involving cache hierarchy depth, shared memory capacity, register file sizing, and on-chip interconnect bandwidth. As technology scaling progresses into advanced nodes, wire delay and energy increasingly dominate over transistor switching costs, making data movement significantly more expensive than computation (Horowitz 2014). Consequently, architectural features such as software-managed scratchpads8, tensor memory accelerators9, and explicit asynchronous data movement engines have become essential enablers of efficiency—but only when software is explicitly designed to exploit them.\nAnother major challenge is energy efficiency, which is particularly critical for edge AI systems operating under strict power and thermal budgets. Unlike datacenter accelerators, which can amortize energy costs across large-scale infrastructure, edge devices must deliver acceptable inference or training performance within envelopes often limited to a few watts. In this regime, inefficiencies stemming from control overhead, redundant memory accesses, or suboptimal precision choices can render otherwise powerful hardware impractical (Chen et al. 2016; Sze et al. 2020). This places increased emphasis on low-precision arithmetic, approximate computing, and hardware specialization—each of which introduces additional complexity at both the circuit and software levels.\nPrecision scaling itself represents a further challenge. While reduced-precision formats such as FP16, INT8, FP8, and more recently FP4 enable substantial gains in throughput and energy efficiency, they also impose strict numerical constraints on algorithms and training stability (Micikevicius et al. 2018; NVIDIA 2024). Supporting these formats efficiently requires non-trivial micro and nanoelectronic innovations, including custom datapaths, specialized normalization logic, and fine-grained control over accumulation and scaling. Software must be co-designed to manage these constraints, often through techniques such as loss scaling, block-wise normalization, and mixed-precision accumulation.\nScalability and programmability constitute another critical dimension of the efficiency challenge. As GPU architectures grow increasingly complex—incorporating heterogeneous execution units10, deep memory hierarchies, and multi-die organizations—the burden placed on programmers and compilers increases substantially. Traditional programming models that abstract away hardware details often fail to expose sufficient control over data layout and execution order, leading to underutilization of available silicon resources (Jouppi et al. 2017). This has motivated the rise of domain-specific languages (DSLs) and performance-oriented compiler frameworks that trade generality for efficiency by making architectural constraints explicit (Tillet, Kung, and Cox 2019; JAX Team 2023).\nFor edge AI applications, these challenges are further compounded by variability in workloads, deployment environments, and real-time constraints. Edge systems must often support a diverse set of models and input conditions while maintaining deterministic latency and reliability. This requirement conflicts with highly specialized hardware designs, which excel at narrow classes of workloads but may lack flexibility (Sze et al. 2017). Bridging this gap demands careful hardware–software co-design, where architectural specialization is balanced against programmability and reuse.",
    "crumbs": [
      "Part II: Introduction and Theoretical Background",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#objectives-and-scope-of-the-thesis",
    "href": "01-intro.html#objectives-and-scope-of-the-thesis",
    "title": "1  Introduction",
    "section": "1.3 Objectives and Scope of the Thesis",
    "text": "1.3 Objectives and Scope of the Thesis\nThe primary objective of this thesis is to systematically analyze and quantify the impact of hardware–software co-design on the efficiency of modern GPU architectures, with a specific focus on AI workloads dominated by dense linear algebra operations. By conducting a detailed microbenchmarking study of NVIDIA’s Hopper (H100) and Blackwell (B200) architectures, this work seeks to elucidate how architectural innovations at the micro and nanoelectronic level translate into tangible performance, energy efficiency, and scalability benefits when matched with appropriately designed software (NVIDIA 2022, 2024).\nA central goal is to characterize the relationship between architectural features and achievable compute efficiency, particularly in terms of performance per watt and proximity to theoretical peak throughput. Rather than relying solely on high-level application benchmarks, this thesis emphasizes fine-grained microbenchmarks—primarily centered on General Matrix-Matrix Multiplication (GEMM)—to isolate specific architectural behaviors such as memory bandwidth utilization, arithmetic intensity, and data movement overheads. This approach enables a more precise attribution of performance bottlenecks to underlying hardware mechanisms, consistent with established performance modeling methodologies such as the Roofline model11 (Williams, Waterman, and Patterson 2009).\nAnother key objective is to compare Hopper and Blackwell as representative points in the evolution of GPU microarchitecture, highlighting the motivations behind Blackwell’s design choices. These include, but are not limited to, enhanced tensor computation units12, support for ultra-low-precision formats, expanded and restructured memory hierarchies, and advanced mechanisms for asynchronous data transfer. By evaluating how these features affect software behavior and efficiency, the thesis aims to provide empirical evidence supporting the necessity of new architectural paradigms for sustaining AI performance scaling under increasingly stringent power and area constraints (Horowitz 2014; Thompson et al. 2021).\nFrom a micro and nanoelectronic design perspective, this thesis seeks to bridge the gap between circuit and architecture-level considerations and system-level performance outcomes. While the experimental focus is on commercially available GPUs, the analysis explicitly interprets results through the lens of silicon efficiency, emphasizing metrics such as energy per operation, data movement cost, and utilization of on-chip resources. In doing so, the thesis aims to reinforce the relevance of microelectronic design decisions in shaping the performance envelope of AI accelerators, even at the highest levels of abstraction.\nIn parallel, the thesis aims to assess the role of modern programming models and domain-specific languages (DSLs) as enablers of hardware–software co-design. By leveraging performance-oriented frameworks such as CUTLASS, Triton, and JAX-based systems, the work explores how software abstractions can expose architectural capabilities while maintaining a balance between programmability and efficiency (Tillet, Kung, and Cox 2019; JAX Team 2023). This analysis contributes to understanding how future software ecosystems must evolve alongside hardware to fully exploit increasingly specialized architectures.\nThe scope of this thesis is deliberately constrained to ensure depth and rigor. Experimentally, it focuses on single-node GPU evaluation, excluding large-scale multi-node distributed training and system-level networking effects. The workloads considered are limited to representative dense linear algebra kernels and closely related AI primitives, rather than end-to-end application benchmarks. While the findings are discussed in the context of both datacenter and edge AI, the experimental platform itself is a high-end accelerator; extrapolations to edge systems are therefore conceptual and architectural rather than empirical.\nIn summary, the scope of this work encompasses (i) a microbenchmark-driven comparison of Hopper and Blackwell GPUs, (ii) an efficiency-focused analysis grounded in micro and nanoelectronic design principles, and (iii) an evaluation of software frameworks as integral components of hardware–software co-design. Collectively, these objectives position the thesis as a foundation for future doctoral research in edge AI and accelerator architecture, providing both methodological tools and conceptual insights relevant to the co-design of next-generation AI systems.",
    "crumbs": [
      "Part II: Introduction and Theoretical Background",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#methodology-overview",
    "href": "01-intro.html#methodology-overview",
    "title": "1  Introduction",
    "section": "1.4 Methodology Overview",
    "text": "1.4 Methodology Overview\nThis thesis adopts a quantitative, experimental methodology grounded in microbenchmarking and performance modeling to analyze the efficiency implications of hardware–software co-design in modern GPU architectures. The methodology is designed to isolate and characterize the impact of architectural features on AI-relevant workloads, with particular emphasis on dense linear algebra kernels representative of contemporary deep learning models\nThe methodological workflow begins with a systematic architectural analysis of NVIDIA’s Hopper and Blackwell GPUs. This analysis focuses on identifying key microarchitectural differences—such as tensor core design, supported numerical formats, memory hierarchy organization, and data movement mechanisms—that are expected to influence performance and energy efficiency. Publicly available technical documentation and research publications are used to establish a baseline understanding of each architecture, providing context for the experimental results presented later in the thesis (NVIDIA 2022, 2024).\nAt the core of the experimental approach is a microbenchmark-based evaluation strategy. Rather than relying on end-to-end AI applications, the methodology employs carefully constructed General Matrix-Matrix Multiplication (GEMM) kernels to exert fine-grained control over computational patterns, memory access behavior, and data precision. This approach enables the systematic exploration of how software design choices—such as tiling parameters, memory layout, and precision selection—interact with hardware capabilities to determine achieved throughput and efficiency (Williams, Waterman, and Patterson 2009).\nTo interpret benchmark results in a principled manner, the methodology incorporates analytical performance modeling, primarily using the Roofline model. By relating achieved performance to arithmetic intensity and hardware-specific bandwidth and compute ceilings, this framework facilitates the identification of transitions between memory-bound and compute-bound regimes. This analysis is instrumental in quantifying how architectural enhancements in Blackwell, relative to Hopper, expand the feasible operating space for AI workloads (Williams, Waterman, and Patterson 2009; Horowitz 2014).\nThe software stack used in this work is deliberately chosen to reflect state-of-the-art performance-oriented programming models. Libraries and frameworks such as CUTLASS, Triton, and JAX-based systems are employed to implement and tune microbenchmarks, allowing the study to evaluate both low-level kernel behavior and higher-level abstraction trade-offs (Tillet, Kung, and Cox 2019; JAX Team 2023). Where applicable, multiple implementations of equivalent kernels are compared to assess the impact of abstraction level on performance portability and hardware utilization.\nMeasurement of performance and efficiency metrics is conducted using vendor-provided profiling tools and standardized benchmarking practices. Key metrics include sustained throughput (FLOP/s), memory bandwidth utilization, power consumption, and derived efficiency measures such as performance per watt. To ensure statistical reliability, benchmarks are repeated under controlled conditions, and results are averaged over multiple runs. Particular care is taken to account for warm-up effects, clock variability, and measurement overhead, in line with best practices for performance evaluation on modern accelerators (Jouppi et al. 2017).\nReproducibility and fairness are treated as first-class methodological concerns. All experimental configurations, including hardware specifications, software versions, compiler settings, and runtime parameters, are explicitly documented. Where possible, benchmarks are designed to be portable across architectures, ensuring that observed differences can be attributed to architectural characteristics rather than implementation artifacts.\nFinally, the methodology explicitly connects experimental findings back to micro and nanoelectronic design considerations. Observed performance trends are interpreted in terms of data movement cost, on-chip resource utilization, and energy efficiency, linking system-level behavior to underlying architectural and physical constraints. This integrative approach reinforces the central thesis that meaningful progress in AI computing efficiency emerges from coordinated advances in hardware design and software optimization.",
    "crumbs": [
      "Part II: Introduction and Theoretical Background",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#structure-of-the-thesis",
    "href": "01-intro.html#structure-of-the-thesis",
    "title": "1  Introduction",
    "section": "1.5 Structure of the Thesis",
    "text": "1.5 Structure of the Thesis\nThis thesis is organized into eight chapters, each addressing a specific aspect of the analysis of hardware–software co-design in modern GPU architectures for AI workloads.\nChapter 1 introduces the motivation, challenges, objectives, and methodological foundations of the work. It establishes the context of efficient AI computing under micro- and nanoelectronic constraints and motivates the need for a co-design approach, with particular attention to the evolution of GPU architectures and their relevance to edge AI.\nChapter 2 provides the necessary background and related work. It reviews the evolution of GPU architectures from earlier generations to Hopper and Blackwell, outlines fundamental principles of hardware–software co-design, and discusses the role of General Matrix-Matrix Multiplication (GEMM) as a core computational primitive in AI workloads. Additionally, this chapter surveys relevant programming models, domain-specific languages, and tools developed by NVIDIA Research and the broader AI systems community.\nChapter 3 presents a detailed architectural comparison between NVIDIA Hopper and Blackwell GPUs. It examines key innovations introduced in Blackwell, including advances in tensor computation units, numerical precision formats, memory hierarchy, and interconnect design, and discusses their implications for performance, efficiency, and scalability.\nChapter 4 defines the metrics and models used to evaluate GPU efficiency throughout the thesis. It introduces performance per watt, arithmetic intensity, memory bandwidth utilization, and related concepts, and frames the analysis using the Roofline performance model to distinguish between compute-bound and memory-bound execution regimes.\nChapter 5 focuses on programming models and software frameworks for modern GPUs. It analyzes domain-specific languages and performance-oriented libraries such as Triton, CUTLASS, and JAX-based systems, highlighting their role in exposing architectural features and enabling effective hardware–software co-design.\nChapter 6 describes the experimental methodology and benchmarking setup in detail. It specifies the hardware platforms, software stack, microbenchmark design, measurement techniques, and reproducibility considerations that underpin the experimental results.\nChapter 7 presents and discusses the experimental results. It compares Hopper and Blackwell across multiple metrics, analyzes performance and efficiency trends, and interprets observed bottlenecks in relation to architectural features and software design choices. A roofline-based analysis is used to contextualize results and assess the impact of architectural innovations.\nChapter 8 concludes the thesis by summarizing the main findings and discussing their implications for hardware–software co-design and edge AI systems. It also outlines directions for future work and potential doctoral research avenues building upon the results of this study.\nFinally, the appendices provide supplementary material, including experimental scripts, extended benchmark results, and technical documentation relevant to tensor memory accelerators and GEMM intrinsics, supporting transparency and reproducibility.\n\n\n\n\nChen, Yu-Hsin et al. 2016. “Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks.” IEEE Journal of Solid-State Circuits 52 (1): 127–38. https://doi.org/10.1109/JSSC.2016.2616357.\n\n\nEsmaeilzadeh, Hadi et al. 2011. “Dark Silicon and the End of Multicore Scaling.” ACM SIGARCH Computer Architecture News 39 (3): 365–76. https://doi.org/10.1145/2024723.2000108.\n\n\nHorowitz, Mark. 2014. “Computing’s Energy Problem (and What We Can Do about It).” ISSCC Digest of Technical Papers, 10–14. https://doi.org/10.1109/ISSCC.2014.6757323.\n\n\nJAX Team. 2023. “The JAX ML Scaling Book.” 2023. https://jax-ml.github.io/scaling-book/.\n\n\nJouppi, Norman P. et al. 2017. “In-Datacenter Performance Analysis of a Tensor Processing Unit.” IEEE Computer 50 (2): 58–68. https://doi.org/10.1109/MC.2017.33.\n\n\n——— et al. 2023. “Ten Years of Tensor Processing Units.” Nature 624: 41–51. https://doi.org/10.1038/s41586-023-06777-1.\n\n\nMicikevicius, Paulius et al. 2018. “Mixed Precision Training.” International Conference on Learning Representations (ICLR).\n\n\nNVIDIA. 2022. “NVIDIA Hopper Architecture in-Depth.” NVIDIA Corporation.\n\n\n———. 2024. “NVIDIA Blackwell Architecture.” NVIDIA Corporation.\n\n\nSze, Vivienne et al. 2017. “Efficient Processing of Deep Neural Networks: A Tutorial and Survey.” Proceedings of the IEEE 105 (12): 2295–2329. https://doi.org/10.1109/JPROC.2017.2761740.\n\n\nSze, Vivienne, Yu-Hsin Chen, Tien-Ju Yang, and Joel Emer. 2020. Efficient Processing of Deep Neural Networks. Morgan & Claypool. https://doi.org/10.2200/S01004ED1V01Y202004CAC050.\n\n\nThompson, Neil C., Kristjan Greenewald, Keeheon Lee, and Gabriel F. Manso. 2021. “Computing Beyond Moore’s Law.” Science 371 (6530): eaba7373. https://doi.org/10.1126/science.aba7373.\n\n\nTillet, Philippe, H. T. Kung, and David Cox. 2019. “Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations.” Proceedings of MLSys.\n\n\nWilliams, Samuel, Andrew Waterman, and David Patterson. 2009. “Roofline: An Insightful Visual Performance Model for Multicore Architectures.” Communications of the ACM 52 (4): 65–76. https://doi.org/10.1145/1498765.1498785.",
    "crumbs": [
      "Part II: Introduction and Theoretical Background",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#footnotes",
    "href": "01-intro.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "Dennard scaling refers to the principle formulated by Robert H. Dennard et al. (1974), according to which, as transistor dimensions are reduced, voltage and current scale proportionally, keeping power density approximately constant. This enabled performance improvements across successive technology generations.↩︎\nTensor cores are specialized hardware units introduced by NVIDIA to accelerate dense matrix–matrix operations, particularly mixed-precision matrix multiply–accumulate (MMA) workloads. They are designed to deliver high throughput for deep learning primitives by operating on small matrix tiles using reduced-precision arithmetic.↩︎\nTensor engines refer to more advanced and specialized successors of tensor cores in recent GPU architectures, integrating support for additional low-precision formats (such as FP8 and FP4), higher degrees of specialization, and tighter coupling with data movement and scheduling mechanisms to optimize end-to-end AI workloads.↩︎\nA workload is said to be memory-bound when its performance is primarily limited by memory bandwidth or memory access latency rather than by the available compute throughput. In such cases, increasing arithmetic capability yields limited performance gains unless memory access patterns, data locality, or data movement mechanisms are improved.↩︎\nDomain-specific languages (DSLs) are programming languages designed to express computations within a restricted application domain, providing high-level abstractions that enable domain-aware optimizations while reducing the burden of low-level hardware management for programmers.↩︎\nCompiler frameworks are extensible software infrastructures that support the construction of compilers and code generation pipelines, typically providing intermediate representations, optimization passes, and backend targets. In the context of AI and GPU computing, they facilitate the systematic mapping of high-level program abstractions onto specialized hardware architectures.↩︎\nAttention mechanisms are a class of neural network operations, most prominently used in Transformer architectures, that compute weighted combinations of input features based on pairwise similarity scores (e.g., dot products between queries and keys). Their computational structure involves large matrix multiplications and memory-intensive softmax and normalization steps, making their performance highly sensitive to data locality, memory bandwidth, and efficient data movement.↩︎\nSoftware-managed scratchpads are explicitly addressable on-chip memory regions whose allocation, placement, and data movement are controlled by software rather than by hardware-managed cache policies. They enable predictable latency and bandwidth characteristics, allowing programmers or compilers to optimize data locality and reuse in performance-critical kernels.↩︎\nTensor memory accelerators refer to specialized hardware units designed to optimize data movement and layout transformations for tensor-oriented workloads. These accelerators reduce the overhead of memory access and data rearrangement by offloading common tensor data movement patterns—such as tiling, transposition, and format conversion—from general-purpose execution units.↩︎\nHeterogeneous execution units refer to the coexistence of multiple types of specialized compute units within a single GPU, such as scalar and vector ALUs, tensor cores, ray tracing units, and fixed-function accelerators. Each unit type exhibits distinct performance characteristics and programming constraints, requiring explicit scheduling and workload mapping to achieve high utilization.↩︎\nRoofline model is an analytical performance model that relates the achievable performance of a computing system to an application’s arithmetic intensity (operations per byte transferred). It defines an upper bound on performance through two limiting ceilings: the hardware’s peak compute throughput and its memory bandwidth. The model is used to determine whether an application is compute-bound or memory-bound and to systematically identify architectural performance bottlenecks.↩︎\nTensor computation unit is a specialized hardware execution unit designed to accelerate dense linear algebra operations—most notably matrix–matrix and matrix–vector multiplications—by performing fused multiply–accumulate operations on small matrix tiles in parallel. Tensor computation units are optimized for high throughput and energy efficiency, typically supporting reduced-precision data formats, and are a fundamental building block for accelerating machine learning and AI workloads on modern GPUs.↩︎",
    "crumbs": [
      "Part II: Introduction and Theoretical Background",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-background.html",
    "href": "02-background.html",
    "title": "2  Background and Related Work",
    "section": "",
    "text": "2.1 Evolution of GPU Architectures: From Volta to Blackwell\nThe rapid evolution of artificial intelligence workloads over the last decade has fundamentally reshaped the design space of high-performance and embedded computing systems. GPUs, originally conceived as fixed-function graphics accelerators, have progressively evolved into highly programmable, throughput-oriented processors optimized for data-parallel workloads. This evolution has been driven not only by raw performance demands, but by the changing computational structure of AI models, the growing dominance of data movement and energy constraints, and the breakdown of traditional scaling laws at the device level.\nThis chapter provides the architectural background necessary to contextualize the experimental analysis presented later in this thesis. In particular, it traces the evolution of NVIDIA GPU architectures from Volta to Blackwell, emphasizing how each generation introduces architectural mechanisms explicitly designed to address emerging limitations in efficiency, scalability, and programmability. The goal is to motivate, from a micro and nanoelectronic perspective, why new architectures are required, and why incremental improvements are insufficient to sustain AI performance growth.",
    "crumbs": [
      "Part II: Introduction and Theoretical Background",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background and Related Work</span>"
    ]
  },
  {
    "objectID": "02-background.html#evolution-of-gpu-architectures-from-volta-to-blackwell",
    "href": "02-background.html#evolution-of-gpu-architectures-from-volta-to-blackwell",
    "title": "2  Background and Related Work",
    "section": "",
    "text": "2.1.1 Volta: Tensor Cores and the Shift Toward AI-Centric Design\nThe introduction of the Volta architecture marked a structural inflection point in GPU design. While earlier architectures primarily focused on graphics and general-purpose throughput, Volta (V100) was the first NVIDIA GPU to explicitly integrate Tensor Cores, specialized units designed to accelerate dense matrix-matrix multiplication (GEMM) with mixed precision arithmetic. This design decision reflected the recognition that AI workloads—particularly deep neural networks—are dominated by linear algebra primitives rather than traditional scalar or vector operations (NVIDIA 2017).\nFrom a microarchitectural standpoint, Tensor Cores represented a move toward domain-specific acceleration within a general-purpose framework. Instead of relying solely on CUDA cores executing fused multiply–add instructions, Volta introduced wide, deeply pipelined datapaths optimized for matrix operations. This significantly improved throughput and energy efficiency for AI workloads, but also established a new dependency: software had to be explicitly adapted to exploit these units, initiating a tighter coupling between architecture and programming models.\n\n\n2.1.2 Turing and Ampere: Maturation and Generalization of Tensor Acceleration\nSubsequent architectures, notably Turing and Ampere, refined and generalized the Tensor Core concept. Ampere (A100) expanded Tensor Core support to additional precisions (TF32, BF16) and improved memory hierarchy characteristics, including larger caches and higher-bandwidth memory subsystems (NVIDIA 2020).\nThese changes addressed a growing mismatch between compute capability and data movement. As AI models increased in size and depth, memory bandwidth and latency increasingly constrained performance, reinforcing the insight that raw compute scaling alone is insufficient. From a design perspective, this period reflects an architectural response to the early manifestations of the memory wall in AI workloads, with incremental improvements in on-chip storage and off-chip bandwidth.\nHowever, despite these advances, Ampere largely preserved the existing execution and programming paradigms. While effective for data center workloads, this continuity limited the ability to exploit more aggressive forms of parallelism and asynchronous execution, particularly under strict power and latency constraints relevant to Edge AI.\n\n\n2.1.3 Hopper: Asynchronous Execution and Data Movement Awareness\nThe Hopper architecture (H100) represents a qualitative shift from compute-centric optimization toward data-movement-aware architecture. While Hopper continues to enhance Tensor Core throughput and precision support (including FP8), its most significant innovations address how data is transferred, scheduled, and synchronized across the chip (NVIDIA 2022b, 2022a).\nKey features such as thread block clusters1 and the Tensor Memory Accelerator (TMA)2 enable more explicit control over data movement between global memory and shared memory, allowing software to overlap computation and communication more effectively. From a microelectronic standpoint, this reflects an architectural acknowledgment that memory accesses—not arithmetic—dominate both latency and energy consumption.\nHopper therefore deepens the hardware–software contract: performance is no longer achieved automatically by launching enough threads, but by structuring computation to align with architectural primitives. This shift is particularly relevant for microbenchmarking studies, as it exposes how architectural features translate into achievable efficiency only under specific software conditions.\n\n\n2.1.4 Blackwell: Architecture for Scaling, Efficiency, and Reduced Precision\nBlackwell (B200) extends the Hopper philosophy further, motivated by the explosive growth of large language models (LLMs) and Mixture-of-Experts3 architectures. These models stress in compute, memory and also system-level scalability and energy efficiency, pushing existing architectural paradigms to their limits (NVIDIA 2025d).\nA defining characteristic of Blackwell is its support for more aggressive reduced-precision formats, including FP4 with microscaling (e.g., NVFP4), alongside second-generation Transformer Engine support (NVIDIA 2025c). These formats significantly reduce data movement and storage requirements, directly addressing energy and bandwidth constraints. However, they also impose stringent requirements on numerical handling, accumulation, and software tooling.\nFrom a design perspective, Blackwell illustrates why new architectures are necessary: emerging AI workloads cannot be efficiently mapped onto legacy datapaths, memory organizations, or precision assumptions without incurring unacceptable inefficiencies. Supporting FP4-scale computation efficiently requires rethinking datapath widths, register file organization, memory packing, and error management—decisions that must be made at the micro and nanoelectronic design level.\n\n\n2.1.5 Why New Architectures Are Necessary\nThe progression from Volta to Blackwell highlights a fundamental conclusion: architectural evolution is driven less by peak performance targets and more by efficiency, scalability, and workload structure. Several converging factors necessitate new architectures:\n\nBreakdown of traditional scaling laws, which prevents frequency or voltage scaling from delivering “free” performance gains (Bohr 2007).\nDominance of data movement costs, making memory hierarchy and interconnect design central to performance and energy efficiency.\nEmergence of new numerical formats, which require native hardware support to be both efficient and numerically robust.\nIncreasing software complexity, which demands architectures that expose controllable primitives for scheduling and data movement rather than opaque execution.\n\nThese pressures are not limited to data centers. In Edge AI, where power, area, and thermal budgets are far tighter, the inefficiencies tolerated in large systems become prohibitive. As a result, insights gained from architectures such as Hopper and Blackwell are directly relevant to the design of future edge accelerators, reinforcing the importance of micro and nanoelectronic expertise in shaping next-generation AI systems.",
    "crumbs": [
      "Part II: Introduction and Theoretical Background",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background and Related Work</span>"
    ]
  },
  {
    "objectID": "02-background.html#programming-models-and-software-stacks-for-modern-gpus",
    "href": "02-background.html#programming-models-and-software-stacks-for-modern-gpus",
    "title": "2  Background and Related Work",
    "section": "2.2 Programming Models and Software Stacks for Modern GPUs",
    "text": "2.2 Programming Models and Software Stacks for Modern GPUs\nAs GPU architectures have evolved from general-purpose throughput processors to highly specialized accelerators for AI, the programming models and software stacks have become a decisive factor in determining achievable performance and efficiency. Architectural features such as Tensor Cores, asynchronous data movement engines, and reduced-precision datapaths only deliver their theoretical benefits if they are explicitly exposed and exploited by software. Consequently, the evolution of GPU programming models mirrors the architectural transition discussed in Section 2.1, reinforcing the necessity of hardware–software co-design.\nThis section reviews the evolution of GPU programming abstractions and software ecosystems—from CUDA and low-level libraries to modern AI-centric frameworks—and explains why increasingly complex software stacks are required to fully utilize architectures such as Hopper and Blackwell.\n\n2.2.1 CUDA as the Foundation: Explicit Parallelism and Memory Hierarchy\nCUDA remains the foundational programming model for NVIDIA GPUs. Its design exposes a hierarchical execution and memory model (threads, warps, thread blocks; registers, shared memory, global memory) that maps closely to GPU microarchitecture. This explicitness has been instrumental in enabling high performance, but it also places a significant burden on the programmer or compiler to manage locality, synchronization, and parallelism efficiently (NVIDIA 2025b).\nFrom a micro and nanoelectronic perspective, CUDA’s memory hierarchy abstraction directly reflects physical design trade-offs: shared memory corresponds to on-chip SRAM with low latency and high energy efficiency, while global memory accesses map to off-chip DRAM or HBM with far higher energy cost. As architectures scale, the gap between these levels widens, making correct software usage increasingly critical.\nWhile CUDA has remained relatively stable conceptually, each new architecture extends it with additional primitives (e.g., warp-level operations, asynchronous copies, cluster-level execution). These extensions illustrate a key point: architectural innovation increasingly requires new software constructs, rather than transparent performance gains.\n\n\n2.2.2 Library-Centric Optimization: cuBLAS, cuDNN, and Kernel Specialization\nFor most AI practitioners, direct CUDA kernel development is abstracted away through highly optimized libraries such as cuBLAS (dense linear algebra) and cuDNN (deep neural networks). These libraries encode architecture-specific knowledge—tiling strategies, memory layouts, pipeline scheduling—that would be impractical to reproduce manually for each application (NVIDIA 2025a, 2025e).\nFrom a co-design standpoint, these libraries serve as a critical translation layer between hardware capabilities and application-level performance. For example, Tensor Core utilization depends not only on hardware availability, but on precise data layouts, alignment, and precision choices implemented within library kernels. As new precisions (FP8, FP4) and data movement mechanisms (e.g., TMA) are introduced, library implementations must be redesigned accordingly.\nThis dependency underscores why new architectures cannot rely on legacy software stacks: without corresponding library evolution, hardware innovations remain underutilized. In Edge AI contexts, where custom accelerators or constrained GPUs may lack mature libraries, this challenge is even more pronounced.\n\n\n2.2.3 Transformer Engine and Precision-Aware Software\nThe emergence of Transformer Engine (TE)4 represents a shift toward architecture-aware, model-specific software. Rather than providing generic linear algebra primitives, TE integrates numerical precision management, scaling policies, and kernel selection specifically optimized for Transformer workloads (NVIDIA 2025f).\nThis approach reflects a deeper level of co-design. Hardware introduces new numerical formats (e.g., FP8 in Hopper, FP4/NVFP4 in Blackwell), while software encodes domain knowledge about which layers tolerate reduced precision and how to manage accumulation and scaling. From a microelectronic design viewpoint, this tight coupling justifies the inclusion of specialized datapaths and control logic that would be inefficient or unused under a generic programming model.\nIn Edge AI, similar ideas are increasingly adopted through quantization-aware runtimes and compiler toolchains, reinforcing the notion that precision is a cross-layer design decision, not merely a software optimization.\n\n\n2.2.4 Compiler and Framework-Level Abstractions: JAX, XLA, and Beyond\nAt a higher abstraction level, modern AI development increasingly relies on compiler-driven frameworks such as JAX, PyTorch, and TensorFlow. These frameworks delegate kernel fusion, operation scheduling, and memory planning to intermediate representations and optimizing compilers (e.g., XLA), aiming to generate hardware-efficient code automatically (Austin et al. 2025a).\nThe JAX ML Scaling Book highlights how performance at scale depends on understanding both the algorithmic structure of models and the characteristics of the underlying hardware, including communication and memory costs. This reinforces a central theme of this thesis: efficient execution emerges from coordinated decisions across abstraction layers, not from isolated optimizations.\nHowever, compiler-based approaches also introduce challenges. Automatically generated code must target increasingly complex architectural features (asynchronous execution, cluster-level parallelism), which may be difficult to infer without explicit annotations or architectural hints. This tension motivates continued exposure of low-level primitives alongside high-level abstractions, particularly for research and microbenchmarking purposes.\n\n\n2.2.5 Implications for Edge AI and Microbenchmarking\nFor Edge AI, the complexity of modern software stacks presents both an opportunity and a challenge. On one hand, advanced libraries and compilers can hide architectural complexity and deliver efficient execution on constrained devices. On the other hand, limited resources and heterogeneous hardware often require explicit control and customization.\nIn this context, microbenchmarking plays a crucial role. By isolating kernels such as GEMM and implementing them across different programming layers (raw CUDA, library calls, framework-generated code), it becomes possible to:\n\nQuantify the gap between theoretical peak and achieved performance,\nIdentify whether bottlenecks arise from hardware limits or software inefficiencies,\nEvaluate how architectural features are exposed—or obscured—by the software stack.\n\nThese insights are essential for assessing the true impact of architectural innovations from Volta through Blackwell and for translating lessons learned to future edge-oriented accelerators.",
    "crumbs": [
      "Part II: Introduction and Theoretical Background",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background and Related Work</span>"
    ]
  },
  {
    "objectID": "02-background.html#hardware-software-co-design-case-studies",
    "href": "02-background.html#hardware-software-co-design-case-studies",
    "title": "2  Background and Related Work",
    "section": "2.3 Hardware-Software Co-Design Case Studies",
    "text": "2.3 Hardware-Software Co-Design Case Studies\nHaving reviewed the architectural evolution of GPUs and the corresponding programming models, this section presents concrete case studies that exemplify how hardware–software co-design materializes in practice. Rather than abstract principles, these cases focus on representative AI workloads whose performance and efficiency are tightly coupled to microarchitectural features and software decisions. The selected case studies—GEMM5, Transformer attention6, and Mixture-of-Experts7 (MoE)—are directly relevant to modern AI systems and provide a clear bridge to the experimental microbenchmarking work developed later in this thesis.\nFrom a micro and nanoelectronic perspective, these cases illustrate how architectural features justify their area, power, and design complexity only when matched by appropriate software abstractions. Conversely, they also show why new architectures are required when existing ones can no longer support emerging workload characteristics efficiently.\n\n2.3.1 GEMM as the Canonical Co-Design Primitive\nGeneral Matrix–Matrix Multiplication (GEMM) is the foundational kernel underlying the majority of compute in deep learning models, including linear layers8, projections in attention mechanisms9, and MLP blocks10. Its importance stems from two properties: it is both compute-intensive and highly sensitive to data movement.\nFrom the hardware side, architectures from Volta onward have introduced Tensor Cores explicitly optimized for GEMM, enabling orders-of-magnitude higher throughput per watt compared to traditional SIMD execution11 (NVIDIA 2017, 2022a). However, achieving peak Tensor Core performance requires careful software orchestration:\n\nData must be tiled to fit on-chip memories (registers and shared memory),\nMemory accesses must be aligned and coalesced,\nComputation and data transfers must be overlapped to hide latency.\n\nThese requirements highlight a central co-design insight: Tensor Cores alone do not guarantee performance. Only when software kernels are structured to match the microarchitectural pipeline—often through hand-tuned libraries such as cuBLAS—does the hardware investment translate into effective performance (NVIDIA 2025a).\nFor this reason, GEMM is a primary target for microbenchmarking in this thesis. By driving the hardware toward the compute-bound Roofline regime, GEMM exposes the true computational ceiling of each architecture and reveals how features such as asynchronous data movement (e.g., TMA in Hopper) affect achievable efficiency.\n\n\n2.3.2 Transformer Attention: Balancing Compute, Memory, and Precision\nWhile GEMM captures the compute core of AI workloads, Transformer attention mechanisms illustrate a more complex co-design challenge involving irregular data access patterns, intermediate storage, and numerical stability. Attention layers combine matrix multiplications with softmax operations, normalization, and reductions, making them sensitive to both compute throughput and memory bandwidth.\nArchitecturally, Hopper and Blackwell address these challenges through a combination of higher Tensor Core throughput, expanded shared memory capabilities, and support for reduced-precision formats (FP8, FP4). However, attention performance depends critically on software techniques such as kernel fusion and precision-aware computation (NVIDIA 2025f).\nThe introduction of optimized attention kernels (e.g., fused attention) demonstrates co-design in action: software reorganizes the computation to minimize memory traffic and exploit on-chip storage, while hardware provides sufficient flexibility and bandwidth to support these fused execution patterns. Without such coordination, attention layers quickly become memory-bound, negating the benefits of increased compute capacity.\nFor Edge AI, attention exemplifies why architectural generality is insufficient. Efficient execution under tight power and latency constraints requires architectures that anticipate such fused, domain-specific workloads and software stacks capable of exploiting them.\n\n\n2.3.3 Mixture-of-Experts: Sparsity, Routing, and System-Level Co-Design\nMixture-of-Experts (MoE) models represent a further escalation in architectural demands. By activating only a subset of parameters per input, MoE introduces conditional computation and sparsity at scale. While this reduces average compute per token, it significantly increases pressure on memory systems and interconnects due to expert routing and load imbalance.\nFrom a hardware perspective, MoE stresses not only compute units but also memory bandwidth, cache coherence, and communication fabrics. Blackwell explicitly targets these workloads by combining aggressive reduced-precision formats with architectural features designed for large-scale parallelism and efficient data movement (NVIDIA 2025d).\nSoftware frameworks must orchestrate expert selection, data routing, and synchronization across devices, often relying on compiler-level transformations and runtime scheduling policies. The JAX ML Scaling Book discusses how such system-level considerations dominate performance as models scale, reinforcing that co-design must extend beyond the chip to the system level (Austin et al. 2025b).\n\n\n2.3.4 Lessons for Micro and Nanoelectronic Design\nAcross these case studies, several recurring themes emerge that are directly relevant to micro and nanoelectronic design:\n\nSpecialization is unavoidable. General-purpose execution units cannot efficiently sustain the performance and energy demands of modern AI workloads.\nData movement dominates energy and performance. Architectural resources devoted to computation must be matched by proportional investment in memory hierarchy and interconnect design.\nPrecision is a hardware concern. Supporting FP8 and FP4 efficiently requires dedicated datapaths and storage structures, not emulation on wider formats.\nSoftware defines realizable efficiency. Architectural features only justify their silicon cost when software exposes and exploits them effectively.\n\nThese insights reinforce the central thesis motivation: new architectures such as Hopper and Blackwell are not incremental upgrades, but necessary responses to fundamental shifts in workload structure and physical constraints. Microbenchmarking GEMM and related kernels provides a rigorous methodology to evaluate how well these co-designed systems meet their goals.",
    "crumbs": [
      "Part II: Introduction and Theoretical Background",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background and Related Work</span>"
    ]
  },
  {
    "objectID": "02-background.html#general-matrix-matrix-multiplication-gemm-in-ai-workloads",
    "href": "02-background.html#general-matrix-matrix-multiplication-gemm-in-ai-workloads",
    "title": "2  Background and Related Work",
    "section": "2.4 General Matrix-Matrix Multiplication (GEMM) in AI Workloads",
    "text": "2.4 General Matrix-Matrix Multiplication (GEMM) in AI Workloads\nGeneral Matrix–Matrix Multiplication (GEMM), defined as \\(C = \\alpha AB + \\beta C\\), is the fundamental computational primitive underpinning the majority of modern AI workloads. Despite its apparent simplicity, GEMM captures the essential tension between compute throughput, memory bandwidth, and numerical precision that defines efficient execution on contemporary accelerators. For this reason, GEMM serves not only as a building block for higher-level operations, but also as a diagnostic kernel for evaluating hardware–software co-design.\nIn the context of this thesis, GEMM is used as the primary microbenchmark to analyze and compare NVIDIA Hopper and Blackwell architectures, as it directly exercises Tensor Cores, memory hierarchies, and reduced-precision datapaths.\n\n2.4.1 GEMM as the Computational Core of AI Models\nMost layers in deep neural networks can be expressed as matrix multiplications. In Transformers, for example, GEMM dominates:\n\nLinear projections for queries, keys, and values.\nFeed-forward network (MLP) layers.\nOutput projections and embedding transformations.\n\nAs a result, the performance and energy efficiency of GEMM strongly correlate with end-to-end model throughput and cost. This centrality has motivated decades of optimization research and has directly influenced accelerator architecture design. NVIDIA’s introduction of Tensor Cores in Volta and their subsequent evolution in Ampere, Hopper, and Blackwell reflect the recognition that optimizing GEMM yields disproportionate benefits for AI workloads (NVIDIA 2017, 2022a).\nFrom a micro and nanoelectronic perspective, GEMM is attractive because it exhibits high arithmetic intensity when data reuse is maximized, making it well suited for dense on-chip computation. However, achieving this theoretical advantage in practice requires careful coordination between hardware capabilities and software implementation.\n\n\n2.4.2 Arithmetic Intensity and the Roofline Perspective\nThe efficiency of GEMM can be formally analyzed using the Roofline model, which relates achieved performance to arithmetic intensity (operations per byte transferred) and available memory bandwidth (Williams, Waterman, and Patterson 2009). GEMM is one of the few kernels capable of reaching the compute-bound regime on modern GPUs, provided that data is sufficiently reused through blocking and tiling strategies.\nThis property makes GEMM particularly valuable for architectural evaluation. If a well-optimized GEMM kernel fails to approach peak performance, the limiting factor is likely architectural (e.g., insufficient compute throughput, pipeline inefficiencies, or suboptimal memory hierarchy design) rather than algorithmic. Consequently, GEMM microbenchmarks are widely used to assess the effective peak performance of accelerators.\nIn Edge AI scenarios, arithmetic intensity is often reduced due to smaller problem sizes and limited on-chip memory, increasing the likelihood of memory-bound execution. This reinforces the need for architectures and software stacks that can adapt GEMM execution strategies across a wide range of operating points.\n\n\n2.4.3 Data Movement and Memory Hierarchy Considerations\nAlthough GEMM is compute-intensive in principle, its realized performance is highly sensitive to data movement. Efficient implementations rely on:\n\nBlocking matrices to fit into registers and shared memory.\nReusing operands across multiple multiply–accumulate operations.\nOverlapping data transfers with computation.\n\nFrom a hardware standpoint, this places stringent requirements on the memory hierarchy: sufficient register file bandwidth, low-latency shared memory, and high-throughput connections to off-chip memory (e.g., HBM). Hopper introduces explicit mechanisms, such as asynchronous memory operations and the Tensor Memory Accelerator (TMA), to reduce the overhead of moving data into on-chip storage (NVIDIA 2022b).\nHowever, these mechanisms are not transparent. Software must be structured to exploit them, which further strengthens the argument for GEMM as a co-design case study: its performance reflects raw hardware capability and also the maturity and quality of the software stack.\n\n\n2.4.4 GEMM as a Microbenchmarking Tool\nBeyond its role in applications, GEMM is uniquely suited as a microbenchmark for architectural analysis. Its regular structure allows precise control over problem size, data layout, and precision, enabling systematic exploration of performance regimes.\nIn this thesis, GEMM microbenchmarking serves multiple purposes:\n\nEstimating the effective peak performance of Hopper and Blackwell.\nIdentifying transitions between memory-bound and compute-bound regimes.\nEvaluating the impact of architectural features such as Tensor Cores, TMA, and reduced-precision support.\n\nBy correlating empirical results with Roofline models and architectural specifications, GEMM benchmarks provide a rigorous basis for comparing architectures and for understanding how hardware–software co-design choices translate into realized efficiency.",
    "crumbs": [
      "Part II: Introduction and Theoretical Background",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background and Related Work</span>"
    ]
  },
  {
    "objectID": "02-background.html#domain-specific-languages-dsls-for-gpu-programming",
    "href": "02-background.html#domain-specific-languages-dsls-for-gpu-programming",
    "title": "2  Background and Related Work",
    "section": "2.5 Domain-Specific Languages (DSLs) for GPU Programming",
    "text": "2.5 Domain-Specific Languages (DSLs) for GPU Programming\nAs GPU architectures evolve toward increasingly specialized and asynchronous execution models, the gap between architectural capability and software productivity widens. Traditional CUDA programming exposes low-level control but demands deep microarchitectural expertise to achieve peak efficiency, particularly for kernels such as GEMM that must carefully orchestrate data movement, tiling, and synchronization. Domain-Specific Languages (DSLs) for GPU programming have emerged to bridge this gap by providing higher-level abstractions that encode architectural knowledge while retaining fine-grained control over performance-critical details.\nIn the context of this thesis, DSLs are especially relevant because they embody hardware–software co-design principles: they are explicitly designed around GPU execution models, memory hierarchies, and tensor acceleration units. This section reviews representative DSLs for GPU programming, with particular emphasis on TK, TileLang, CUTE (CUTLASS), and Gluon, which are most closely aligned with microarchitectural concerns and GEMM-style workloads.\n\n2.5.1 Motivation for DSLs in GPU Computing\nThe primary motivation for GPU DSLs is not abstraction for its own sake, but controlled specialization. Modern AI kernels require:\n\nPrecise tiling and data layout choices.\nExplicit management of shared memory and registers.\nOverlap of computation and data movement.\nEfficient use of Tensor Cores and reduced-precision formats.\n\nEncoding these decisions directly in CUDA leads to code that is complex, brittle, and architecture-specific. DSLs address this challenge by providing structured representations of computation and data movement that can be systematically mapped to hardware. From a micro and nanoelectronic perspective, DSLs act as software counterparts to architectural primitives, enabling efficient exploitation of silicon features without sacrificing portability or maintainability.\n\n\n2.5.2 Triton: Productivity-Oriented Kernel Specialization\nTriton is a Python-embedded DSL designed to simplify the development of custom GPU kernels while achieving performance comparable to hand-written CUDA. It exposes a programming model centered on blocks and tiles, abstracting away some low-level details such as thread indexing while retaining explicit control over memory access patterns and parallelism (Tillet, Kung, and Cox 2021).\nTriton has been particularly successful for rapid prototyping of GEMM and attention kernels in AI frameworks. However, its abstraction level prioritizes productivity and compiler-driven optimization, which can limit explicit control over certain microarchitectural features (e.g., fine-grained shared memory scheduling). As such, Triton is representative of DSLs that trade maximal control for ease of use.\n\n\n2.5.3 TK (Tensor Kernel): Explicit Co-Design with Tensor Cores\nTK (Tensor Kernel) represents a class of DSLs explicitly designed around Tensor Core-centric computation. Rather than treating tensor acceleration as an opaque backend feature, TK exposes tensor-level operations and tiling strategies as first-class constructs. This design aligns closely with the execution model of modern GPUs, where performance is dominated by how effectively Tensor Cores are fed with data.\nFrom a co-design standpoint, TK is notable because it:\n\nEncodes Tensor Core tile shapes and data layouts directly in the language.\nMakes register, and shared-memory, level data reuse explicit.\nEncourages software structures that map naturally to hardware pipelines.\n\nThis explicitness is highly relevant for microbenchmarking and architectural evaluation, as it allows controlled experimentation with different tiling and scheduling strategies while preserving a clear mapping to microarchitectural resources. In this sense, TK functions as a research-oriented DSL, enabling exploration of architectural trade-offs without dropping to raw CUDA.\n\n\n2.5.4 TileLang: Tiling as a First-Class Abstraction\nTileLang pushes the DSL concept further by elevating tiling—the central optimization strategy in GEMM and many AI kernels—to a first-class abstraction. Instead of expressing computation in terms of scalar or vector operations, TileLang structures programs around hierarchical tiles that correspond directly to GPU execution levels (thread, warp, block, cluster).\nThis approach is particularly powerful from a hardware–software co-design perspective:\n\nTiling decisions directly reflect on-chip memory capacities and bandwidth.\nHierarchical tiles map naturally to registers, shared memory, and global memory.\nThe language structure mirrors the physical organization of the GPU.\n\nFor micro and nanoelectronic design, TileLang is interesting because it makes the constraints of the memory hierarchy explicit in the software representation. This transparency facilitates reasoning about why certain architectural features (e.g., larger shared memory, faster on-chip interconnects) translate into performance gains for specific workloads.\n\n\n2.5.5 CUTE (CUTLASS): Template-Based Architectural Specialization\nCUTE, a core component of NVIDIA’s CUTLASS library, is a C++ template-based DSL designed to express high-performance tensor operations in a way that is both generic and architecture-aware (NVIDIA 2023). Rather than a standalone language, CUTE provides abstractions for tensor layouts, tiling, and data movement that are resolved at compile time.\nCUTE is particularly significant because it underpins many production-grade GEMM kernels used in cuBLAS and related libraries. Its design reflects deep microarchitectural insight:\n\nTensor layouts encode memory coalescing and alignment constraints,\nTiling abstractions map directly to Tensor Core shapes,\nCompile-time specialization enables zero-overhead abstractions.\n\nFrom a co-design perspective, CUTE exemplifies how DSL concepts can be embedded within systems-level programming languages to achieve both performance and portability. It also illustrates why new architectures require corresponding software evolution: changes in Tensor Core shapes or memory hierarchy necessitate new template instantiations and layout strategies.\n\n\n2.5.6 Gluon: Bridging DSLs and High-Level Frameworks\nGluon occupies a complementary position in the DSL landscape by focusing on integration with higher-level frameworks while preserving the ability to express hardware-efficient kernels. Rather than targeting only kernel authors, Gluon aims to connect model-level abstractions with low-level execution strategies.\nThis bridging role is particularly relevant for Edge AI and research workflows, where rapid iteration and deployment matter as much as raw performance. By enabling DSL-defined kernels to be composed within larger computational graphs, Gluon supports end-to-end co-design: architectural constraints influence kernel structure, which in turn informs model design choices.\nFrom a microarchitectural viewpoint, Gluon demonstrates how DSLs can propagate hardware-awareness upward in the software stack, influencing kernel performance and algorithmic structure and deployment decisions.\n\n\n2.5.7 Pallas and the Broader DSL Ecosystem\nPallas, developed within the JAX ecosystem, represents another approach to GPU DSLs by integrating kernel-level control into a functional, compiler-driven framework (Google 2023). While powerful, Pallas emphasizes compiler optimization over explicit microarchitectural control, positioning it differently from DSLs such as TK, TileLang, and CUTE.\nCollectively, these DSLs illustrate a spectrum of design points:\n\nProductivity-oriented (Triton, Pallas),\nArchitecture-explicit and research-focused (TK, TileLang),\nProduction-grade and template-specialized (CUTE),\nFramework-bridging (Gluon).",
    "crumbs": [
      "Part II: Introduction and Theoretical Background",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background and Related Work</span>"
    ]
  },
  {
    "objectID": "02-background.html#roofline-model-and-performance-metrics",
    "href": "02-background.html#roofline-model-and-performance-metrics",
    "title": "2  Background and Related Work",
    "section": "2.6 Roofline Model and Performance Metrics",
    "text": "2.6 Roofline Model and Performance Metrics\nTo rigorously evaluate the efficiency of modern GPU architectures, raw peak specifications (e.g., TFLOPS) are insufficient. What ultimately matters is how close real workloads can approach these peaks under practical constraints imposed by memory bandwidth, data movement, and power. The Roofline model provides a unifying analytical framework to reason about these limits by linking algorithmic characteristics to architectural capabilities.\nIn this thesis, the Roofline model is adopted as the primary performance analysis tool to interpret GEMM microbenchmark results on Hopper and Blackwell architectures. Beyond peak performance, complementary metrics such as performance per watt and memory efficiency are introduced to reflect constraints that are especially relevant for Edge AI.\n\n2.6.1 The Roofline Model: Fundamentals\nThe Roofline model characterizes the maximum attainable performance of a kernel as a function of its arithmetic intensity (AI), defined as the ratio of floating-point operations to bytes transferred from main memory (Williams, Waterman, and Patterson 2009). Formally,\n\\[\n\\text{Performance} \\leq \\min \\left( P_{\\text{peak}}, \\text{AI} \\times B_{\\text{mem}} \\right)\n\\]\nwhere (\\(P_{\\text{peak}}\\)) is the peak compute throughput of the processor and (\\(B_{\\text{mem}}\\)) is the achievable memory bandwidth.\nGraphically, the model is represented as a piecewise linear curve:\n\n\n\n\n\n\nFigure 2.1: Roofline Model\n\n\n\n\nA sloped region, where performance is limited by memory bandwidth (memory-bound regime),\nA flat roof, where performance is limited by compute throughput (compute-bound regime).\n\nThe intersection point between these regions defines the minimum arithmetic intensity required to fully utilize the compute units.\nFrom a microarchitectural perspective, Roofline exposes the balance—or imbalance—between compute resources and the memory subsystem. Architectural evolution from Volta to Blackwell can be interpreted as a continuous effort to raise both the roof (through higher Tensor Core throughput) and the slope (through increased memory bandwidth and improved data movement mechanisms).\n\n\n2.6.2 Arithmetic Intensity in GEMM and AI Kernels\nGEMM is particularly well suited for Roofline analysis because its arithmetic intensity is analytically tractable and tunable through blocking strategies. For an ideal GEMM implementation, arithmetic intensity increases with matrix size and effective data reuse, enabling execution in the compute-bound regime on sufficiently large problems.\nHowever, real implementations deviate from this ideal due to finite on-chip memory, imperfect reuse, and overheads associated with synchronization and data transfers. These effects become more pronounced in Edge AI, where smaller batch sizes and reduced memory capacity often force GEMM into the memory-bound regime.\nBy systematically varying problem sizes and data layouts, GEMM microbenchmarks allow direct observation of transitions between Roofline regimes. This capability is central to the experimental methodology of this thesis, as it enables attribution of performance gaps to either compute limitations or memory inefficiencies.\n\n\n2.6.3 Beyond a Single Roof: Hierarchical Roofline Models\nModern GPUs feature multi-level memory hierarchies, including registers, shared memory, L2 cache, and off-chip HBM. A single Roofline curve based on off-chip bandwidth may therefore obscure important bottlenecks at intermediate levels.\nHierarchical or multi-roof Roofline models extend the original framework by introducing separate bandwidth ceilings for different memory levels (Williams, Waterman, and Patterson 2009). This approach is particularly relevant for architectures such as Hopper and Blackwell, where explicit data movement between global and shared memory (e.g., via TMA) can significantly alter effective bandwidth.\nFrom a co-design perspective, hierarchical Roofline analysis highlights where software must intervene to exploit on-chip locality. If performance is bounded by shared memory bandwidth rather than HBM, architectural investment in Tensor Cores cannot be fully realized without corresponding kernel restructuring.\n\n\n2.6.4 Performance per Watt and Energy-Aware Metrics\nWhile the Roofline model focuses on performance limits, energy efficiency is a first-order constraint in both data center and Edge AI contexts. Performance per watt (GFLOPS/W) provides a direct measure of how effectively an architecture converts power into useful computation.\nArchitectural features such as reduced-precision Tensor Cores and high-bandwidth on-chip memories aim to improve this metric by reducing energy per operation and per byte moved (NVIDIA 2022a, 2025d). However, as with raw performance, these gains are only realized when software achieves high utilization.\nIn Edge AI, performance per watt often dominates over absolute performance. Consequently, Roofline analysis in this thesis is complemented by energy-normalized metrics to capture trade-offs between throughput and power consumption, providing a more holistic evaluation of architectural efficiency.\n\n\n2.6.5 Memory Efficiency and Bandwidth Utilization\nAnother critical metric is memory efficiency, defined as the fraction of theoretical peak bandwidth that is actually achieved by a kernel. Low bandwidth utilization may indicate suboptimal access patterns, insufficient concurrency, or architectural mismatches between memory and compute.\nFor GEMM, high memory efficiency is typically achieved only when data reuse is maximized and transfers are overlapped with computation. Hopper’s asynchronous data movement primitives and Blackwell’s further enhancements are explicitly designed to raise effective bandwidth utilization, but their impact must be quantified empirically.\nMemory efficiency metrics are particularly relevant for Edge AI accelerators, where bandwidth is scarce and often shared across heterogeneous components. Insights gained from high-end GPU analysis can therefore inform the design of more balanced edge-oriented architectures.\n\n\n2.6.6 Roofline as a Tool for Architectural Comparison\nOne of the strengths of the Roofline model is its suitability for cross-architecture comparison. By normalizing performance against architectural ceilings, it becomes possible to compare how efficiently different generations (e.g., Hopper vs. Blackwell) exploit their available resources.\nIn this thesis, Roofline plots are used to:\n\nVisualize achievable performance relative to theoretical limits,\nIdentify shifts in bottlenecks between architectures,\nAssess whether architectural innovations effectively translate into higher realized efficiency.\n\nThis analytical approach aligns naturally with the goals of micro- and nanoelectronic design: evaluating whether increased silicon complexity and power budget deliver proportional gains under realistic workloads.\n\n\n\n\nAustin, Jacob, Sholto Douglas, Roy Frostig, et al. 2025a. “How to Scale Your Model.” Online Book; Google DeepMind.\n\n\n———, et al. 2025b. “How to Scale Your Model.” Online Book; Google DeepMind.\n\n\nBohr, Mark. 2007. “A 30 Year Retrospective on Dennard’s MOSFET Scaling Paper.” IEEE.\n\n\nGoogle. 2023. “Pallas: A JAX Kernel Language.” Technical Documentation.\n\n\nNVIDIA. 2017. “NVIDIA Tesla V100 GPU Architecture.” Whitepaper.\n\n\n———. 2020. “NVIDIA A100 Tensor Core GPU Architecture.” Whitepaper.\n\n\n———. 2022a. “NVIDIA H100 Tensor Core GPU Architecture.” Whitepaper.\n\n\n———. 2022b. “NVIDIA Hopper Architecture in-Depth.” NVIDIA Technical Blog.\n\n\n———. 2023. “CUTLASS: CUDA Templates for Linear Algebra Subroutines.” GitHub Repository.\n\n\n———. 2025a. “cuBLAS Library User Guide.” NVIDIA Documentation.\n\n\n———. 2025b. “CUDA c++ Programming Guide.” NVIDIA Documentation.\n\n\n———. 2025c. “Introducing NVFP4 for Efficient and Accurate Low-Precision Inference.” NVIDIA Technical Blog.\n\n\n———. 2025d. “NVIDIA Blackwell Architecture.” NVIDIA Technology Overview.\n\n\n———. 2025e. “NVIDIA cuDNN Documentation.” NVIDIA Documentation.\n\n\n———. 2025f. “Transformer Engine Documentation.” NVIDIA Documentation.\n\n\nTillet, Philippe, H. T. Kung, and David Cox. 2021. “Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations.” Proceedings of MLSys.\n\n\nWilliams, Samuel, Andrew Waterman, and David Patterson. 2009. “Roofline: An Insightful Visual Performance Model for Multicore Architectures.” Communications of the ACM 52 (4): 65–76. https://doi.org/10.1145/1498765.1498785.",
    "crumbs": [
      "Part II: Introduction and Theoretical Background",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background and Related Work</span>"
    ]
  },
  {
    "objectID": "03-architecture.html",
    "href": "03-architecture.html",
    "title": "3  Architecture Comparison: Hopper vs Blackwell",
    "section": "",
    "text": "3.1 Overview of Hopper Architecture",
    "crumbs": [
      "Part III: System Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Architecture Comparison: Hopper vs Blackwell</span>"
    ]
  },
  {
    "objectID": "03-architecture.html#overview-of-blackwell-architecture",
    "href": "03-architecture.html#overview-of-blackwell-architecture",
    "title": "3  Architecture Comparison: Hopper vs Blackwell",
    "section": "3.2 Overview of Blackwell Architecture",
    "text": "3.2 Overview of Blackwell Architecture",
    "crumbs": [
      "Part III: System Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Architecture Comparison: Hopper vs Blackwell</span>"
    ]
  },
  {
    "objectID": "03-architecture.html#key-innovations-in-blackwell",
    "href": "03-architecture.html#key-innovations-in-blackwell",
    "title": "3  Architecture Comparison: Hopper vs Blackwell",
    "section": "3.3 Key Innovations in Blackwell",
    "text": "3.3 Key Innovations in Blackwell\n\n3.3.1 Ultra Tensor Cores and New Precision Formats (FP8, FP4)\n\n\n3.3.2 Transformer Engine and FP4 Micro Scaling\n\n\n3.3.3 Multi-Die Chip Design and Interconnect (NVLink, NVSwitch)\n\n\n3.3.4 Memory System: HBM3e, L2 Cache, and Shared Memory",
    "crumbs": [
      "Part III: System Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Architecture Comparison: Hopper vs Blackwell</span>"
    ]
  },
  {
    "objectID": "03-architecture.html#performancewatt-and-area-efficiency-considerations",
    "href": "03-architecture.html#performancewatt-and-area-efficiency-considerations",
    "title": "3  Architecture Comparison: Hopper vs Blackwell",
    "section": "3.4 Performance/Watt and Area Efficiency Considerations",
    "text": "3.4 Performance/Watt and Area Efficiency Considerations",
    "crumbs": [
      "Part III: System Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Architecture Comparison: Hopper vs Blackwell</span>"
    ]
  },
  {
    "objectID": "03-architecture.html#summary-of-architectural-differences",
    "href": "03-architecture.html#summary-of-architectural-differences",
    "title": "3  Architecture Comparison: Hopper vs Blackwell",
    "section": "3.5 Summary of Architectural Differences",
    "text": "3.5 Summary of Architectural Differences",
    "crumbs": [
      "Part III: System Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Architecture Comparison: Hopper vs Blackwell</span>"
    ]
  },
  {
    "objectID": "04-metrics.html",
    "href": "04-metrics.html",
    "title": "4  Metrics for GPU Efficiency",
    "section": "",
    "text": "4.1 Performance per Watt",
    "crumbs": [
      "Part III: System Design",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Metrics for GPU Efficiency</span>"
    ]
  },
  {
    "objectID": "04-metrics.html#compute-throughput-by-data-type",
    "href": "04-metrics.html#compute-throughput-by-data-type",
    "title": "4  Metrics for GPU Efficiency",
    "section": "4.2 Compute Throughput by Data Type",
    "text": "4.2 Compute Throughput by Data Type",
    "crumbs": [
      "Part III: System Design",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Metrics for GPU Efficiency</span>"
    ]
  },
  {
    "objectID": "04-metrics.html#memory-bandwidth-and-arithmetic-intensity",
    "href": "04-metrics.html#memory-bandwidth-and-arithmetic-intensity",
    "title": "4  Metrics for GPU Efficiency",
    "section": "4.3 Memory Bandwidth and Arithmetic Intensity",
    "text": "4.3 Memory Bandwidth and Arithmetic Intensity",
    "crumbs": [
      "Part III: System Design",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Metrics for GPU Efficiency</span>"
    ]
  },
  {
    "objectID": "04-metrics.html#power-thermal-design-and-silicon-area-constraints",
    "href": "04-metrics.html#power-thermal-design-and-silicon-area-constraints",
    "title": "4  Metrics for GPU Efficiency",
    "section": "4.4 Power, Thermal Design, and Silicon Area Constraints",
    "text": "4.4 Power, Thermal Design, and Silicon Area Constraints",
    "crumbs": [
      "Part III: System Design",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Metrics for GPU Efficiency</span>"
    ]
  },
  {
    "objectID": "04-metrics.html#efficiency-bottlenecks-from-memory-bound-to-compute-bound",
    "href": "04-metrics.html#efficiency-bottlenecks-from-memory-bound-to-compute-bound",
    "title": "4  Metrics for GPU Efficiency",
    "section": "4.5 Efficiency Bottlenecks: From Memory Bound to Compute Bound",
    "text": "4.5 Efficiency Bottlenecks: From Memory Bound to Compute Bound",
    "crumbs": [
      "Part III: System Design",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Metrics for GPU Efficiency</span>"
    ]
  },
  {
    "objectID": "05-programming.html",
    "href": "05-programming.html",
    "title": "5  Programming Models for Modern GPUs",
    "section": "",
    "text": "5.1 Introduction to GPU DSLs for Performance",
    "crumbs": [
      "Part IV: Implementation and Methodology",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Programming Models for Modern GPUs</span>"
    ]
  },
  {
    "objectID": "05-programming.html#triton",
    "href": "05-programming.html#triton",
    "title": "5  Programming Models for Modern GPUs",
    "section": "5.2 Triton",
    "text": "5.2 Triton",
    "crumbs": [
      "Part IV: Implementation and Methodology",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Programming Models for Modern GPUs</span>"
    ]
  },
  {
    "objectID": "05-programming.html#thunderkittens-tk",
    "href": "05-programming.html#thunderkittens-tk",
    "title": "5  Programming Models for Modern GPUs",
    "section": "5.3 ThunderKittens (TK)",
    "text": "5.3 ThunderKittens (TK)",
    "crumbs": [
      "Part IV: Implementation and Methodology",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Programming Models for Modern GPUs</span>"
    ]
  },
  {
    "objectID": "05-programming.html#tilelang",
    "href": "05-programming.html#tilelang",
    "title": "5  Programming Models for Modern GPUs",
    "section": "5.4 TileLang",
    "text": "5.4 TileLang",
    "crumbs": [
      "Part IV: Implementation and Methodology",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Programming Models for Modern GPUs</span>"
    ]
  },
  {
    "objectID": "05-programming.html#cute-and-cutlass",
    "href": "05-programming.html#cute-and-cutlass",
    "title": "5  Programming Models for Modern GPUs",
    "section": "5.5 Cute and CUTLASS",
    "text": "5.5 Cute and CUTLASS",
    "crumbs": [
      "Part IV: Implementation and Methodology",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Programming Models for Modern GPUs</span>"
    ]
  },
  {
    "objectID": "05-programming.html#gluon",
    "href": "05-programming.html#gluon",
    "title": "5  Programming Models for Modern GPUs",
    "section": "5.6 Gluon",
    "text": "5.6 Gluon",
    "crumbs": [
      "Part IV: Implementation and Methodology",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Programming Models for Modern GPUs</span>"
    ]
  },
  {
    "objectID": "05-programming.html#pallas-and-the-jax-ml-scaling-framework",
    "href": "05-programming.html#pallas-and-the-jax-ml-scaling-framework",
    "title": "5  Programming Models for Modern GPUs",
    "section": "5.7 Pallas and the JAX ML Scaling Framework",
    "text": "5.7 Pallas and the JAX ML Scaling Framework",
    "crumbs": [
      "Part IV: Implementation and Methodology",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Programming Models for Modern GPUs</span>"
    ]
  },
  {
    "objectID": "05-programming.html#summary-dsls-as-enablers-of-architectural-efficiency",
    "href": "05-programming.html#summary-dsls-as-enablers-of-architectural-efficiency",
    "title": "5  Programming Models for Modern GPUs",
    "section": "5.8 Summary: DSLs as Enablers of Architectural Efficiency",
    "text": "5.8 Summary: DSLs as Enablers of Architectural Efficiency",
    "crumbs": [
      "Part IV: Implementation and Methodology",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Programming Models for Modern GPUs</span>"
    ]
  },
  {
    "objectID": "06-methodology.html",
    "href": "06-methodology.html",
    "title": "6  Methodology and Experimental Setup",
    "section": "",
    "text": "6.1 Objectives of Benchmarking",
    "crumbs": [
      "Part IV: Implementation and Methodology",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Methodology and Experimental Setup</span>"
    ]
  },
  {
    "objectID": "06-methodology.html#hardware-platforms-and-specifications",
    "href": "06-methodology.html#hardware-platforms-and-specifications",
    "title": "6  Methodology and Experimental Setup",
    "section": "6.2 Hardware Platforms and Specifications",
    "text": "6.2 Hardware Platforms and Specifications\n\n6.2.1 Hopper H100\n\n\n6.2.2 Blackwell B200",
    "crumbs": [
      "Part IV: Implementation and Methodology",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Methodology and Experimental Setup</span>"
    ]
  },
  {
    "objectID": "06-methodology.html#software-tools-and-libraries-used",
    "href": "06-methodology.html#software-tools-and-libraries-used",
    "title": "6  Methodology and Experimental Setup",
    "section": "6.3 Software Tools and Libraries Used",
    "text": "6.3 Software Tools and Libraries Used",
    "crumbs": [
      "Part IV: Implementation and Methodology",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Methodology and Experimental Setup</span>"
    ]
  },
  {
    "objectID": "06-methodology.html#microbenchmark-design-gemm-kernel-implementations",
    "href": "06-methodology.html#microbenchmark-design-gemm-kernel-implementations",
    "title": "6  Methodology and Experimental Setup",
    "section": "6.4 Microbenchmark Design: GEMM Kernel Implementations",
    "text": "6.4 Microbenchmark Design: GEMM Kernel Implementations",
    "crumbs": [
      "Part IV: Implementation and Methodology",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Methodology and Experimental Setup</span>"
    ]
  },
  {
    "objectID": "06-methodology.html#measurement-techniques",
    "href": "06-methodology.html#measurement-techniques",
    "title": "6  Methodology and Experimental Setup",
    "section": "6.5 Measurement Techniques",
    "text": "6.5 Measurement Techniques\n\n6.5.1 Throughput (FLOP/s)\n\n\n6.5.2 Power Consumption and Efficiency\n\n\n6.5.3 Memory Bandwidth",
    "crumbs": [
      "Part IV: Implementation and Methodology",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Methodology and Experimental Setup</span>"
    ]
  },
  {
    "objectID": "06-methodology.html#ensuring-fairness-and-reproducibility",
    "href": "06-methodology.html#ensuring-fairness-and-reproducibility",
    "title": "6  Methodology and Experimental Setup",
    "section": "6.6 Ensuring Fairness and Reproducibility",
    "text": "6.6 Ensuring Fairness and Reproducibility",
    "crumbs": [
      "Part IV: Implementation and Methodology",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Methodology and Experimental Setup</span>"
    ]
  },
  {
    "objectID": "07-results.html",
    "href": "07-results.html",
    "title": "7  Results and Discussion",
    "section": "",
    "text": "7.1 Performance Comparison Across Data Types",
    "crumbs": [
      "Part V: Results and Discussion",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Results and Discussion</span>"
    ]
  },
  {
    "objectID": "07-results.html#analysis-of-performance-per-watt",
    "href": "07-results.html#analysis-of-performance-per-watt",
    "title": "7  Results and Discussion",
    "section": "7.2 Analysis of Performance per Watt",
    "text": "7.2 Analysis of Performance per Watt",
    "crumbs": [
      "Part V: Results and Discussion",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Results and Discussion</span>"
    ]
  },
  {
    "objectID": "07-results.html#memory-bandwidth-observations",
    "href": "07-results.html#memory-bandwidth-observations",
    "title": "7  Results and Discussion",
    "section": "7.3 Memory Bandwidth Observations",
    "text": "7.3 Memory Bandwidth Observations",
    "crumbs": [
      "Part V: Results and Discussion",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Results and Discussion</span>"
    ]
  },
  {
    "objectID": "07-results.html#impact-of-tma-tensor-memory-accelerator",
    "href": "07-results.html#impact-of-tma-tensor-memory-accelerator",
    "title": "7  Results and Discussion",
    "section": "7.4 Impact of TMA (Tensor Memory Accelerator)",
    "text": "7.4 Impact of TMA (Tensor Memory Accelerator)",
    "crumbs": [
      "Part V: Results and Discussion",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Results and Discussion</span>"
    ]
  },
  {
    "objectID": "07-results.html#roofline-analysis-compute-vs-memory-bound",
    "href": "07-results.html#roofline-analysis-compute-vs-memory-bound",
    "title": "7  Results and Discussion",
    "section": "7.5 Roofline Analysis: Compute vs Memory Bound",
    "text": "7.5 Roofline Analysis: Compute vs Memory Bound",
    "crumbs": [
      "Part V: Results and Discussion",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Results and Discussion</span>"
    ]
  },
  {
    "objectID": "07-results.html#real-world-relevance-case-study-on-transformer-inferencetraining",
    "href": "07-results.html#real-world-relevance-case-study-on-transformer-inferencetraining",
    "title": "7  Results and Discussion",
    "section": "7.6 Real-World Relevance: Case Study on Transformer Inference/Training",
    "text": "7.6 Real-World Relevance: Case Study on Transformer Inference/Training",
    "crumbs": [
      "Part V: Results and Discussion",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Results and Discussion</span>"
    ]
  },
  {
    "objectID": "07-results.html#discussion-of-bottlenecks-and-architectural-impact",
    "href": "07-results.html#discussion-of-bottlenecks-and-architectural-impact",
    "title": "7  Results and Discussion",
    "section": "7.7 Discussion of Bottlenecks and Architectural Impact",
    "text": "7.7 Discussion of Bottlenecks and Architectural Impact",
    "crumbs": [
      "Part V: Results and Discussion",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Results and Discussion</span>"
    ]
  },
  {
    "objectID": "08-conclusion.html",
    "href": "08-conclusion.html",
    "title": "8  Conclusions and Future Work",
    "section": "",
    "text": "8.1 Summary of Findings",
    "crumbs": [
      "Part V: Results and Discussion",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conclusions and Future Work</span>"
    ]
  },
  {
    "objectID": "08-conclusion.html#implications-for-hardwaresoftware-co-design",
    "href": "08-conclusion.html#implications-for-hardwaresoftware-co-design",
    "title": "8  Conclusions and Future Work",
    "section": "8.2 Implications for Hardware–Software Co-Design",
    "text": "8.2 Implications for Hardware–Software Co-Design",
    "crumbs": [
      "Part V: Results and Discussion",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conclusions and Future Work</span>"
    ]
  },
  {
    "objectID": "08-conclusion.html#relevance-to-edge-computing",
    "href": "08-conclusion.html#relevance-to-edge-computing",
    "title": "8  Conclusions and Future Work",
    "section": "8.3 Relevance to Edge Computing",
    "text": "8.3 Relevance to Edge Computing",
    "crumbs": [
      "Part V: Results and Discussion",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conclusions and Future Work</span>"
    ]
  },
  {
    "objectID": "08-conclusion.html#future-work-and-doctoral-research-directions",
    "href": "08-conclusion.html#future-work-and-doctoral-research-directions",
    "title": "8  Conclusions and Future Work",
    "section": "8.4 Future Work and Doctoral Research Directions",
    "text": "8.4 Future Work and Doctoral Research Directions",
    "crumbs": [
      "Part V: Results and Discussion",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conclusions and Future Work</span>"
    ]
  },
  {
    "objectID": "02-background.html#footnotes",
    "href": "02-background.html#footnotes",
    "title": "2  Background and Related Work",
    "section": "",
    "text": "Thread block clusters refer to a hierarchical grouping of multiple thread blocks in modern GPU architectures that allows their co-residency and explicit synchronization across a defined set of Streaming Multiprocessors (SMs), enabling efficient data sharing through shared memory and reducing latency from global memory accesses.↩︎\nTensor Memory Accelerator refer to a dedicated hardware unit in modern GPUs that asynchronously and programmably manages multidimensional data transfers between global memory and shared memory, offloading these operations from compute cores and enabling overlap of data movement and execution, thereby improving performance and energy efficiency.↩︎\nMixture of Experts (MoE) refer to a neural network architecture in which multiple specialized sub-networks (experts) are selectively activated for each input by a gating mechanism, so that only a subset of the model’s parameters is evaluated per inference or training step, enabling improved scalability and computational efficiency at the cost of increased communication and control complexity.↩︎\nTransformer Engine refer to a library for accelerating Transformer models on NVIDIA GPUs that automatically manages mixed-precision calculations, leveraging 8-bit floating point (FP8) formats on Tensor Cores to maximize throughput and memory efficiency while maintaining model accuracy.↩︎\nGEMM (General Matrix-Matrix Multiplication) is the fundamental linear algebra primitive (\\(C = \\alpha AB + \\beta C\\)) that underpins the dense layers of deep neural networks. In the context of GPUs, GEMMs are highly optimized to maximize the utilization of Tensor Cores and arithmetic intensity.↩︎\nTransformer Attention is a mechanism that models dependencies between input tokens by computing scaled dot-products of Query, Key, and Value matrices. From a microarchitectural perspective, attention is often memory-bandwidth bound due to the quadratic complexity of sequence length and the need to load large Key-Value caches.↩︎\nMixture-of-Experts (MoE): A sparse model architecture that decouples model size from computational cost by activating only a subset of parameters (specific “experts”) per token. This approach introduces unique hardware challenges, such as dynamic routing logic and high-bandwidth requirements for fetching expert weights.↩︎\nLinear Layers are also known as fully connected or dense layers, these are fundamental neural network components where inputs are mapped to outputs via a weight matrix (\\(y=xW^T+b\\)). In hardware, they translate directly to large, dense matrix multiplications (GEMMs) that are ideal for saturating GPU Tensor Cores.↩︎\nProjections in Attention Mechanisms: The specific linear transformations used to map input embeddings into the Query (Q), Key (K), and Value (V) subspaces required for self-attention. Computationally, these are parallel GEMM operations performed prior to the attention calculation to align data for relevance scoring.↩︎\nMLP Blocks (Multilayer Perceptron Blocks): The position-wise feed-forward networks within a Transformer layer, typically composed of two linear transformations separated by a non-linear activation function (e.g., GeLU or SwiGLU). In modern Large Language Models, MLP blocks account for approximately two-thirds of the total parameters and floating-point operations.↩︎\nSIMD (Single Instruction, Multiple Data): A parallel computing classification (Flynn’s taxonomy) where a single control instruction triggers the simultaneous execution of the same operation across multiple data points. In the context of NVIDIA GPUs, this refers to the traditional vector-based execution model of CUDA Cores (computing 1D arrays), as opposed to the matrix-based execution model of Tensor Cores.↩︎",
    "crumbs": [
      "Part II: Introduction and Theoretical Background",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background and Related Work</span>"
    ]
  }
]