[
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Chapter 1 – Introduction",
    "section": "",
    "text": "1.1 Motivation and Context: The Need for Hardware–Software Co-Design\nThe rapid evolution of artificial intelligence (AI), particularly deep learning models based on large-scale matrix operations, has fundamentally reshaped the requirements imposed on modern computing systems. Contemporary workloads such as transformer-based models, large language models (LLMs), and multimodal AI systems exhibit unprecedented computational intensity, memory bandwidth demand, and energy consumption. These characteristics challenge traditional computing paradigms and demand a holistic approach in which hardware and software are designed in close coordination, rather than as independent layers of abstraction (Jouppi et al. 2017; Sze et al. 2020).\nHistorically, general-purpose processors benefited from Dennard scaling1 and instruction-level parallelism, allowing software performance to improve transparently with each technology generation. However, as CMOS scaling entered the sub-10 nm regime, physical constraints related to power density, leakage currents, interconnect delay, and thermal dissipation have severely limited frequency scaling and brute-force performance improvements (Esmaeilzadeh et al. 2011; Thompson et al. 2021). In this context, micro and nanoelectronic design choices, such as transistor architecture, memory hierarchy, interconnect topology, and on-die specialization have become first-order determinants of achievable system-level performance and efficiency.\nGraphics Processing Units (GPUs) have emerged as the dominant computing substrate for AI due to their massive parallelism and high arithmetic throughput. Yet, modern GPUs are no longer generic accelerators; they are highly specialized systems whose efficiency depends critically on software being explicitly structured to exploit architectural features such as tensor cores2, hierarchical memory systems, and fine-grained synchronization mechanisms (NVIDIA 2022). This tight coupling is particularly evident in the evolution from NVIDIA’s Hopper architecture to the more recent Blackwell architecture, where architectural innovations such as ultra-specialized tensor engines3, new low-precision formats (e.g., FP8 and FP4), and advanced memory movement engines fundamentally reshape the optimal software execution model (NVIDIA 2024).\nFrom a hardware design perspective, the motivation for hardware–software co-design is rooted in the need to maximize performance per watt and performance per unit area, two metrics that are especially critical in both datacenter-scale systems and edge deployments. Decisions made at the microarchitectural and circuit levels—such as register file sizing, shared memory organization, cache coherence policies, and on-chip network design—directly constrain which software access patterns are efficient or even feasible (Sze et al. 2020). Consequently, software frameworks that ignore these constraints often underutilize available silicon resources, leading to memory-bound4 execution and suboptimal energy efficiency.\nConversely, from a software and algorithmic standpoint, AI workloads are increasingly designed with an awareness of hardware realities. Techniques such as operator fusion, tiling strategies, mixed-precision arithmetic, and explicit data movement orchestration have become central to achieving peak performance. Domain-specific languages (DSLs)5 and compiler frameworks6—such as Triton, CUTLASS, and JAX-based systems—embody this philosophy by exposing hardware capabilities directly to the programmer or compiler, thereby enabling software to push the hardware toward compute-bound operation (JAX Team 2023; Tillet, Kung, and Cox 2019).\nThis paradigm is particularly relevant for edge AI, where constraints on power, thermal envelope, and silicon area are even more stringent than in datacenter environments. Edge devices require carefully balanced designs in which architectural specialization compensates for limited resources, and software must be explicitly optimized to match the underlying hardware characteristics (Sze et al. 2017). Although this thesis focuses experimentally on high-end GPUs, the insights derived from microbenchmarking architectures such as Hopper and Blackwell are directly transferable to edge-class accelerators, where hardware–software co-design is often the only viable path to achieving acceptable performance and energy efficiency.\nIn this context, the motivation of this thesis is twofold. First, it aims to demonstrate, through microbenchmarking, how architectural innovations at the micro and nanoelectronic level translate into measurable performance and efficiency gains only when matched with appropriately designed software. Second, it seeks to highlight that future progress in AI systems—both at the edge and in large-scale deployments—will depend not on isolated advances in hardware or software, but on their co-evolution as a unified design problem (Jouppi et al. 2023).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Chapter 1 – Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#challenges-in-efficient-compute-for-ai-and-edge-applications",
    "href": "01-intro.html#challenges-in-efficient-compute-for-ai-and-edge-applications",
    "title": "1  Chapter 1 – Introduction",
    "section": "1.2 Challenges in Efficient Compute for AI and Edge Applications",
    "text": "1.2 Challenges in Efficient Compute for AI and Edge Applications\nThe efficient execution of modern AI workloads presents a multifaceted set of challenges that span algorithm design, software systems, microarchitecture, and physical implementation. While advances in deep learning have been driven primarily by increases in model size and data availability, the resulting computational demands increasingly stress the fundamental limits of contemporary hardware, especially in power- and area-constrained environments such as edge devices (Thompson et al. 2021; Sze et al. 2017).\nA primary challenge arises from the mismatch between arithmetic throughput and memory bandwidth, commonly referred to as the memory wall. Although modern accelerators provide massive peak compute capabilities—often measured in tens or hundreds of teraFLOPS—real-world AI kernels frequently fail to achieve these peaks due to insufficient data reuse and limited effective memory bandwidth (Williams, Waterman, and Patterson 2009). This imbalance is particularly pronounced in workloads dominated by General Matrix-Matrix Multiplication (GEMM) and attention mechanisms7, where performance depends critically on maximizing arithmetic intensity through careful tiling, blocking, and data movement strategies.\nFrom a microarchitectural standpoint, this challenge translates directly into design trade-offs involving cache hierarchy depth, shared memory capacity, register file sizing, and on-chip interconnect bandwidth. As technology scaling progresses into advanced nodes, wire delay and energy increasingly dominate over transistor switching costs, making data movement significantly more expensive than computation (Horowitz 2014). Consequently, architectural features such as software-managed scratchpads8, tensor memory accelerators9, and explicit asynchronous data movement engines have become essential enablers of efficiency—but only when software is explicitly designed to exploit them.\nAnother major challenge is energy efficiency, which is particularly critical for edge AI systems operating under strict power and thermal budgets. Unlike datacenter accelerators, which can amortize energy costs across large-scale infrastructure, edge devices must deliver acceptable inference or training performance within envelopes often limited to a few watts. In this regime, inefficiencies stemming from control overhead, redundant memory accesses, or suboptimal precision choices can render otherwise powerful hardware impractical (Chen et al. 2016; Sze et al. 2020). This places increased emphasis on low-precision arithmetic, approximate computing, and hardware specialization—each of which introduces additional complexity at both the circuit and software levels.\nPrecision scaling itself represents a further challenge. While reduced-precision formats such as FP16, INT8, FP8, and more recently FP4 enable substantial gains in throughput and energy efficiency, they also impose strict numerical constraints on algorithms and training stability (Micikevicius et al. 2018; NVIDIA 2024). Supporting these formats efficiently requires non-trivial micro and nanoelectronic innovations, including custom datapaths, specialized normalization logic, and fine-grained control over accumulation and scaling. Software must be co-designed to manage these constraints, often through techniques such as loss scaling, block-wise normalization, and mixed-precision accumulation.\nScalability and programmability constitute another critical dimension of the efficiency challenge. As GPU architectures grow increasingly complex—incorporating heterogeneous execution units10, deep memory hierarchies, and multi-die organizations—the burden placed on programmers and compilers increases substantially. Traditional programming models that abstract away hardware details often fail to expose sufficient control over data layout and execution order, leading to underutilization of available silicon resources (Jouppi et al. 2017). This has motivated the rise of domain-specific languages (DSLs) and performance-oriented compiler frameworks that trade generality for efficiency by making architectural constraints explicit (Tillet, Kung, and Cox 2019; JAX Team 2023).\nFor edge AI applications, these challenges are further compounded by variability in workloads, deployment environments, and real-time constraints. Edge systems must often support a diverse set of models and input conditions while maintaining deterministic latency and reliability. This requirement conflicts with highly specialized hardware designs, which excel at narrow classes of workloads but may lack flexibility (Sze et al. 2017). Bridging this gap demands careful hardware–software co-design, where architectural specialization is balanced against programmability and reuse.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Chapter 1 – Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#objectives-and-scope-of-the-thesis",
    "href": "01-intro.html#objectives-and-scope-of-the-thesis",
    "title": "1  Chapter 1 – Introduction",
    "section": "1.3 Objectives and Scope of the Thesis",
    "text": "1.3 Objectives and Scope of the Thesis\nThe primary objective of this thesis is to systematically analyze and quantify the impact of hardware–software co-design on the efficiency of modern GPU architectures, with a specific focus on AI workloads dominated by dense linear algebra operations. By conducting a detailed microbenchmarking study of NVIDIA’s Hopper (H100) and Blackwell (B200) architectures, this work seeks to elucidate how architectural innovations at the micro and nanoelectronic level translate into tangible performance, energy efficiency, and scalability benefits when matched with appropriately designed software (NVIDIA 2022, 2024).\nA central goal is to characterize the relationship between architectural features and achievable compute efficiency, particularly in terms of performance per watt and proximity to theoretical peak throughput. Rather than relying solely on high-level application benchmarks, this thesis emphasizes fine-grained microbenchmarks—primarily centered on General Matrix-Matrix Multiplication (GEMM)—to isolate specific architectural behaviors such as memory bandwidth utilization, arithmetic intensity, and data movement overheads. This approach enables a more precise attribution of performance bottlenecks to underlying hardware mechanisms, consistent with established performance modeling methodologies such as the Roofline model11 (Williams, Waterman, and Patterson 2009).\nAnother key objective is to compare Hopper and Blackwell as representative points in the evolution of GPU microarchitecture, highlighting the motivations behind Blackwell’s design choices. These include, but are not limited to, enhanced tensor computation units12, support for ultra-low-precision formats, expanded and restructured memory hierarchies, and advanced mechanisms for asynchronous data transfer. By evaluating how these features affect software behavior and efficiency, the thesis aims to provide empirical evidence supporting the necessity of new architectural paradigms for sustaining AI performance scaling under increasingly stringent power and area constraints (Horowitz 2014; Thompson et al. 2021).\nFrom a micro and nanoelectronic design perspective, this thesis seeks to bridge the gap between circuit and architecture-level considerations and system-level performance outcomes. While the experimental focus is on commercially available GPUs, the analysis explicitly interprets results through the lens of silicon efficiency, emphasizing metrics such as energy per operation, data movement cost, and utilization of on-chip resources. In doing so, the thesis aims to reinforce the relevance of microelectronic design decisions in shaping the performance envelope of AI accelerators, even at the highest levels of abstraction.\nIn parallel, the thesis aims to assess the role of modern programming models and domain-specific languages (DSLs) as enablers of hardware–software co-design. By leveraging performance-oriented frameworks such as CUTLASS, Triton, and JAX-based systems, the work explores how software abstractions can expose architectural capabilities while maintaining a balance between programmability and efficiency (Tillet, Kung, and Cox 2019; JAX Team 2023). This analysis contributes to understanding how future software ecosystems must evolve alongside hardware to fully exploit increasingly specialized architectures.\nThe scope of this thesis is deliberately constrained to ensure depth and rigor. Experimentally, it focuses on single-node GPU evaluation, excluding large-scale multi-node distributed training and system-level networking effects. The workloads considered are limited to representative dense linear algebra kernels and closely related AI primitives, rather than end-to-end application benchmarks. While the findings are discussed in the context of both datacenter and edge AI, the experimental platform itself is a high-end accelerator; extrapolations to edge systems are therefore conceptual and architectural rather than empirical.\nIn summary, the scope of this work encompasses (i) a microbenchmark-driven comparison of Hopper and Blackwell GPUs, (ii) an efficiency-focused analysis grounded in micro and nanoelectronic design principles, and (iii) an evaluation of software frameworks as integral components of hardware–software co-design. Collectively, these objectives position the thesis as a foundation for future doctoral research in edge AI and accelerator architecture, providing both methodological tools and conceptual insights relevant to the co-design of next-generation AI systems.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Chapter 1 – Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#methodology-overview",
    "href": "01-intro.html#methodology-overview",
    "title": "1  Chapter 1 – Introduction",
    "section": "1.4 Methodology Overview",
    "text": "1.4 Methodology Overview\nThis thesis adopts a quantitative, experimental methodology grounded in microbenchmarking and performance modeling to analyze the efficiency implications of hardware–software co-design in modern GPU architectures. The methodology is designed to isolate and characterize the impact of architectural features on AI-relevant workloads, with particular emphasis on dense linear algebra kernels representative of contemporary deep learning models\nThe methodological workflow begins with a systematic architectural analysis of NVIDIA’s Hopper and Blackwell GPUs. This analysis focuses on identifying key microarchitectural differences—such as tensor core design, supported numerical formats, memory hierarchy organization, and data movement mechanisms—that are expected to influence performance and energy efficiency. Publicly available technical documentation and research publications are used to establish a baseline understanding of each architecture, providing context for the experimental results presented later in the thesis (NVIDIA 2022, 2024).\nAt the core of the experimental approach is a microbenchmark-based evaluation strategy. Rather than relying on end-to-end AI applications, the methodology employs carefully constructed General Matrix-Matrix Multiplication (GEMM) kernels to exert fine-grained control over computational patterns, memory access behavior, and data precision. This approach enables the systematic exploration of how software design choices—such as tiling parameters, memory layout, and precision selection—interact with hardware capabilities to determine achieved throughput and efficiency (Williams, Waterman, and Patterson 2009).\nTo interpret benchmark results in a principled manner, the methodology incorporates analytical performance modeling, primarily using the Roofline model. By relating achieved performance to arithmetic intensity and hardware-specific bandwidth and compute ceilings, this framework facilitates the identification of transitions between memory-bound and compute-bound regimes. This analysis is instrumental in quantifying how architectural enhancements in Blackwell, relative to Hopper, expand the feasible operating space for AI workloads (Williams, Waterman, and Patterson 2009; Horowitz 2014).\nThe software stack used in this work is deliberately chosen to reflect state-of-the-art performance-oriented programming models. Libraries and frameworks such as CUTLASS, Triton, and JAX-based systems are employed to implement and tune microbenchmarks, allowing the study to evaluate both low-level kernel behavior and higher-level abstraction trade-offs (Tillet, Kung, and Cox 2019; JAX Team 2023). Where applicable, multiple implementations of equivalent kernels are compared to assess the impact of abstraction level on performance portability and hardware utilization.\nMeasurement of performance and efficiency metrics is conducted using vendor-provided profiling tools and standardized benchmarking practices. Key metrics include sustained throughput (FLOP/s), memory bandwidth utilization, power consumption, and derived efficiency measures such as performance per watt. To ensure statistical reliability, benchmarks are repeated under controlled conditions, and results are averaged over multiple runs. Particular care is taken to account for warm-up effects, clock variability, and measurement overhead, in line with best practices for performance evaluation on modern accelerators (Jouppi et al. 2017).\nReproducibility and fairness are treated as first-class methodological concerns. All experimental configurations, including hardware specifications, software versions, compiler settings, and runtime parameters, are explicitly documented. Where possible, benchmarks are designed to be portable across architectures, ensuring that observed differences can be attributed to architectural characteristics rather than implementation artifacts.\nFinally, the methodology explicitly connects experimental findings back to micro and nanoelectronic design considerations. Observed performance trends are interpreted in terms of data movement cost, on-chip resource utilization, and energy efficiency, linking system-level behavior to underlying architectural and physical constraints. This integrative approach reinforces the central thesis that meaningful progress in AI computing efficiency emerges from coordinated advances in hardware design and software optimization.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Chapter 1 – Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#structure-of-the-thesis",
    "href": "01-intro.html#structure-of-the-thesis",
    "title": "1  Chapter 1 – Introduction",
    "section": "1.5 Structure of the Thesis",
    "text": "1.5 Structure of the Thesis\nThis thesis is organized into eight chapters, each addressing a specific aspect of the analysis of hardware–software co-design in modern GPU architectures for AI workloads.\nChapter 1 introduces the motivation, challenges, objectives, and methodological foundations of the work. It establishes the context of efficient AI computing under micro- and nanoelectronic constraints and motivates the need for a co-design approach, with particular attention to the evolution of GPU architectures and their relevance to edge AI.\nChapter 2 provides the necessary background and related work. It reviews the evolution of GPU architectures from earlier generations to Hopper and Blackwell, outlines fundamental principles of hardware–software co-design, and discusses the role of General Matrix-Matrix Multiplication (GEMM) as a core computational primitive in AI workloads. Additionally, this chapter surveys relevant programming models, domain-specific languages, and tools developed by NVIDIA Research and the broader AI systems community.\nChapter 3 presents a detailed architectural comparison between NVIDIA Hopper and Blackwell GPUs. It examines key innovations introduced in Blackwell, including advances in tensor computation units, numerical precision formats, memory hierarchy, and interconnect design, and discusses their implications for performance, efficiency, and scalability.\nChapter 4 defines the metrics and models used to evaluate GPU efficiency throughout the thesis. It introduces performance per watt, arithmetic intensity, memory bandwidth utilization, and related concepts, and frames the analysis using the Roofline performance model to distinguish between compute-bound and memory-bound execution regimes.\nChapter 5 focuses on programming models and software frameworks for modern GPUs. It analyzes domain-specific languages and performance-oriented libraries such as Triton, CUTLASS, and JAX-based systems, highlighting their role in exposing architectural features and enabling effective hardware–software co-design.\nChapter 6 describes the experimental methodology and benchmarking setup in detail. It specifies the hardware platforms, software stack, microbenchmark design, measurement techniques, and reproducibility considerations that underpin the experimental results.\nChapter 7 presents and discusses the experimental results. It compares Hopper and Blackwell across multiple metrics, analyzes performance and efficiency trends, and interprets observed bottlenecks in relation to architectural features and software design choices. A roofline-based analysis is used to contextualize results and assess the impact of architectural innovations.\nChapter 8 concludes the thesis by summarizing the main findings and discussing their implications for hardware–software co-design and edge AI systems. It also outlines directions for future work and potential doctoral research avenues building upon the results of this study.\nFinally, the appendices provide supplementary material, including experimental scripts, extended benchmark results, and technical documentation relevant to tensor memory accelerators and GEMM intrinsics, supporting transparency and reproducibility.\n\n\n\n\nChen, Yu-Hsin et al. 2016. “Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks.” IEEE Journal of Solid-State Circuits 52 (1): 127–38. https://doi.org/10.1109/JSSC.2016.2616357.\n\n\nEsmaeilzadeh, Hadi et al. 2011. “Dark Silicon and the End of Multicore Scaling.” ACM SIGARCH Computer Architecture News 39 (3): 365–76. https://doi.org/10.1145/2024723.2000108.\n\n\nHorowitz, Mark. 2014. “Computing’s Energy Problem (and What We Can Do about It).” ISSCC Digest of Technical Papers, 10–14. https://doi.org/10.1109/ISSCC.2014.6757323.\n\n\nJAX Team. 2023. “The JAX ML Scaling Book.” 2023. https://jax-ml.github.io/scaling-book/.\n\n\nJouppi, Norman P. et al. 2017. “In-Datacenter Performance Analysis of a Tensor Processing Unit.” IEEE Computer 50 (2): 58–68. https://doi.org/10.1109/MC.2017.33.\n\n\n——— et al. 2023. “Ten Years of Tensor Processing Units.” Nature 624: 41–51. https://doi.org/10.1038/s41586-023-06777-1.\n\n\nMicikevicius, Paulius et al. 2018. “Mixed Precision Training.” International Conference on Learning Representations (ICLR).\n\n\nNVIDIA. 2022. “NVIDIA Hopper Architecture in-Depth.” NVIDIA Corporation.\n\n\n———. 2024. “NVIDIA Blackwell Architecture.” NVIDIA Corporation.\n\n\nSze, Vivienne et al. 2017. “Efficient Processing of Deep Neural Networks: A Tutorial and Survey.” Proceedings of the IEEE 105 (12): 2295–2329. https://doi.org/10.1109/JPROC.2017.2761740.\n\n\nSze, Vivienne, Yu-Hsin Chen, Tien-Ju Yang, and Joel Emer. 2020. Efficient Processing of Deep Neural Networks. Morgan & Claypool. https://doi.org/10.2200/S01004ED1V01Y202004CAC050.\n\n\nThompson, Neil C., Kristjan Greenewald, Keeheon Lee, and Gabriel F. Manso. 2021. “Computing Beyond Moore’s Law.” Science 371 (6530): eaba7373. https://doi.org/10.1126/science.aba7373.\n\n\nTillet, Philippe, H. T. Kung, and David Cox. 2019. “Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations.” Proceedings of MLSys.\n\n\nWilliams, Samuel, Andrew Waterman, and David Patterson. 2009. “Roofline: An Insightful Visual Performance Model for Multicore Architectures.” Communications of the ACM 52 (4): 65–76. https://doi.org/10.1145/1498765.1498785.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Chapter 1 – Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#footnotes",
    "href": "01-intro.html#footnotes",
    "title": "1  Chapter 1 – Introduction",
    "section": "",
    "text": "Dennard scaling refers to the principle formulated by Robert H. Dennard et al. (1974), according to which, as transistor dimensions are reduced, voltage and current scale proportionally, keeping power density approximately constant. This enabled performance improvements across successive technology generations.↩︎\nTensor cores are specialized hardware units introduced by NVIDIA to accelerate dense matrix–matrix operations, particularly mixed-precision matrix multiply–accumulate (MMA) workloads. They are designed to deliver high throughput for deep learning primitives by operating on small matrix tiles using reduced-precision arithmetic.↩︎\nTensor engines refer to more advanced and specialized successors of tensor cores in recent GPU architectures, integrating support for additional low-precision formats (such as FP8 and FP4), higher degrees of specialization, and tighter coupling with data movement and scheduling mechanisms to optimize end-to-end AI workloads.↩︎\nA workload is said to be memory-bound when its performance is primarily limited by memory bandwidth or memory access latency rather than by the available compute throughput. In such cases, increasing arithmetic capability yields limited performance gains unless memory access patterns, data locality, or data movement mechanisms are improved.↩︎\nDomain-specific languages (DSLs) are programming languages designed to express computations within a restricted application domain, providing high-level abstractions that enable domain-aware optimizations while reducing the burden of low-level hardware management for programmers.↩︎\nCompiler frameworks are extensible software infrastructures that support the construction of compilers and code generation pipelines, typically providing intermediate representations, optimization passes, and backend targets. In the context of AI and GPU computing, they facilitate the systematic mapping of high-level program abstractions onto specialized hardware architectures.↩︎\nAttention mechanisms are a class of neural network operations, most prominently used in Transformer architectures, that compute weighted combinations of input features based on pairwise similarity scores (e.g., dot products between queries and keys). Their computational structure involves large matrix multiplications and memory-intensive softmax and normalization steps, making their performance highly sensitive to data locality, memory bandwidth, and efficient data movement.↩︎\nSoftware-managed scratchpads are explicitly addressable on-chip memory regions whose allocation, placement, and data movement are controlled by software rather than by hardware-managed cache policies. They enable predictable latency and bandwidth characteristics, allowing programmers or compilers to optimize data locality and reuse in performance-critical kernels.↩︎\nTensor memory accelerators refer to specialized hardware units designed to optimize data movement and layout transformations for tensor-oriented workloads. These accelerators reduce the overhead of memory access and data rearrangement by offloading common tensor data movement patterns—such as tiling, transposition, and format conversion—from general-purpose execution units.↩︎\nHeterogeneous execution units refer to the coexistence of multiple types of specialized compute units within a single GPU, such as scalar and vector ALUs, tensor cores, ray tracing units, and fixed-function accelerators. Each unit type exhibits distinct performance characteristics and programming constraints, requiring explicit scheduling and workload mapping to achieve high utilization.↩︎\nRoofline model is an analytical performance model that relates the achievable performance of a computing system to an application’s arithmetic intensity (operations per byte transferred). It defines an upper bound on performance through two limiting ceilings: the hardware’s peak compute throughput and its memory bandwidth. The model is used to determine whether an application is compute-bound or memory-bound and to systematically identify architectural performance bottlenecks.↩︎\nTensor computation unit is a specialized hardware execution unit designed to accelerate dense linear algebra operations—most notably matrix–matrix and matrix–vector multiplications—by performing fused multiply–accumulate operations on small matrix tiles in parallel. Tensor computation units are optimized for high throughput and energy efficiency, typically supporting reduced-precision data formats, and are a fundamental building block for accelerating machine learning and AI workloads on modern GPUs.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Chapter 1 – Introduction</span>"
    ]
  }
]