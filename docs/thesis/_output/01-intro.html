<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>1&nbsp; Chapter 1 – Introduction – The Anatomy of an Efficient Blackwell GEMM</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./02-background.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-5ea8f22911a6df30576a2a25a24cdd7a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-intro.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Chapter 1 – Introduction</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">The Anatomy of an Efficient Blackwell GEMM</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./abstract.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Abstract</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">index.html</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Chapter 1 – Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-background.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Chapter 2 – Background and Related Work</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-architecture.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Chapter 3 – Architecture Comparison: Hopper vs Blackwell</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-metrics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Chapter 4 – Metrics for GPU Efficiency</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-programming.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Chapter 5 – Programming Models for Modern GPUs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-methodology.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Chapter 6 – Methodology and Experimental Setup</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-results.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Chapter 7 – Results and Discussion</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Chapter 8 – Conclusions and Future Work</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-appendices.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Appendices</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#motivation-and-context-the-need-for-hardwaresoftware-co-design" id="toc-motivation-and-context-the-need-for-hardwaresoftware-co-design" class="nav-link active" data-scroll-target="#motivation-and-context-the-need-for-hardwaresoftware-co-design"><span class="header-section-number">1.1</span> Motivation and Context: The Need for Hardware–Software Co-Design</a></li>
  <li><a href="#challenges-in-efficient-compute-for-ai-and-edge-applications" id="toc-challenges-in-efficient-compute-for-ai-and-edge-applications" class="nav-link" data-scroll-target="#challenges-in-efficient-compute-for-ai-and-edge-applications"><span class="header-section-number">1.2</span> Challenges in Efficient Compute for AI and Edge Applications</a></li>
  <li><a href="#objectives-and-scope-of-the-thesis" id="toc-objectives-and-scope-of-the-thesis" class="nav-link" data-scroll-target="#objectives-and-scope-of-the-thesis"><span class="header-section-number">1.3</span> Objectives and Scope of the Thesis</a></li>
  <li><a href="#methodology-overview" id="toc-methodology-overview" class="nav-link" data-scroll-target="#methodology-overview"><span class="header-section-number">1.4</span> Methodology Overview</a></li>
  <li><a href="#structure-of-the-thesis" id="toc-structure-of-the-thesis" class="nav-link" data-scroll-target="#structure-of-the-thesis"><span class="header-section-number">1.5</span> Structure of the Thesis</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Chapter 1 – Introduction</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="motivation-and-context-the-need-for-hardwaresoftware-co-design" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="motivation-and-context-the-need-for-hardwaresoftware-co-design"><span class="header-section-number">1.1</span> Motivation and Context: The Need for Hardware–Software Co-Design</h2>
<p>The rapid evolution of artificial intelligence (AI), particularly deep learning models based on large-scale matrix operations, has fundamentally reshaped the requirements imposed on modern computing systems. Contemporary workloads such as transformer-based models, large language models (LLMs), and multimodal AI systems exhibit unprecedented computational intensity, memory bandwidth demand, and energy consumption. These characteristics challenge traditional computing paradigms and demand a holistic approach in which <strong>hardware and software are designed in close coordination</strong>, rather than as independent layers of abstraction <span class="citation" data-cites="jouppi2017datacenter sze2020efficient">(<a href="#ref-jouppi2017datacenter" role="doc-biblioref">Jouppi et al. 2017</a>; <a href="#ref-sze2020efficient" role="doc-biblioref">Sze et al. 2020</a>)</span>.</p>
<p>Historically, general-purpose processors benefited from <em>Dennard scaling</em><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> and instruction-level parallelism, allowing software performance to improve transparently with each technology generation. However, as CMOS scaling entered the sub-10 nm regime, physical constraints related to power density, leakage currents, interconnect delay, and thermal dissipation have severely limited frequency scaling and brute-force performance improvements <span class="citation" data-cites="esmaeilzadeh2011dark thompson2021computing">(<a href="#ref-esmaeilzadeh2011dark" role="doc-biblioref">Esmaeilzadeh et al. 2011</a>; <a href="#ref-thompson2021computing" role="doc-biblioref">Thompson et al. 2021</a>)</span>. In this context, <strong>micro and nanoelectronic design choices</strong>, such as transistor architecture, memory hierarchy, interconnect topology, and on-die specialization have become first-order determinants of achievable system-level performance and efficiency.</p>
<p>Graphics Processing Units (GPUs) have emerged as the dominant computing substrate for AI due to their massive parallelism and high arithmetic throughput. Yet, modern GPUs are no longer generic accelerators; they are highly specialized systems whose efficiency depends critically on software being explicitly structured to exploit architectural features such as <em>tensor cores</em><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, hierarchical memory systems, and fine-grained synchronization mechanisms <span class="citation" data-cites="nvidia_hopper_whitepaper">(<a href="#ref-nvidia_hopper_whitepaper" role="doc-biblioref">NVIDIA 2022</a>)</span>. This tight coupling is particularly evident in the evolution from NVIDIA’s Hopper architecture to the more recent Blackwell architecture, where architectural innovations such as ultra-specialized <em>tensor engines</em><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>, new low-precision formats (e.g., FP8 and FP4), and advanced memory movement engines fundamentally reshape the optimal software execution model <span class="citation" data-cites="nvidia_blackwell_whitepaper">(<a href="#ref-nvidia_blackwell_whitepaper" role="doc-biblioref">NVIDIA 2024</a>)</span>.</p>
<p>From a <strong>hardware design perspective</strong>, the motivation for hardware–software co-design is rooted in the need to maximize <em>performance per watt</em> and <em>performance per unit area</em>, two metrics that are especially critical in both datacenter-scale systems and edge deployments. Decisions made at the microarchitectural and circuit levels—such as register file sizing, shared memory organization, cache coherence policies, and on-chip network design—directly constrain which software access patterns are efficient or even feasible <span class="citation" data-cites="sze2020efficient">(<a href="#ref-sze2020efficient" role="doc-biblioref">Sze et al. 2020</a>)</span>. Consequently, software frameworks that ignore these constraints often underutilize available silicon resources, leading to <em>memory-bound</em><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> execution and suboptimal energy efficiency.</p>
<p>Conversely, from a <strong>software and algorithmic standpoint</strong>, AI workloads are increasingly designed with an awareness of hardware realities. Techniques such as operator fusion, tiling strategies, mixed-precision arithmetic, and explicit data movement orchestration have become central to achieving peak performance. <em>Domain-specific languages (DSLs)</em><a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> and <em>compiler frameworks</em><a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>—such as Triton, CUTLASS, and JAX-based systems—embody this philosophy by exposing hardware capabilities directly to the programmer or compiler, thereby enabling software to push the hardware toward compute-bound operation <span class="citation" data-cites="jax_scaling_book tillet2019triton">(<a href="#ref-jax_scaling_book" role="doc-biblioref">JAX Team 2023</a>; <a href="#ref-tillet2019triton" role="doc-biblioref">Tillet, Kung, and Cox 2019</a>)</span>.</p>
<p>This paradigm is particularly relevant for <strong>edge AI</strong>, where constraints on power, thermal envelope, and silicon area are even more stringent than in datacenter environments. Edge devices require carefully balanced designs in which architectural specialization compensates for limited resources, and software must be explicitly optimized to match the underlying hardware characteristics <span class="citation" data-cites="sze2017efficient">(<a href="#ref-sze2017efficient" role="doc-biblioref">Sze et al. 2017</a>)</span>. Although this thesis focuses experimentally on high-end GPUs, the insights derived from microbenchmarking architectures such as Hopper and Blackwell are directly transferable to edge-class accelerators, where hardware–software co-design is often the only viable path to achieving acceptable performance and energy efficiency.</p>
<p>In this context, the motivation of this thesis is twofold. First, it aims to <strong>demonstrate, through microbenchmarking</strong>, how architectural innovations at the micro and nanoelectronic level translate into measurable performance and efficiency gains only when matched with appropriately designed software. Second, it seeks to highlight that future progress in AI systems—both at the edge and in large-scale deployments—will depend not on isolated advances in hardware or software, but on their <strong>co-evolution as a unified design problem</strong> <span class="citation" data-cites="jouppi2023tenyears">(<a href="#ref-jouppi2023tenyears" role="doc-biblioref">Jouppi et al. 2023</a>)</span>.</p>
</section>
<section id="challenges-in-efficient-compute-for-ai-and-edge-applications" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="challenges-in-efficient-compute-for-ai-and-edge-applications"><span class="header-section-number">1.2</span> Challenges in Efficient Compute for AI and Edge Applications</h2>
<p>The efficient execution of modern AI workloads presents a multifaceted set of challenges that span algorithm design, software systems, microarchitecture, and physical implementation. While advances in deep learning have been driven primarily by increases in model size and data availability, the resulting computational demands increasingly stress the fundamental limits of contemporary hardware, especially in power- and area-constrained environments such as edge devices <span class="citation" data-cites="thompson2021computing sze2017efficient">(<a href="#ref-thompson2021computing" role="doc-biblioref">Thompson et al. 2021</a>; <a href="#ref-sze2017efficient" role="doc-biblioref">Sze et al. 2017</a>)</span>.</p>
<p>A primary challenge arises from the <strong>mismatch between arithmetic throughput and memory bandwidth</strong>, commonly referred to as the <em>memory wall</em>. Although modern accelerators provide massive peak compute capabilities—often measured in tens or hundreds of teraFLOPS—real-world AI kernels frequently fail to achieve these peaks due to insufficient data reuse and limited effective memory bandwidth <span class="citation" data-cites="williams2009roofline">(<a href="#ref-williams2009roofline" role="doc-biblioref">Williams, Waterman, and Patterson 2009</a>)</span>. This imbalance is particularly pronounced in workloads dominated by General Matrix-Matrix Multiplication (GEMM) and <em>attention mechanisms</em><a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>, where performance depends critically on maximizing arithmetic intensity through careful tiling, blocking, and data movement strategies.</p>
<p>From a <strong>microarchitectural standpoint</strong>, this challenge translates directly into design trade-offs involving cache hierarchy depth, shared memory capacity, register file sizing, and on-chip interconnect bandwidth. As technology scaling progresses into advanced nodes, wire delay and energy increasingly dominate over transistor switching costs, making data movement significantly more expensive than computation <span class="citation" data-cites="horowitz2014energy">(<a href="#ref-horowitz2014energy" role="doc-biblioref">Horowitz 2014</a>)</span>. Consequently, architectural features such as <em>software-managed scratchpads</em><a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>, <em>tensor memory accelerators</em><a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>, and explicit asynchronous data movement engines have become essential enablers of efficiency—but only when software is explicitly designed to exploit them.</p>
<p>Another major challenge is <strong>energy efficiency</strong>, which is particularly critical for edge AI systems operating under strict power and thermal budgets. Unlike datacenter accelerators, which can amortize energy costs across large-scale infrastructure, edge devices must deliver acceptable inference or training performance within envelopes often limited to a few watts. In this regime, inefficiencies stemming from control overhead, redundant memory accesses, or suboptimal precision choices can render otherwise powerful hardware impractical <span class="citation" data-cites="chen2016eyeriss sze2020efficient">(<a href="#ref-chen2016eyeriss" role="doc-biblioref">Chen et al. 2016</a>; <a href="#ref-sze2020efficient" role="doc-biblioref">Sze et al. 2020</a>)</span>. This places increased emphasis on low-precision arithmetic, approximate computing, and hardware specialization—each of which introduces additional complexity at both the circuit and software levels.</p>
<p>Precision scaling itself represents a further challenge. While reduced-precision formats such as FP16, INT8, FP8, and more recently FP4 enable substantial gains in throughput and energy efficiency, they also impose strict numerical constraints on algorithms and training stability <span class="citation" data-cites="micikevicius2018mixed nvidia_blackwell_whitepaper">(<a href="#ref-micikevicius2018mixed" role="doc-biblioref">Micikevicius et al. 2018</a>; <a href="#ref-nvidia_blackwell_whitepaper" role="doc-biblioref">NVIDIA 2024</a>)</span>. Supporting these formats efficiently requires non-trivial micro and nanoelectronic innovations, including custom datapaths, specialized normalization logic, and fine-grained control over accumulation and scaling. Software must be co-designed to manage these constraints, often through techniques such as loss scaling, block-wise normalization, and mixed-precision accumulation.</p>
<p>Scalability and programmability constitute another critical dimension of the efficiency challenge. As GPU architectures grow increasingly complex—incorporating <em>heterogeneous execution units</em><a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>, deep memory hierarchies, and multi-die organizations—the burden placed on programmers and compilers increases substantially. Traditional programming models that abstract away hardware details often fail to expose sufficient control over data layout and execution order, leading to underutilization of available silicon resources <span class="citation" data-cites="jouppi2017datacenter">(<a href="#ref-jouppi2017datacenter" role="doc-biblioref">Jouppi et al. 2017</a>)</span>. This has motivated the rise of <strong>domain-specific languages (DSLs)</strong> and performance-oriented compiler frameworks that trade generality for efficiency by making architectural constraints explicit <span class="citation" data-cites="tillet2019triton jax_scaling_book">(<a href="#ref-tillet2019triton" role="doc-biblioref">Tillet, Kung, and Cox 2019</a>; <a href="#ref-jax_scaling_book" role="doc-biblioref">JAX Team 2023</a>)</span>.</p>
<p>For <strong>edge AI applications</strong>, these challenges are further compounded by variability in workloads, deployment environments, and real-time constraints. Edge systems must often support a diverse set of models and input conditions while maintaining deterministic latency and reliability. This requirement conflicts with highly specialized hardware designs, which excel at narrow classes of workloads but may lack flexibility <span class="citation" data-cites="sze2017efficient">(<a href="#ref-sze2017efficient" role="doc-biblioref">Sze et al. 2017</a>)</span>. Bridging this gap demands careful hardware–software co-design, where architectural specialization is balanced against programmability and reuse.</p>
</section>
<section id="objectives-and-scope-of-the-thesis" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="objectives-and-scope-of-the-thesis"><span class="header-section-number">1.3</span> Objectives and Scope of the Thesis</h2>
<p>The primary objective of this thesis is to <strong>systematically analyze and quantify the impact of hardware–software co-design on the efficiency of modern GPU architectures</strong>, with a specific focus on AI workloads dominated by dense linear algebra operations. By conducting a detailed microbenchmarking study of NVIDIA’s Hopper (H100) and Blackwell (B200) architectures, this work seeks to elucidate how architectural innovations at the micro and nanoelectronic level translate into tangible performance, energy efficiency, and scalability benefits when matched with appropriately designed software <span class="citation" data-cites="nvidia_hopper_whitepaper nvidia_blackwell_whitepaper">(<a href="#ref-nvidia_hopper_whitepaper" role="doc-biblioref">NVIDIA 2022</a>, <a href="#ref-nvidia_blackwell_whitepaper" role="doc-biblioref">2024</a>)</span>.</p>
<p>A central goal is to <strong>characterize the relationship between architectural features and achievable compute efficiency</strong>, particularly in terms of performance per watt and proximity to theoretical peak throughput. Rather than relying solely on high-level application benchmarks, this thesis emphasizes fine-grained microbenchmarks—primarily centered on General Matrix-Matrix Multiplication (GEMM)—to isolate specific architectural behaviors such as memory bandwidth utilization, arithmetic intensity, and data movement overheads. This approach enables a more precise attribution of performance bottlenecks to underlying hardware mechanisms, consistent with established performance modeling methodologies such as the Roofline model<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> <span class="citation" data-cites="williams2009roofline">(<a href="#ref-williams2009roofline" role="doc-biblioref">Williams, Waterman, and Patterson 2009</a>)</span>.</p>
<p>Another key objective is to <strong>compare Hopper and Blackwell as representative points in the evolution of GPU microarchitecture</strong>, highlighting the motivations behind Blackwell’s design choices. These include, but are not limited to, enhanced tensor computation units<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>, support for ultra-low-precision formats, expanded and restructured memory hierarchies, and advanced mechanisms for asynchronous data transfer. By evaluating how these features affect software behavior and efficiency, the thesis aims to provide empirical evidence supporting the necessity of new architectural paradigms for sustaining AI performance scaling under increasingly stringent power and area constraints <span class="citation" data-cites="horowitz2014energy thompson2021computing">(<a href="#ref-horowitz2014energy" role="doc-biblioref">Horowitz 2014</a>; <a href="#ref-thompson2021computing" role="doc-biblioref">Thompson et al. 2021</a>)</span>.</p>
<p>From a <strong>micro and nanoelectronic design perspective</strong>, this thesis seeks to bridge the gap between circuit and architecture-level considerations and system-level performance outcomes. While the experimental focus is on commercially available GPUs, the analysis explicitly interprets results through the lens of silicon efficiency, emphasizing metrics such as energy per operation, data movement cost, and utilization of on-chip resources. In doing so, the thesis aims to reinforce the relevance of microelectronic design decisions in shaping the performance envelope of AI accelerators, even at the highest levels of abstraction.</p>
<p>In parallel, the thesis aims to <strong>assess the role of modern programming models and domain-specific languages (DSLs)</strong> as enablers of hardware–software co-design. By leveraging performance-oriented frameworks such as CUTLASS, Triton, and JAX-based systems, the work explores how software abstractions can expose architectural capabilities while maintaining a balance between programmability and efficiency <span class="citation" data-cites="tillet2019triton jax_scaling_book">(<a href="#ref-tillet2019triton" role="doc-biblioref">Tillet, Kung, and Cox 2019</a>; <a href="#ref-jax_scaling_book" role="doc-biblioref">JAX Team 2023</a>)</span>. This analysis contributes to understanding how future software ecosystems must evolve alongside hardware to fully exploit increasingly specialized architectures.</p>
<p><span style="background-color: yellow">The scope of this thesis is deliberately constrained to ensure depth and rigor. Experimentally, it focuses on single-node GPU evaluation, excluding large-scale multi-node distributed training and system-level networking effects. The workloads considered are limited to representative dense linear algebra kernels and closely related AI primitives, rather than end-to-end application benchmarks. While the findings are discussed in the context of both datacenter and edge AI, the experimental platform itself is a high-end accelerator; extrapolations to edge systems are therefore conceptual and architectural rather than empirical.</span></p>
<p>In summary, the scope of this work encompasses (i) a microbenchmark-driven comparison of Hopper and Blackwell GPUs, (ii) an efficiency-focused analysis grounded in micro and nanoelectronic design principles, and (iii) an evaluation of software frameworks as integral components of hardware–software co-design. <span style="background-color: yellow">Collectively, these objectives position the thesis as a foundation for future doctoral research in edge AI and accelerator architecture, providing both methodological tools and conceptual insights relevant to the co-design of next-generation AI systems.</span></p>
</section>
<section id="methodology-overview" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="methodology-overview"><span class="header-section-number">1.4</span> Methodology Overview</h2>
<p><del>This thesis adopts a <strong>quantitative, experimental methodology</strong> grounded in microbenchmarking and performance modeling to analyze the efficiency implications of hardware–software co-design in modern GPU architectures. The methodology is designed to isolate and characterize the impact of architectural features on AI-relevant workloads, with particular emphasis on dense linear algebra kernels representative of contemporary deep learning models</del></p>
<p>The methodological workflow begins with a <strong>systematic architectural analysis</strong> of NVIDIA’s Hopper and Blackwell GPUs. This analysis focuses on identifying key microarchitectural differences—such as tensor core design, supported numerical formats, memory hierarchy organization, and data movement mechanisms—that are expected to influence performance and energy efficiency. Publicly available technical documentation and research publications are used to establish a baseline understanding of each architecture, providing context for the experimental results presented later in the thesis <span class="citation" data-cites="nvidia_hopper_whitepaper nvidia_blackwell_whitepaper">(<a href="#ref-nvidia_hopper_whitepaper" role="doc-biblioref">NVIDIA 2022</a>, <a href="#ref-nvidia_blackwell_whitepaper" role="doc-biblioref">2024</a>)</span>.</p>
<p>At the core of the experimental approach is a <strong>microbenchmark-based evaluation</strong> strategy. Rather than relying on end-to-end AI applications, the methodology employs carefully constructed General Matrix-Matrix Multiplication (GEMM) kernels to exert fine-grained control over computational patterns, memory access behavior, and data precision. This approach enables the systematic exploration of how software design choices—such as tiling parameters, memory layout, and precision selection—interact with hardware capabilities to determine achieved throughput and efficiency <span class="citation" data-cites="williams2009roofline">(<a href="#ref-williams2009roofline" role="doc-biblioref">Williams, Waterman, and Patterson 2009</a>)</span>.</p>
<p>To interpret benchmark results in a principled manner, the methodology incorporates <strong>analytical performance modeling</strong>, primarily using the Roofline model. By relating achieved performance to arithmetic intensity and hardware-specific bandwidth and compute ceilings, this framework facilitates the identification of transitions between memory-bound and compute-bound regimes. This analysis is instrumental in quantifying how architectural enhancements in Blackwell, relative to Hopper, expand the feasible operating space for AI workloads <span class="citation" data-cites="williams2009roofline horowitz2014energy">(<a href="#ref-williams2009roofline" role="doc-biblioref">Williams, Waterman, and Patterson 2009</a>; <a href="#ref-horowitz2014energy" role="doc-biblioref">Horowitz 2014</a>)</span>.</p>
<p>The software stack used in this work is deliberately chosen to reflect <strong>state-of-the-art performance-oriented programming models</strong>. Libraries and frameworks such as CUTLASS, <span style="background-color: lightgreen">Triton, and JAX-based systems</span> are employed to implement and tune microbenchmarks, allowing the study to evaluate both low-level kernel behavior and higher-level abstraction trade-offs <span class="citation" data-cites="tillet2019triton jax_scaling_book">(<a href="#ref-tillet2019triton" role="doc-biblioref">Tillet, Kung, and Cox 2019</a>; <a href="#ref-jax_scaling_book" role="doc-biblioref">JAX Team 2023</a>)</span>. Where applicable, multiple implementations of equivalent kernels are compared to assess the impact of abstraction level on performance portability and hardware utilization.</p>
<p>Measurement of <strong>performance and efficiency metrics</strong> is conducted using vendor-provided profiling tools and standardized benchmarking practices. Key metrics include sustained throughput (FLOP/s), memory bandwidth utilization, power consumption, and derived efficiency measures such as performance per watt. To ensure statistical reliability, benchmarks are repeated under controlled conditions, and results are averaged over multiple runs. Particular care is taken to account for <span style="background-color: lightgreen">warm-up effects</span>, clock variability, and measurement overhead, in line with best practices for performance evaluation on modern accelerators <span class="citation" data-cites="jouppi2017datacenter">(<a href="#ref-jouppi2017datacenter" role="doc-biblioref">Jouppi et al. 2017</a>)</span>.</p>
<p>Reproducibility and fairness are treated as first-class methodological concerns. <span style="background-color: lightgreen">All experimental configurations, including hardware specifications, software versions, compiler settings, and runtime parameters, are explicitly documented</span>. Where possible, benchmarks are designed to be portable across architectures, ensuring that observed differences can be attributed to architectural characteristics rather than implementation artifacts.</p>
<p>Finally, the methodology explicitly connects experimental findings back to <strong>micro and nanoelectronic design considerations</strong>. Observed performance trends are interpreted in terms of data movement cost, on-chip resource utilization, and energy efficiency, linking system-level behavior to underlying architectural and physical constraints. This integrative approach reinforces the central thesis that meaningful progress in AI computing efficiency emerges from coordinated advances in hardware design and software optimization.</p>
</section>
<section id="structure-of-the-thesis" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="structure-of-the-thesis"><span class="header-section-number">1.5</span> Structure of the Thesis</h2>
<p>This thesis is organized into eight chapters, each addressing a specific aspect of the analysis of hardware–software co-design in modern GPU architectures for AI workloads.</p>
<p><strong>Chapter 1</strong> introduces the motivation, challenges, objectives, and methodological foundations of the work. It establishes the context of efficient AI computing under micro- and nanoelectronic constraints and motivates the need for a co-design approach, with particular attention to the evolution of GPU architectures and their relevance to edge AI.</p>
<p><strong>Chapter 2</strong> provides the necessary background and related work. It reviews the evolution of GPU architectures from earlier generations to Hopper and Blackwell, outlines fundamental principles of hardware–software co-design, and discusses the role of General Matrix-Matrix Multiplication (GEMM) as a core computational primitive in AI workloads. Additionally, this chapter surveys relevant programming models, domain-specific languages, and tools developed by NVIDIA Research and the broader AI systems community.</p>
<p><strong>Chapter 3</strong> presents a detailed architectural comparison between NVIDIA Hopper and Blackwell GPUs. It examines key innovations introduced in Blackwell, including advances in tensor computation units, numerical precision formats, memory hierarchy, and interconnect design, and discusses their implications for performance, efficiency, and scalability.</p>
<p><strong>Chapter 4</strong> defines the metrics and models used to evaluate GPU efficiency throughout the thesis. It introduces performance per watt, arithmetic intensity, memory bandwidth utilization, and related concepts, and frames the analysis using the Roofline performance model to distinguish between compute-bound and memory-bound execution regimes.</p>
<p><strong>Chapter 5</strong> focuses on programming models and software frameworks for modern GPUs. It analyzes domain-specific languages and performance-oriented libraries such as <span style="background-color: lightgreen">Triton, CUTLASS, and JAX-based systems</span>, highlighting their role in exposing architectural features and enabling effective hardware–software co-design.</p>
<p><strong>Chapter 6</strong> describes the experimental methodology and benchmarking setup in detail. It specifies the hardware platforms, software stack, microbenchmark design, measurement techniques, and reproducibility considerations that underpin the experimental results.</p>
<p><strong>Chapter 7</strong> presents and discusses the experimental results. It compares Hopper and Blackwell across multiple metrics, analyzes performance and efficiency trends, and interprets observed bottlenecks in relation to architectural features and software design choices. A roofline-based analysis is used to contextualize results and assess the impact of architectural innovations.</p>
<p><strong>Chapter 8</strong> concludes the thesis by summarizing the main findings and discussing their implications for hardware–software co-design and edge AI systems. It also outlines directions for future work and potential doctoral research avenues building upon the results of this study.</p>
<p>Finally, the <strong>appendices</strong> provide supplementary material, including experimental scripts, extended benchmark results, and technical documentation relevant to tensor memory accelerators and GEMM intrinsics, supporting transparency and reproducibility.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-chen2016eyeriss" class="csl-entry" role="listitem">
Chen, Yu-Hsin et al. 2016. <span>“Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks.”</span> <em>IEEE Journal of Solid-State Circuits</em> 52 (1): 127–38. <a href="https://doi.org/10.1109/JSSC.2016.2616357">https://doi.org/10.1109/JSSC.2016.2616357</a>.
</div>
<div id="ref-esmaeilzadeh2011dark" class="csl-entry" role="listitem">
Esmaeilzadeh, Hadi et al. 2011. <span>“Dark Silicon and the End of Multicore Scaling.”</span> <em>ACM SIGARCH Computer Architecture News</em> 39 (3): 365–76. <a href="https://doi.org/10.1145/2024723.2000108">https://doi.org/10.1145/2024723.2000108</a>.
</div>
<div id="ref-horowitz2014energy" class="csl-entry" role="listitem">
Horowitz, Mark. 2014. <span>“Computing’s Energy Problem (and What We Can Do about It).”</span> <em>ISSCC Digest of Technical Papers</em>, 10–14. <a href="https://doi.org/10.1109/ISSCC.2014.6757323">https://doi.org/10.1109/ISSCC.2014.6757323</a>.
</div>
<div id="ref-jax_scaling_book" class="csl-entry" role="listitem">
JAX Team. 2023. <span>“The JAX ML Scaling Book.”</span> 2023. <a href="https://jax-ml.github.io/scaling-book/">https://jax-ml.github.io/scaling-book/</a>.
</div>
<div id="ref-jouppi2017datacenter" class="csl-entry" role="listitem">
Jouppi, Norman P. et al. 2017. <span>“In-Datacenter Performance Analysis of a Tensor Processing Unit.”</span> <em>IEEE Computer</em> 50 (2): 58–68. <a href="https://doi.org/10.1109/MC.2017.33">https://doi.org/10.1109/MC.2017.33</a>.
</div>
<div id="ref-jouppi2023tenyears" class="csl-entry" role="listitem">
——— et al. 2023. <span>“Ten Years of Tensor Processing Units.”</span> <em>Nature</em> 624: 41–51. <a href="https://doi.org/10.1038/s41586-023-06777-1">https://doi.org/10.1038/s41586-023-06777-1</a>.
</div>
<div id="ref-micikevicius2018mixed" class="csl-entry" role="listitem">
Micikevicius, Paulius et al. 2018. <span>“Mixed Precision Training.”</span> <em>International Conference on Learning Representations (ICLR)</em>.
</div>
<div id="ref-nvidia_hopper_whitepaper" class="csl-entry" role="listitem">
NVIDIA. 2022. <span>“NVIDIA Hopper Architecture in-Depth.”</span> NVIDIA Corporation.
</div>
<div id="ref-nvidia_blackwell_whitepaper" class="csl-entry" role="listitem">
———. 2024. <span>“NVIDIA Blackwell Architecture.”</span> NVIDIA Corporation.
</div>
<div id="ref-sze2017efficient" class="csl-entry" role="listitem">
Sze, Vivienne et al. 2017. <span>“Efficient Processing of Deep Neural Networks: A Tutorial and Survey.”</span> <em>Proceedings of the IEEE</em> 105 (12): 2295–2329. <a href="https://doi.org/10.1109/JPROC.2017.2761740">https://doi.org/10.1109/JPROC.2017.2761740</a>.
</div>
<div id="ref-sze2020efficient" class="csl-entry" role="listitem">
Sze, Vivienne, Yu-Hsin Chen, Tien-Ju Yang, and Joel Emer. 2020. <em>Efficient Processing of Deep Neural Networks</em>. Morgan &amp; Claypool. <a href="https://doi.org/10.2200/S01004ED1V01Y202004CAC050">https://doi.org/10.2200/S01004ED1V01Y202004CAC050</a>.
</div>
<div id="ref-thompson2021computing" class="csl-entry" role="listitem">
Thompson, Neil C., Kristjan Greenewald, Keeheon Lee, and Gabriel F. Manso. 2021. <span>“Computing Beyond Moore’s Law.”</span> <em>Science</em> 371 (6530): eaba7373. <a href="https://doi.org/10.1126/science.aba7373">https://doi.org/10.1126/science.aba7373</a>.
</div>
<div id="ref-tillet2019triton" class="csl-entry" role="listitem">
Tillet, Philippe, H. T. Kung, and David Cox. 2019. <span>“Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations.”</span> <em>Proceedings of MLSys</em>.
</div>
<div id="ref-williams2009roofline" class="csl-entry" role="listitem">
Williams, Samuel, Andrew Waterman, and David Patterson. 2009. <span>“Roofline: An Insightful Visual Performance Model for Multicore Architectures.”</span> <em>Communications of the ACM</em> 52 (4): 65–76. <a href="https://doi.org/10.1145/1498765.1498785">https://doi.org/10.1145/1498765.1498785</a>.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p><em>Dennard scaling</em> refers to the principle formulated by Robert H. Dennard et al.&nbsp;(1974), according to which, as transistor dimensions are reduced, voltage and current scale proportionally, keeping power density approximately constant. This enabled performance improvements across successive technology generations.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><em>Tensor cores</em> are specialized hardware units introduced by NVIDIA to accelerate dense matrix–matrix operations, particularly mixed-precision matrix multiply–accumulate (MMA) workloads. They are designed to deliver high throughput for deep learning primitives by operating on small matrix tiles using reduced-precision arithmetic.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p><em>Tensor engines</em> refer to more advanced and specialized successors of tensor cores in recent GPU architectures, integrating support for additional low-precision formats (such as FP8 and FP4), higher degrees of specialization, and tighter coupling with data movement and scheduling mechanisms to optimize end-to-end AI workloads.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>A workload is said to be <em>memory-bound</em> when its performance is primarily limited by memory bandwidth or memory access latency rather than by the available compute throughput. In such cases, increasing arithmetic capability yields limited performance gains unless memory access patterns, data locality, or data movement mechanisms are improved.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><em>Domain-specific languages (DSLs)</em> are programming languages designed to express computations within a restricted application domain, providing high-level abstractions that enable domain-aware optimizations while reducing the burden of low-level hardware management for programmers.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p><em>Compiler frameworks</em> are extensible software infrastructures that support the construction of compilers and code generation pipelines, typically providing intermediate representations, optimization passes, and backend targets. In the context of AI and GPU computing, they facilitate the systematic mapping of high-level program abstractions onto specialized hardware architectures.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p><em>Attention mechanisms</em> are a class of neural network operations, most prominently used in Transformer architectures, that compute weighted combinations of input features based on pairwise similarity scores (e.g., dot products between queries and keys). Their computational structure involves large matrix multiplications and memory-intensive softmax and normalization steps, making their performance highly sensitive to data locality, memory bandwidth, and efficient data movement.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p><em>Software-managed scratchpads</em> are explicitly addressable on-chip memory regions whose allocation, placement, and data movement are controlled by software rather than by hardware-managed cache policies. They enable predictable latency and bandwidth characteristics, allowing programmers or compilers to optimize data locality and reuse in performance-critical kernels.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p><em>Tensor memory accelerators</em> refer to specialized hardware units designed to optimize data movement and layout transformations for tensor-oriented workloads. These accelerators reduce the overhead of memory access and data rearrangement by offloading common tensor data movement patterns—such as tiling, transposition, and format conversion—from general-purpose execution units.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p><em>Heterogeneous execution units</em> refer to the coexistence of multiple types of specialized compute units within a single GPU, such as scalar and vector ALUs, tensor cores, ray tracing units, and fixed-function accelerators. Each unit type exhibits distinct performance characteristics and programming constraints, requiring explicit scheduling and workload mapping to achieve high utilization.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p><em>Roofline model</em> is an analytical performance model that relates the achievable performance of a computing system to an application’s <strong>arithmetic intensity</strong> (operations per byte transferred). It defines an upper bound on performance through two limiting ceilings: the hardware’s <strong>peak compute throughput</strong> and its <strong>memory bandwidth</strong>. The model is used to determine whether an application is compute-bound or memory-bound and to systematically identify architectural performance bottlenecks.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p><em>Tensor computation unit</em> is a specialized hardware execution unit designed to accelerate dense linear algebra operations—most notably matrix–matrix and matrix–vector multiplications—by performing fused multiply–accumulate operations on small matrix tiles in parallel. Tensor computation units are optimized for high throughput and energy efficiency, typically supporting reduced-precision data formats, and are a fundamental building block for accelerating machine learning and AI workloads on modern GPUs.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link" aria-label="index.html">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">index.html</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./02-background.html" class="pagination-link" aria-label="Chapter 2 – Background and Related Work">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Chapter 2 – Background and Related Work</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>