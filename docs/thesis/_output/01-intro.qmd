```{=latex}
\pagenumbering{arabic}
```

# Introduction
## Motivation and Context: The Need for Hardware–Software Co-Design

The rapid evolution of artificial intelligence (AI), particularly deep learning models based on large-scale matrix operations, has fundamentally reshaped the requirements imposed on modern computing systems. Contemporary workloads such as transformer-based models, large language models (LLMs), and multimodal AI systems exhibit unprecedented computational intensity, memory bandwidth demand, and energy consumption. These characteristics challenge traditional computing paradigms and demand a holistic approach in which **hardware and software are designed in close coordination**, rather than as independent layers of abstraction [@jouppi2017datacenter; @sze2020efficient].

Historically, general-purpose processors benefited from *Dennard scaling*[^1] and instruction-level parallelism, allowing software performance to improve transparently with each technology generation. However, as CMOS scaling entered the sub-10 nm regime, physical constraints related to power density, leakage currents, interconnect delay, and thermal dissipation have severely limited frequency scaling and brute-force performance improvements [@esmaeilzadeh2011dark; @thompson2021computing]. In this context, **micro and nanoelectronic design choices**, such as transistor architecture, memory hierarchy, interconnect topology, and on-die specialization have become first-order determinants of achievable system-level performance and efficiency.

Graphics Processing Units (GPUs) have emerged as the dominant computing substrate for AI due to their massive parallelism and high arithmetic throughput. Yet, modern GPUs are no longer generic accelerators; they are highly specialized systems whose efficiency depends critically on software being explicitly structured to exploit architectural features such as *tensor cores*[^2], hierarchical memory systems, and fine-grained synchronization mechanisms [@nvidia_hopper_whitepaper]. This tight coupling is particularly evident in the evolution from NVIDIA’s Hopper architecture to the more recent Blackwell architecture, where architectural innovations such as ultra-specialized *tensor engines*[^3], new low-precision formats (e.g., FP8 and FP4), and advanced memory movement engines fundamentally reshape the optimal software execution model [@nvidia_blackwell_whitepaper].

From a **hardware design perspective**, the motivation for hardware–software co-design is rooted in the need to maximize *performance per watt* and *performance per unit area*, two metrics that are especially critical in both datacenter-scale systems and edge deployments. Decisions made at the microarchitectural and circuit levels—such as register file sizing, shared memory organization, cache coherence policies, and on-chip network design—directly constrain which software access patterns are efficient or even feasible [@sze2020efficient]. Consequently, software frameworks that ignore these constraints often underutilize available silicon resources, leading to *memory-bound*[^4] execution and suboptimal energy efficiency.

Conversely, from a **software and algorithmic standpoint**, AI workloads are increasingly designed with an awareness of hardware realities. Techniques such as operator fusion, tiling strategies, mixed-precision arithmetic, and explicit data movement orchestration have become central to achieving peak performance. *Domain-specific languages (DSLs)*[^5] and *compiler frameworks*[^6]—such as Triton, CUTLASS, and JAX-based systems—embody this philosophy by exposing hardware capabilities directly to the programmer or compiler, thereby enabling software to push the hardware toward compute-bound operation [@jax_scaling_book; @tillet2019triton].

This paradigm is particularly relevant for **edge AI**, where constraints on power, thermal envelope, and silicon area are even more stringent than in datacenter environments. Edge devices require carefully balanced designs in which architectural specialization compensates for limited resources, and software must be explicitly optimized to match the underlying hardware characteristics [@sze2017efficient]. Although this thesis focuses experimentally on high-end GPUs, the insights derived from microbenchmarking architectures such as Hopper and Blackwell are directly transferable to edge-class accelerators, where hardware–software co-design is often the only viable path to achieving acceptable performance and energy efficiency.

In this context, the motivation of this thesis is twofold. First, it aims to **demonstrate, through microbenchmarking**, how architectural innovations at the micro and nanoelectronic level translate into measurable performance and efficiency gains only when matched with appropriately designed software. Second, it seeks to highlight that future progress in AI systems—both at the edge and in large-scale deployments—will depend not on isolated advances in hardware or software, but on their **co-evolution as a unified design problem** [@jouppi2023tenyears].

## Challenges in Efficient Compute for AI and Edge Applications

The efficient execution of modern AI workloads presents a multifaceted set of challenges that span algorithm design, software systems, microarchitecture, and physical implementation. While advances in deep learning have been driven primarily by increases in model size and data availability, the resulting computational demands increasingly stress the fundamental limits of contemporary hardware, especially in power- and area-constrained environments such as edge devices [@thompson2021computing; @sze2017efficient].

A primary challenge arises from the **mismatch between arithmetic throughput and memory bandwidth**, commonly referred to as the *memory wall*. Although modern accelerators provide massive peak compute capabilities—often measured in tens or hundreds of teraFLOPS—real-world AI kernels frequently fail to achieve these peaks due to insufficient data reuse and limited effective memory bandwidth [@williams2009roofline]. This imbalance is particularly pronounced in workloads dominated by General Matrix-Matrix Multiplication (GEMM) and *attention mechanisms*[^7], where performance depends critically on maximizing arithmetic intensity through careful tiling, blocking, and data movement strategies.

From a **microarchitectural standpoint**, this challenge translates directly into design trade-offs involving cache hierarchy depth, shared memory capacity, register file sizing, and on-chip interconnect bandwidth. As technology scaling progresses into advanced nodes, wire delay and energy increasingly dominate over transistor switching costs, making data movement significantly more expensive than computation [@horowitz2014energy]. Consequently, architectural features such as *software-managed scratchpads*[^8], *tensor memory accelerators*[^9], and explicit asynchronous data movement engines have become essential enablers of efficiency—but only when software is explicitly designed to exploit them.

Another major challenge is **energy efficiency**, which is particularly critical for edge AI systems operating under strict power and thermal budgets. Unlike datacenter accelerators, which can amortize energy costs across large-scale infrastructure, edge devices must deliver acceptable inference or training performance within envelopes often limited to a few watts. In this regime, inefficiencies stemming from control overhead, redundant memory accesses, or suboptimal precision choices can render otherwise powerful hardware impractical [@chen2016eyeriss; @sze2020efficient]. This places increased emphasis on low-precision arithmetic, approximate computing, and hardware specialization—each of which introduces additional complexity at both the circuit and software levels.

Precision scaling itself represents a further challenge. While reduced-precision formats such as FP16, INT8, FP8, and more recently FP4 enable substantial gains in throughput and energy efficiency, they also impose strict numerical constraints on algorithms and training stability [@micikevicius2018mixed; @nvidia_blackwell_whitepaper]. Supporting these formats efficiently requires non-trivial micro and nanoelectronic innovations, including custom datapaths, specialized normalization logic, and fine-grained control over accumulation and scaling. Software must be co-designed to manage these constraints, often through techniques such as loss scaling, block-wise normalization, and mixed-precision accumulation.

Scalability and programmability constitute another critical dimension of the efficiency challenge. As GPU architectures grow increasingly complex—incorporating *heterogeneous execution units*[^10], deep memory hierarchies, and multi-die organizations—the burden placed on programmers and compilers increases substantially. Traditional programming models that abstract away hardware details often fail to expose sufficient control over data layout and execution order, leading to underutilization of available silicon resources [@jouppi2017datacenter]. This has motivated the rise of **domain-specific languages (DSLs)** and performance-oriented compiler frameworks that trade generality for efficiency by making architectural constraints explicit [@tillet2019triton; @jax_scaling_book].

For **edge AI applications**, these challenges are further compounded by variability in workloads, deployment environments, and real-time constraints. Edge systems must often support a diverse set of models and input conditions while maintaining deterministic latency and reliability. This requirement conflicts with highly specialized hardware designs, which excel at narrow classes of workloads but may lack flexibility [@sze2017efficient]. Bridging this gap demands careful hardware–software co-design, where architectural specialization is balanced against programmability and reuse.

## Objectives and Scope of the Thesis

The primary objective of this thesis is to **systematically analyze and quantify the impact of hardware–software co-design on the efficiency of modern GPU architectures**, with a specific focus on AI workloads dominated by dense linear algebra operations. By conducting a detailed microbenchmarking study of NVIDIA’s Hopper (H100) and Blackwell (B200) architectures, this work seeks to elucidate how architectural innovations at the micro and nanoelectronic level translate into tangible performance, energy efficiency, and scalability benefits when matched with appropriately designed software [@nvidia_hopper_whitepaper; @nvidia_blackwell_whitepaper].

A central goal is to **characterize the relationship between architectural features and achievable compute efficiency**, particularly in terms of performance per watt and proximity to theoretical peak throughput. Rather than relying solely on high-level application benchmarks, this thesis emphasizes fine-grained microbenchmarks—primarily centered on General Matrix-Matrix Multiplication (GEMM)—to isolate specific architectural behaviors such as memory bandwidth utilization, arithmetic intensity, and data movement overheads. This approach enables a more precise attribution of performance bottlenecks to underlying hardware mechanisms, consistent with established performance modeling methodologies such as the Roofline model[^11] [@williams2009roofline].

Another key objective is to **compare Hopper and Blackwell as representative points in the evolution of GPU microarchitecture**, highlighting the motivations behind Blackwell’s design choices. These include, but are not limited to, enhanced tensor computation units[^12], support for ultra-low-precision formats, expanded and restructured memory hierarchies, and advanced mechanisms for asynchronous data transfer. By evaluating how these features affect software behavior and efficiency, the thesis aims to provide empirical evidence supporting the necessity of new architectural paradigms for sustaining AI performance scaling under increasingly stringent power and area constraints [@horowitz2014energy; @thompson2021computing].

From a **micro and nanoelectronic design perspective**, this thesis seeks to bridge the gap between circuit and architecture-level considerations and system-level performance outcomes. While the experimental focus is on commercially available GPUs, the analysis explicitly interprets results through the lens of silicon efficiency, emphasizing metrics such as energy per operation, data movement cost, and utilization of on-chip resources. In doing so, the thesis aims to reinforce the relevance of microelectronic design decisions in shaping the performance envelope of AI accelerators, even at the highest levels of abstraction.

In parallel, the thesis aims to **assess the role of modern programming models and domain-specific languages (DSLs)** as enablers of hardware–software co-design. By leveraging performance-oriented frameworks such as CUTLASS, Triton, and JAX-based systems, the work explores how software abstractions can expose architectural capabilities while maintaining a balance between programmability and efficiency [@tillet2019triton; @jax_scaling_book]. This analysis contributes to understanding how future software ecosystems must evolve alongside hardware to fully exploit increasingly specialized architectures.

<span style="background-color: yellow">The scope of this thesis is deliberately constrained to ensure depth and rigor. Experimentally, it focuses on single-node GPU evaluation, excluding large-scale multi-node distributed training and system-level networking effects. The workloads considered are limited to representative dense linear algebra kernels and closely related AI primitives, rather than end-to-end application benchmarks. While the findings are discussed in the context of both datacenter and edge AI, the experimental platform itself is a high-end accelerator; extrapolations to edge systems are therefore conceptual and architectural rather than empirical.</span>

In summary, the scope of this work encompasses (i) a microbenchmark-driven comparison of Hopper and Blackwell GPUs, (ii) an efficiency-focused analysis grounded in micro and nanoelectronic design principles, and (iii) an evaluation of software frameworks as integral components of hardware–software co-design. <span style="background-color: yellow">Collectively, these objectives position the thesis as a foundation for future doctoral research in edge AI and accelerator architecture, providing both methodological tools and conceptual insights relevant to the co-design of next-generation AI systems.</span>

## Methodology Overview

~~This thesis adopts a **quantitative, experimental methodology** grounded in microbenchmarking and performance modeling to analyze the efficiency implications of hardware–software co-design in modern GPU architectures. The methodology is designed to isolate and characterize the impact of architectural features on AI-relevant workloads, with particular emphasis on dense linear algebra kernels representative of contemporary deep learning models~~

The methodological workflow begins with a **systematic architectural analysis** of NVIDIA’s Hopper and Blackwell GPUs. This analysis focuses on identifying key microarchitectural differences—such as tensor core design, supported numerical formats, memory hierarchy organization, and data movement mechanisms—that are expected to influence performance and energy efficiency. Publicly available technical documentation and research publications are used to establish a baseline understanding of each architecture, providing context for the experimental results presented later in the thesis [@nvidia_hopper_whitepaper; @nvidia_blackwell_whitepaper].

At the core of the experimental approach is a **microbenchmark-based evaluation** strategy. Rather than relying on end-to-end AI applications, the methodology employs carefully constructed General Matrix-Matrix Multiplication (GEMM) kernels to exert fine-grained control over computational patterns, memory access behavior, and data precision. This approach enables the systematic exploration of how software design choices—such as tiling parameters, memory layout, and precision selection—interact with hardware capabilities to determine achieved throughput and efficiency [@williams2009roofline].

To interpret benchmark results in a principled manner, the methodology incorporates **analytical performance modeling**, primarily using the Roofline model. By relating achieved performance to arithmetic intensity and hardware-specific bandwidth and compute ceilings, this framework facilitates the identification of transitions between memory-bound and compute-bound regimes. This analysis is instrumental in quantifying how architectural enhancements in Blackwell, relative to Hopper, expand the feasible operating space for AI workloads [@williams2009roofline; @horowitz2014energy].

The software stack used in this work is deliberately chosen to reflect **state-of-the-art performance-oriented programming models**. Libraries and frameworks such as CUTLASS, <span style="background-color: lightgreen">Triton, and JAX-based systems</span> are employed to implement and tune microbenchmarks, allowing the study to evaluate both low-level kernel behavior and higher-level abstraction trade-offs [@tillet2019triton; @jax_scaling_book]. Where applicable, multiple implementations of equivalent kernels are compared to assess the impact of abstraction level on performance portability and hardware utilization.

Measurement of **performance and efficiency metrics** is conducted using vendor-provided profiling tools and standardized benchmarking practices. Key metrics include sustained throughput (FLOP/s), memory bandwidth utilization, power consumption, and derived efficiency measures such as performance per watt. To ensure statistical reliability, benchmarks are repeated under controlled conditions, and results are averaged over multiple runs. Particular care is taken to account for <span style="background-color: lightgreen">warm-up effects</span>, clock variability, and measurement overhead, in line with best practices for performance evaluation on modern accelerators [@jouppi2017datacenter].

Reproducibility and fairness are treated as first-class methodological concerns. <span style="background-color: lightgreen">All experimental configurations, including hardware specifications, software versions, compiler settings, and runtime parameters, are explicitly documented</span>. Where possible, benchmarks are designed to be portable across architectures, ensuring that observed differences can be attributed to architectural characteristics rather than implementation artifacts.

Finally, the methodology explicitly connects experimental findings back to **micro and nanoelectronic design considerations**. Observed performance trends are interpreted in terms of data movement cost, on-chip resource utilization, and energy efficiency, linking system-level behavior to underlying architectural and physical constraints. This integrative approach reinforces the central thesis that meaningful progress in AI computing efficiency emerges from coordinated advances in hardware design and software optimization.

## Structure of the Thesis

This thesis is organized into eight chapters, each addressing a specific aspect of the analysis of hardware–software co-design in modern GPU architectures for AI workloads.

**Chapter 1** introduces the motivation, challenges, objectives, and methodological foundations of the work. It establishes the context of efficient AI computing under micro- and nanoelectronic constraints and motivates the need for a co-design approach, with particular attention to the evolution of GPU architectures and their relevance to edge AI.

**Chapter 2** provides the necessary background and related work. It reviews the evolution of GPU architectures from earlier generations to Hopper and Blackwell, outlines fundamental principles of hardware–software co-design, and discusses the role of General Matrix-Matrix Multiplication (GEMM) as a core computational primitive in AI workloads. Additionally, this chapter surveys relevant programming models, domain-specific languages, and tools developed by NVIDIA Research and the broader AI systems community.

**Chapter 3** presents a detailed architectural comparison between NVIDIA Hopper and Blackwell GPUs. It examines key innovations introduced in Blackwell, including advances in tensor computation units, numerical precision formats, memory hierarchy, and interconnect design, and discusses their implications for performance, efficiency, and scalability.

**Chapter 4** defines the metrics and models used to evaluate GPU efficiency throughout the thesis. It introduces performance per watt, arithmetic intensity, memory bandwidth utilization, and related concepts, and frames the analysis using the Roofline performance model to distinguish between compute-bound and memory-bound execution regimes.

**Chapter 5** focuses on programming models and software frameworks for modern GPUs. It analyzes domain-specific languages and performance-oriented libraries such as <span style="background-color: lightgreen">Triton, CUTLASS, and JAX-based systems</span>, highlighting their role in exposing architectural features and enabling effective hardware–software co-design.

**Chapter 6** describes the experimental methodology and benchmarking setup in detail. It specifies the hardware platforms, software stack, microbenchmark design, measurement techniques, and reproducibility considerations that underpin the experimental results.

**Chapter 7** presents and discusses the experimental results. It compares Hopper and Blackwell across multiple metrics, analyzes performance and efficiency trends, and interprets observed bottlenecks in relation to architectural features and software design choices. A roofline-based analysis is used to contextualize results and assess the impact of architectural innovations.

**Chapter 8** concludes the thesis by summarizing the main findings and discussing their implications for hardware–software co-design and edge AI systems. It also outlines directions for future work and potential doctoral research avenues building upon the results of this study.

Finally, the **appendices** provide supplementary material, including experimental scripts, extended benchmark results, and technical documentation relevant to tensor memory accelerators and GEMM intrinsics, supporting transparency and reproducibility.

[^1]: *Dennard scaling* refers to the principle formulated by Robert H. Dennard et al. (1974),
according to which, as transistor dimensions are reduced, voltage and current scale proportionally,
keeping power density approximately constant. This enabled performance improvements across successive
technology generations.

[^2]: *Tensor cores* are specialized hardware units introduced by NVIDIA to accelerate dense matrix--matrix operations, particularly mixed-precision matrix multiply--accumulate (MMA) workloads. They are designed to deliver high throughput for deep learning primitives by operating on small matrix tiles using reduced-precision arithmetic.

[^3]: *Tensor engines* refer to more advanced and specialized successors of tensor cores in recent GPU
architectures, integrating support for additional low-precision formats (such as FP8 and FP4), higher degrees
of specialization, and tighter coupling with data movement and scheduling mechanisms to optimize end-to-end AI
workloads.

[^4]: A workload is said to be *memory-bound* when its performance is primarily limited by memory bandwidth or memory access latency rather than by the available compute throughput. In such cases, increasing arithmetic capability yields limited performance gains unless memory access patterns, data locality, or data movement mechanisms are improved.

[^5]: *Domain-specific languages (DSLs)* are programming languages designed to express computations within a restricted application domain, providing high-level abstractions that enable domain-aware optimizations while reducing the burden of low-level hardware management for programmers.

[^6]: *Compiler frameworks* are extensible software infrastructures that support the construction of compilers and code generation pipelines, typically providing intermediate representations, optimization passes, and backend targets. In the context of AI and GPU computing, they facilitate the systematic mapping of high-level program abstractions onto specialized hardware architectures.

[^7]: *Attention mechanisms* are a class of neural network operations, most prominently used in Transformer architectures, that compute weighted combinations of input features based on pairwise similarity scores (e.g., dot products between queries and keys). Their computational structure involves large matrix multiplications and memory-intensive softmax and normalization steps, making their performance highly sensitive to data locality, memory bandwidth, and efficient data movement.

[^8]: *Software-managed scratchpads* are explicitly addressable on-chip memory regions whose allocation, placement, and data movement are controlled by software rather than by hardware-managed cache policies. They enable predictable latency and bandwidth characteristics, allowing programmers or compilers to optimize data locality and reuse in performance-critical kernels.
[^9]: *Tensor memory accelerators* refer to specialized hardware units designed to optimize data movement and layout transformations for tensor-oriented workloads. These accelerators reduce the overhead of memory access and data rearrangement by offloading common tensor data movement patterns—such as tiling, transposition, and format conversion—from general-purpose execution units.

[^10]: *Heterogeneous execution units* refer to the coexistence of multiple types of specialized compute units within a single GPU, such as scalar and vector ALUs, tensor cores, ray tracing units, and fixed-function accelerators. Each unit type exhibits distinct performance characteristics and programming constraints, requiring explicit scheduling and workload mapping to achieve high utilization.

[^11]: *Roofline model* is an analytical performance model that relates the achievable performance of a computing system to an application’s **arithmetic intensity** (operations per byte transferred). It defines an upper bound on performance through two limiting ceilings: the hardware’s **peak compute throughput** and its **memory bandwidth**. The model is used to determine whether an application is compute-bound or memory-bound and to systematically identify architectural performance bottlenecks.

[^12]: *Tensor computation unit* is a specialized hardware execution unit designed to accelerate dense linear algebra operations—most notably matrix–matrix and matrix–vector multiplications—by performing fused multiply–accumulate operations on small matrix tiles in parallel. Tensor computation units are optimized for high throughput and energy efficiency, typically supporting reduced-precision data formats, and are a fundamental building block for accelerating machine learning and AI workloads on modern GPUs.
